{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch를 이용한 간단한 CNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    # Pytorch 기본 라이브러리\n",
    "import torch.nn as nn   # Neural Network (nn)에 관련된 모듈들\n",
    "from torch.utils.data import DataLoader # Database의 데이터들을 불러오기 위한 라이브러리\n",
    "\n",
    "from torchvision import transforms, datasets    # 불러온 입력을 처리하는 모듈들\n",
    "\n",
    "import os   # 경로 탐색, 접근 등을 처리하기 위한 라이브러리\n",
    "import numpy as np # 행렬 연산에 사용하는 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습을 위한 하이퍼 파리미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 128\n",
    "num_epoch = 5\n",
    "\n",
    "ckpt_dir = './checkpoint'   # 학습된 파라미터를 저장할 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):       # nn.Module를 상속받아서 사용.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # 상속해준 부모 클래스의 Method를 사용할 때 super(하위클래스, 하위클래스의 객체)\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=10, \n",
    "                               kernel_size=5, \n",
    "                               stride=1, \n",
    "                               padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)    # cf. AvgPool2d\n",
    "\n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, \n",
    "                               out_channels=20, \n",
    "                               kernel_size=5, \n",
    "                               stride=1, \n",
    "                               padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)    # H * W * C = 320\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=320, \n",
    "                             out_features=50)             # Fully-Connected Layer\n",
    "        self.relu1_fc1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):   # x: 입력영상 (torch.Tensor())\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 320)     # x: B=128, C=20, H=4, W=4\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1_fc1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습된 파라미터를 저장하거나 불러오는 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(ckpt_dir, net, optim, epoch):      # ckpt_dir: checkpoint를 저장할 경로, net, optim, epoch\n",
    "    if not os.path.exists(ckpt_dir):    # ckpt_dir이 존재하는지 확인하는 함수\n",
    "        os.makedirs(ckpt_dir)           # 디렉토리를 만들어주는 함수\n",
    "\n",
    "    torch.save({'net': net.state_dict(),        # 네트워크에 있는 변수들\n",
    "                'optim': optim.state_dict()},   # optimizer에 있는 변수들\n",
    "               './%s/model_epoch%d.pth' % (ckpt_dir, epoch))\n",
    "\n",
    "def load(ckpt_dir, net, optim):\n",
    "    ckpt_lst = os.listdir(ckpt_dir)  # 입력한 디렉토리 내의 모든 파일과 디렉토리의 리스트를 반환\n",
    "    ckpt_lst.sort()  # 정렬\n",
    "\n",
    "    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "\n",
    "    return net, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서식문자를 이용한 문자열 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat is animal.\n",
      "2 + 3 is 5\n"
     ]
    }
   ],
   "source": [
    "print('%s is %s.' % ('Cat', 'animal'))\n",
    "print('%d + %d is %d' % (2, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "a = ['b', 'a', 'c']\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Database 불러오기\n",
    "### Database가 없을 경우 Pytorch에 내장되어 있는 MNIST 다운로드 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "dataset = datasets.MNIST(download=True, \n",
    "                         root='./', \n",
    "                         train=True, \n",
    "                         transform=transform)   # 개별 데이터 파일의 처리를 담당\n",
    "loader = DataLoader(dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, \n",
    "                    num_workers=0)  # 미니배치 처리를 담당\n",
    "\n",
    "num_data = len(loader.dataset)\n",
    "num_batch = num_data // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 네트워크 및 손실함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "params = net.parameters()\n",
    "\n",
    "fn_loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def fn_pred(output):\n",
    "    return torch.softmax(output, dim=1)     # B=128 (dim=0), 10 (dim=1)\n",
    "\n",
    "def fn_accuracy(pred, label):\n",
    "    return (pred.max(dim=1)[1] == label).type(torch.float).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimizer 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0001/0005 | BATCH 0000/0468 | LOSS: 2.2929 | ACC 0.1328\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0001/0468 | LOSS: 2.2890 | ACC 0.1406\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0002/0468 | LOSS: 2.2940 | ACC 0.1276\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0003/0468 | LOSS: 2.2978 | ACC 0.1113\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0004/0468 | LOSS: 2.2983 | ACC 0.1172\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0005/0468 | LOSS: 2.2978 | ACC 0.1198\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0006/0468 | LOSS: 2.2970 | ACC 0.1217\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0007/0468 | LOSS: 2.2967 | ACC 0.1191\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0008/0468 | LOSS: 2.2957 | ACC 0.1259\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0009/0468 | LOSS: 2.2965 | ACC 0.1281\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0010/0468 | LOSS: 2.2971 | ACC 0.1314\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0011/0468 | LOSS: 2.2964 | ACC 0.1328\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0012/0468 | LOSS: 2.2953 | ACC 0.1346\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0013/0468 | LOSS: 2.2952 | ACC 0.1350\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0014/0468 | LOSS: 2.2953 | ACC 0.1339\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0015/0468 | LOSS: 2.2957 | ACC 0.1318\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0016/0468 | LOSS: 2.2950 | ACC 0.1324\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0017/0468 | LOSS: 2.2945 | ACC 0.1328\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0018/0468 | LOSS: 2.2944 | ACC 0.1349\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0019/0468 | LOSS: 2.2940 | ACC 0.1355\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0020/0468 | LOSS: 2.2935 | ACC 0.1354\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0021/0468 | LOSS: 2.2931 | ACC 0.1374\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0022/0468 | LOSS: 2.2935 | ACC 0.1376\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0023/0468 | LOSS: 2.2934 | ACC 0.1374\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0024/0468 | LOSS: 2.2928 | ACC 0.1384\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0025/0468 | LOSS: 2.2927 | ACC 0.1379\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0026/0468 | LOSS: 2.2923 | ACC 0.1395\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0027/0468 | LOSS: 2.2920 | ACC 0.1398\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0028/0468 | LOSS: 2.2918 | ACC 0.1406\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0029/0468 | LOSS: 2.2923 | ACC 0.1398\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0030/0468 | LOSS: 2.2924 | ACC 0.1399\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0031/0468 | LOSS: 2.2922 | ACC 0.1394\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0032/0468 | LOSS: 2.2918 | ACC 0.1394\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0033/0468 | LOSS: 2.2915 | ACC 0.1402\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0034/0468 | LOSS: 2.2916 | ACC 0.1393\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0035/0468 | LOSS: 2.2913 | ACC 0.1398\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0036/0468 | LOSS: 2.2910 | ACC 0.1391\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0037/0468 | LOSS: 2.2905 | ACC 0.1398\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0038/0468 | LOSS: 2.2900 | ACC 0.1410\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0039/0468 | LOSS: 2.2898 | ACC 0.1412\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0040/0468 | LOSS: 2.2895 | ACC 0.1412\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0041/0468 | LOSS: 2.2889 | ACC 0.1429\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0042/0468 | LOSS: 2.2887 | ACC 0.1437\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0043/0468 | LOSS: 2.2883 | ACC 0.1451\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0044/0468 | LOSS: 2.2882 | ACC 0.1458\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0045/0468 | LOSS: 2.2880 | ACC 0.1462\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0046/0468 | LOSS: 2.2877 | ACC 0.1474\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0047/0468 | LOSS: 2.2872 | ACC 0.1481\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0048/0468 | LOSS: 2.2873 | ACC 0.1475\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0049/0468 | LOSS: 2.2871 | ACC 0.1475\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0050/0468 | LOSS: 2.2870 | ACC 0.1486\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0051/0468 | LOSS: 2.2867 | ACC 0.1505\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0052/0468 | LOSS: 2.2864 | ACC 0.1523\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0053/0468 | LOSS: 2.2860 | ACC 0.1536\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0054/0468 | LOSS: 2.2859 | ACC 0.1550\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0055/0468 | LOSS: 2.2856 | ACC 0.1561\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0056/0468 | LOSS: 2.2854 | ACC 0.1569\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0057/0468 | LOSS: 2.2853 | ACC 0.1581\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0058/0468 | LOSS: 2.2850 | ACC 0.1594\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0059/0468 | LOSS: 2.2848 | ACC 0.1602\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0060/0468 | LOSS: 2.2846 | ACC 0.1619\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0061/0468 | LOSS: 2.2843 | ACC 0.1631\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0062/0468 | LOSS: 2.2839 | ACC 0.1641\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0063/0468 | LOSS: 2.2836 | ACC 0.1648\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0064/0468 | LOSS: 2.2832 | ACC 0.1668\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0065/0468 | LOSS: 2.2830 | ACC 0.1680\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0066/0468 | LOSS: 2.2828 | ACC 0.1684\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0067/0468 | LOSS: 2.2826 | ACC 0.1692\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0068/0468 | LOSS: 2.2822 | ACC 0.1705\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0069/0468 | LOSS: 2.2819 | ACC 0.1725\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0070/0468 | LOSS: 2.2816 | ACC 0.1734\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0071/0468 | LOSS: 2.2813 | ACC 0.1746\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0072/0468 | LOSS: 2.2810 | ACC 0.1758\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0073/0468 | LOSS: 2.2807 | ACC 0.1768\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0074/0468 | LOSS: 2.2804 | ACC 0.1778\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0075/0468 | LOSS: 2.2802 | ACC 0.1780\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0076/0468 | LOSS: 2.2799 | ACC 0.1784\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0077/0468 | LOSS: 2.2797 | ACC 0.1790\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0078/0468 | LOSS: 2.2794 | ACC 0.1795\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0079/0468 | LOSS: 2.2791 | ACC 0.1806\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0080/0468 | LOSS: 2.2788 | ACC 0.1817\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0081/0468 | LOSS: 2.2785 | ACC 0.1825\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0082/0468 | LOSS: 2.2783 | ACC 0.1825\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0083/0468 | LOSS: 2.2779 | ACC 0.1841\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0084/0468 | LOSS: 2.2777 | ACC 0.1843\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0085/0468 | LOSS: 2.2773 | ACC 0.1862\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0086/0468 | LOSS: 2.2770 | ACC 0.1869\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0087/0468 | LOSS: 2.2767 | ACC 0.1875\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0088/0468 | LOSS: 2.2764 | ACC 0.1885\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0089/0468 | LOSS: 2.2761 | ACC 0.1898\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0090/0468 | LOSS: 2.2756 | ACC 0.1914\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0091/0468 | LOSS: 2.2752 | ACC 0.1925\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0092/0468 | LOSS: 2.2748 | ACC 0.1937\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0093/0468 | LOSS: 2.2745 | ACC 0.1946\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0094/0468 | LOSS: 2.2741 | ACC 0.1952\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0095/0468 | LOSS: 2.2738 | ACC 0.1954\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0096/0468 | LOSS: 2.2734 | ACC 0.1964\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0097/0468 | LOSS: 2.2731 | ACC 0.1972\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0098/0468 | LOSS: 2.2728 | ACC 0.1977\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0099/0468 | LOSS: 2.2725 | ACC 0.1982\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0100/0468 | LOSS: 2.2722 | ACC 0.1983\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0101/0468 | LOSS: 2.2719 | ACC 0.1988\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0102/0468 | LOSS: 2.2716 | ACC 0.1996\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0103/0468 | LOSS: 2.2713 | ACC 0.2000\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0104/0468 | LOSS: 2.2709 | ACC 0.2012\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0105/0468 | LOSS: 2.2706 | ACC 0.2019\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0106/0468 | LOSS: 2.2702 | ACC 0.2031\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0107/0468 | LOSS: 2.2698 | ACC 0.2044\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0108/0468 | LOSS: 2.2694 | ACC 0.2048\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0109/0468 | LOSS: 2.2691 | ACC 0.2053\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0110/0468 | LOSS: 2.2687 | ACC 0.2066\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0111/0468 | LOSS: 2.2684 | ACC 0.2070\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0112/0468 | LOSS: 2.2681 | ACC 0.2081\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0113/0468 | LOSS: 2.2678 | ACC 0.2082\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0114/0468 | LOSS: 2.2674 | ACC 0.2093\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0115/0468 | LOSS: 2.2671 | ACC 0.2094\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0116/0468 | LOSS: 2.2667 | ACC 0.2103\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0117/0468 | LOSS: 2.2663 | ACC 0.2111\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0118/0468 | LOSS: 2.2659 | ACC 0.2119\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0119/0468 | LOSS: 2.2656 | ACC 0.2126\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0120/0468 | LOSS: 2.2652 | ACC 0.2136\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0121/0468 | LOSS: 2.2648 | ACC 0.2139\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0122/0468 | LOSS: 2.2644 | ACC 0.2151\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0123/0468 | LOSS: 2.2641 | ACC 0.2157\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0124/0468 | LOSS: 2.2637 | ACC 0.2167\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0125/0468 | LOSS: 2.2633 | ACC 0.2180\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0126/0468 | LOSS: 2.2628 | ACC 0.2191\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0127/0468 | LOSS: 2.2623 | ACC 0.2205\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0128/0468 | LOSS: 2.2618 | ACC 0.2215\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0129/0468 | LOSS: 2.2614 | ACC 0.2227\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0130/0468 | LOSS: 2.2609 | ACC 0.2242\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0131/0468 | LOSS: 2.2605 | ACC 0.2247\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0132/0468 | LOSS: 2.2601 | ACC 0.2257\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0133/0468 | LOSS: 2.2595 | ACC 0.2276\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0134/0468 | LOSS: 2.2590 | ACC 0.2289\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0135/0468 | LOSS: 2.2586 | ACC 0.2299\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0136/0468 | LOSS: 2.2581 | ACC 0.2311\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0137/0468 | LOSS: 2.2576 | ACC 0.2321\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0138/0468 | LOSS: 2.2571 | ACC 0.2331\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0139/0468 | LOSS: 2.2567 | ACC 0.2344\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0140/0468 | LOSS: 2.2562 | ACC 0.2353\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0141/0468 | LOSS: 2.2559 | ACC 0.2356\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0142/0468 | LOSS: 2.2553 | ACC 0.2368\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0143/0468 | LOSS: 2.2548 | ACC 0.2380\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0144/0468 | LOSS: 2.2544 | ACC 0.2388\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0145/0468 | LOSS: 2.2539 | ACC 0.2397\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0146/0468 | LOSS: 2.2534 | ACC 0.2408\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0147/0468 | LOSS: 2.2530 | ACC 0.2417\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0148/0468 | LOSS: 2.2524 | ACC 0.2423\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0149/0468 | LOSS: 2.2519 | ACC 0.2433\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0150/0468 | LOSS: 2.2515 | ACC 0.2444\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0151/0468 | LOSS: 2.2509 | ACC 0.2456\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0152/0468 | LOSS: 2.2504 | ACC 0.2469\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0153/0468 | LOSS: 2.2499 | ACC 0.2481\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0154/0468 | LOSS: 2.2494 | ACC 0.2496\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0155/0468 | LOSS: 2.2488 | ACC 0.2504\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0156/0468 | LOSS: 2.2482 | ACC 0.2515\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0157/0468 | LOSS: 2.2477 | ACC 0.2523\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0158/0468 | LOSS: 2.2471 | ACC 0.2535\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0159/0468 | LOSS: 2.2465 | ACC 0.2547\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0160/0468 | LOSS: 2.2460 | ACC 0.2561\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0161/0468 | LOSS: 2.2454 | ACC 0.2575\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0162/0468 | LOSS: 2.2448 | ACC 0.2587\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0163/0468 | LOSS: 2.2443 | ACC 0.2602\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0164/0468 | LOSS: 2.2436 | ACC 0.2617\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0165/0468 | LOSS: 2.2429 | ACC 0.2628\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0166/0468 | LOSS: 2.2422 | ACC 0.2641\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0167/0468 | LOSS: 2.2415 | ACC 0.2653\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0168/0468 | LOSS: 2.2409 | ACC 0.2662\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0169/0468 | LOSS: 2.2404 | ACC 0.2673\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0170/0468 | LOSS: 2.2397 | ACC 0.2686\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0171/0468 | LOSS: 2.2390 | ACC 0.2700\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0172/0468 | LOSS: 2.2384 | ACC 0.2716\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0173/0468 | LOSS: 2.2377 | ACC 0.2727\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0174/0468 | LOSS: 2.2370 | ACC 0.2738\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0175/0468 | LOSS: 2.2363 | ACC 0.2748\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0176/0468 | LOSS: 2.2356 | ACC 0.2763\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0177/0468 | LOSS: 2.2349 | ACC 0.2773\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0178/0468 | LOSS: 2.2342 | ACC 0.2782\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0179/0468 | LOSS: 2.2335 | ACC 0.2795\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0180/0468 | LOSS: 2.2329 | ACC 0.2803\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0181/0468 | LOSS: 2.2321 | ACC 0.2813\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0182/0468 | LOSS: 2.2314 | ACC 0.2826\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0183/0468 | LOSS: 2.2307 | ACC 0.2838\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0184/0468 | LOSS: 2.2299 | ACC 0.2853\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0185/0468 | LOSS: 2.2291 | ACC 0.2866\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0186/0468 | LOSS: 2.2283 | ACC 0.2878\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0187/0468 | LOSS: 2.2277 | ACC 0.2887\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0188/0468 | LOSS: 2.2268 | ACC 0.2901\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0189/0468 | LOSS: 2.2262 | ACC 0.2910\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0190/0468 | LOSS: 2.2254 | ACC 0.2922\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0191/0468 | LOSS: 2.2245 | ACC 0.2933\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0192/0468 | LOSS: 2.2235 | ACC 0.2948\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0193/0468 | LOSS: 2.2228 | ACC 0.2961\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0194/0468 | LOSS: 2.2220 | ACC 0.2972\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0195/0468 | LOSS: 2.2211 | ACC 0.2987\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0196/0468 | LOSS: 2.2202 | ACC 0.3000\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0197/0468 | LOSS: 2.2193 | ACC 0.3013\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0198/0468 | LOSS: 2.2184 | ACC 0.3023\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0199/0468 | LOSS: 2.2175 | ACC 0.3034\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0200/0468 | LOSS: 2.2165 | ACC 0.3048\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0201/0468 | LOSS: 2.2156 | ACC 0.3060\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0202/0468 | LOSS: 2.2146 | ACC 0.3071\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0203/0468 | LOSS: 2.2137 | ACC 0.3080\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0204/0468 | LOSS: 2.2127 | ACC 0.3091\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0205/0468 | LOSS: 2.2118 | ACC 0.3105\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0206/0468 | LOSS: 2.2107 | ACC 0.3114\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0207/0468 | LOSS: 2.2098 | ACC 0.3123\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0208/0468 | LOSS: 2.2089 | ACC 0.3133\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0209/0468 | LOSS: 2.2076 | ACC 0.3147\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0210/0468 | LOSS: 2.2065 | ACC 0.3158\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0211/0468 | LOSS: 2.2052 | ACC 0.3170\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0212/0468 | LOSS: 2.2042 | ACC 0.3181\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0213/0468 | LOSS: 2.2029 | ACC 0.3193\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0214/0468 | LOSS: 2.2018 | ACC 0.3203\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0215/0468 | LOSS: 2.2006 | ACC 0.3210\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0216/0468 | LOSS: 2.1995 | ACC 0.3220\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0217/0468 | LOSS: 2.1985 | ACC 0.3226\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0218/0468 | LOSS: 2.1976 | ACC 0.3232\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0219/0468 | LOSS: 2.1962 | ACC 0.3245\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0220/0468 | LOSS: 2.1952 | ACC 0.3253\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0221/0468 | LOSS: 2.1941 | ACC 0.3263\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0222/0468 | LOSS: 2.1927 | ACC 0.3274\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0223/0468 | LOSS: 2.1916 | ACC 0.3280\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0224/0468 | LOSS: 2.1904 | ACC 0.3290\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0225/0468 | LOSS: 2.1893 | ACC 0.3295\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0226/0468 | LOSS: 2.1881 | ACC 0.3303\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0227/0468 | LOSS: 2.1870 | ACC 0.3308\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0228/0468 | LOSS: 2.1858 | ACC 0.3314\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0229/0468 | LOSS: 2.1844 | ACC 0.3323\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0230/0468 | LOSS: 2.1829 | ACC 0.3335\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0231/0468 | LOSS: 2.1815 | ACC 0.3343\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0232/0468 | LOSS: 2.1804 | ACC 0.3348\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0233/0468 | LOSS: 2.1789 | ACC 0.3358\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0234/0468 | LOSS: 2.1776 | ACC 0.3367\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0235/0468 | LOSS: 2.1761 | ACC 0.3376\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0236/0468 | LOSS: 2.1747 | ACC 0.3381\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0237/0468 | LOSS: 2.1734 | ACC 0.3390\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0238/0468 | LOSS: 2.1721 | ACC 0.3396\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0239/0468 | LOSS: 2.1706 | ACC 0.3404\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0240/0468 | LOSS: 2.1691 | ACC 0.3414\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0241/0468 | LOSS: 2.1677 | ACC 0.3421\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0242/0468 | LOSS: 2.1662 | ACC 0.3428\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0243/0468 | LOSS: 2.1648 | ACC 0.3438\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0244/0468 | LOSS: 2.1632 | ACC 0.3445\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0245/0468 | LOSS: 2.1617 | ACC 0.3456\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0246/0468 | LOSS: 2.1602 | ACC 0.3465\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0247/0468 | LOSS: 2.1584 | ACC 0.3474\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0248/0468 | LOSS: 2.1569 | ACC 0.3480\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0249/0468 | LOSS: 2.1552 | ACC 0.3489\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0250/0468 | LOSS: 2.1537 | ACC 0.3496\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0251/0468 | LOSS: 2.1518 | ACC 0.3506\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0252/0468 | LOSS: 2.1503 | ACC 0.3513\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0253/0468 | LOSS: 2.1485 | ACC 0.3522\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0254/0468 | LOSS: 2.1467 | ACC 0.3530\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0255/0468 | LOSS: 2.1448 | ACC 0.3540\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0256/0468 | LOSS: 2.1429 | ACC 0.3549\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0257/0468 | LOSS: 2.1409 | ACC 0.3556\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0258/0468 | LOSS: 2.1389 | ACC 0.3564\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0259/0468 | LOSS: 2.1372 | ACC 0.3573\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0260/0468 | LOSS: 2.1352 | ACC 0.3582\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0261/0468 | LOSS: 2.1332 | ACC 0.3591\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0262/0468 | LOSS: 2.1313 | ACC 0.3600\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0263/0468 | LOSS: 2.1295 | ACC 0.3606\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0264/0468 | LOSS: 2.1273 | ACC 0.3615\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0265/0468 | LOSS: 2.1253 | ACC 0.3622\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0266/0468 | LOSS: 2.1236 | ACC 0.3627\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0267/0468 | LOSS: 2.1216 | ACC 0.3637\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0268/0468 | LOSS: 2.1197 | ACC 0.3646\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0269/0468 | LOSS: 2.1177 | ACC 0.3654\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0270/0468 | LOSS: 2.1152 | ACC 0.3666\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0271/0468 | LOSS: 2.1132 | ACC 0.3677\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0272/0468 | LOSS: 2.1109 | ACC 0.3687\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0273/0468 | LOSS: 2.1088 | ACC 0.3696\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0274/0468 | LOSS: 2.1064 | ACC 0.3705\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0275/0468 | LOSS: 2.1041 | ACC 0.3715\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0276/0468 | LOSS: 2.1021 | ACC 0.3723\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0277/0468 | LOSS: 2.1000 | ACC 0.3733\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0278/0468 | LOSS: 2.0978 | ACC 0.3742\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0279/0468 | LOSS: 2.0959 | ACC 0.3751\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0280/0468 | LOSS: 2.0939 | ACC 0.3758\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0281/0468 | LOSS: 2.0916 | ACC 0.3767\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0282/0468 | LOSS: 2.0893 | ACC 0.3778\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0283/0468 | LOSS: 2.0873 | ACC 0.3784\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0284/0468 | LOSS: 2.0853 | ACC 0.3789\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0285/0468 | LOSS: 2.0830 | ACC 0.3799\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0286/0468 | LOSS: 2.0807 | ACC 0.3808\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0287/0468 | LOSS: 2.0787 | ACC 0.3814\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0288/0468 | LOSS: 2.0765 | ACC 0.3823\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0289/0468 | LOSS: 2.0742 | ACC 0.3832\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0290/0468 | LOSS: 2.0716 | ACC 0.3842\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0291/0468 | LOSS: 2.0694 | ACC 0.3851\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0292/0468 | LOSS: 2.0669 | ACC 0.3860\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0293/0468 | LOSS: 2.0648 | ACC 0.3866\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0294/0468 | LOSS: 2.0624 | ACC 0.3875\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0295/0468 | LOSS: 2.0601 | ACC 0.3884\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0296/0468 | LOSS: 2.0577 | ACC 0.3893\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0297/0468 | LOSS: 2.0550 | ACC 0.3904\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0298/0468 | LOSS: 2.0523 | ACC 0.3915\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0299/0468 | LOSS: 2.0503 | ACC 0.3920\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0300/0468 | LOSS: 2.0476 | ACC 0.3931\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0301/0468 | LOSS: 2.0453 | ACC 0.3940\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0302/0468 | LOSS: 2.0430 | ACC 0.3949\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0303/0468 | LOSS: 2.0406 | ACC 0.3959\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0304/0468 | LOSS: 2.0379 | ACC 0.3967\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0305/0468 | LOSS: 2.0356 | ACC 0.3976\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0306/0468 | LOSS: 2.0332 | ACC 0.3983\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0307/0468 | LOSS: 2.0307 | ACC 0.3991\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0308/0468 | LOSS: 2.0284 | ACC 0.4001\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0309/0468 | LOSS: 2.0259 | ACC 0.4010\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0310/0468 | LOSS: 2.0236 | ACC 0.4020\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0311/0468 | LOSS: 2.0210 | ACC 0.4028\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0312/0468 | LOSS: 2.0183 | ACC 0.4038\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0313/0468 | LOSS: 2.0159 | ACC 0.4048\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0314/0468 | LOSS: 2.0135 | ACC 0.4057\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0315/0468 | LOSS: 2.0111 | ACC 0.4066\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0316/0468 | LOSS: 2.0084 | ACC 0.4076\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0317/0468 | LOSS: 2.0062 | ACC 0.4084\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0318/0468 | LOSS: 2.0035 | ACC 0.4094\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0319/0468 | LOSS: 2.0007 | ACC 0.4105\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0320/0468 | LOSS: 1.9981 | ACC 0.4115\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0321/0468 | LOSS: 1.9956 | ACC 0.4123\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0322/0468 | LOSS: 1.9932 | ACC 0.4131\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0323/0468 | LOSS: 1.9906 | ACC 0.4140\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0324/0468 | LOSS: 1.9878 | ACC 0.4151\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0325/0468 | LOSS: 1.9850 | ACC 0.4161\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0326/0468 | LOSS: 1.9824 | ACC 0.4170\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0327/0468 | LOSS: 1.9796 | ACC 0.4181\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0328/0468 | LOSS: 1.9770 | ACC 0.4191\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0329/0468 | LOSS: 1.9743 | ACC 0.4199\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0330/0468 | LOSS: 1.9719 | ACC 0.4207\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0331/0468 | LOSS: 1.9694 | ACC 0.4217\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0332/0468 | LOSS: 1.9669 | ACC 0.4226\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0333/0468 | LOSS: 1.9643 | ACC 0.4234\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0334/0468 | LOSS: 1.9616 | ACC 0.4243\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0335/0468 | LOSS: 1.9590 | ACC 0.4252\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0336/0468 | LOSS: 1.9563 | ACC 0.4261\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0337/0468 | LOSS: 1.9536 | ACC 0.4270\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0338/0468 | LOSS: 1.9510 | ACC 0.4278\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0339/0468 | LOSS: 1.9485 | ACC 0.4289\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0340/0468 | LOSS: 1.9460 | ACC 0.4299\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0341/0468 | LOSS: 1.9437 | ACC 0.4306\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0342/0468 | LOSS: 1.9408 | ACC 0.4316\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0343/0468 | LOSS: 1.9379 | ACC 0.4326\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0344/0468 | LOSS: 1.9353 | ACC 0.4334\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0345/0468 | LOSS: 1.9327 | ACC 0.4342\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0346/0468 | LOSS: 1.9303 | ACC 0.4350\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0347/0468 | LOSS: 1.9279 | ACC 0.4357\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0348/0468 | LOSS: 1.9252 | ACC 0.4367\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0349/0468 | LOSS: 1.9225 | ACC 0.4375\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0350/0468 | LOSS: 1.9200 | ACC 0.4383\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0351/0468 | LOSS: 1.9173 | ACC 0.4392\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0352/0468 | LOSS: 1.9148 | ACC 0.4399\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0353/0468 | LOSS: 1.9121 | ACC 0.4408\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0354/0468 | LOSS: 1.9092 | ACC 0.4416\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0355/0468 | LOSS: 1.9066 | ACC 0.4425\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0356/0468 | LOSS: 1.9040 | ACC 0.4433\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0357/0468 | LOSS: 1.9009 | ACC 0.4443\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0358/0468 | LOSS: 1.8982 | ACC 0.4452\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0359/0468 | LOSS: 1.8954 | ACC 0.4462\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0360/0468 | LOSS: 1.8929 | ACC 0.4470\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0361/0468 | LOSS: 1.8902 | ACC 0.4477\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0362/0468 | LOSS: 1.8877 | ACC 0.4484\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0363/0468 | LOSS: 1.8850 | ACC 0.4492\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0364/0468 | LOSS: 1.8824 | ACC 0.4500\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0365/0468 | LOSS: 1.8798 | ACC 0.4507\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0366/0468 | LOSS: 1.8772 | ACC 0.4515\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0367/0468 | LOSS: 1.8744 | ACC 0.4525\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0368/0468 | LOSS: 1.8717 | ACC 0.4533\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0369/0468 | LOSS: 1.8690 | ACC 0.4542\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0370/0468 | LOSS: 1.8663 | ACC 0.4550\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0371/0468 | LOSS: 1.8635 | ACC 0.4559\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0372/0468 | LOSS: 1.8606 | ACC 0.4568\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0373/0468 | LOSS: 1.8580 | ACC 0.4577\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0374/0468 | LOSS: 1.8553 | ACC 0.4586\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0375/0468 | LOSS: 1.8527 | ACC 0.4595\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0376/0468 | LOSS: 1.8498 | ACC 0.4603\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0377/0468 | LOSS: 1.8473 | ACC 0.4610\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0378/0468 | LOSS: 1.8447 | ACC 0.4618\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0379/0468 | LOSS: 1.8419 | ACC 0.4627\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0380/0468 | LOSS: 1.8391 | ACC 0.4636\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0381/0468 | LOSS: 1.8363 | ACC 0.4644\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0382/0468 | LOSS: 1.8339 | ACC 0.4653\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0383/0468 | LOSS: 1.8319 | ACC 0.4658\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0384/0468 | LOSS: 1.8296 | ACC 0.4665\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0385/0468 | LOSS: 1.8271 | ACC 0.4672\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0386/0468 | LOSS: 1.8244 | ACC 0.4680\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0387/0468 | LOSS: 1.8217 | ACC 0.4689\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0388/0468 | LOSS: 1.8191 | ACC 0.4697\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0389/0468 | LOSS: 1.8164 | ACC 0.4705\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0390/0468 | LOSS: 1.8140 | ACC 0.4713\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0391/0468 | LOSS: 1.8114 | ACC 0.4721\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0392/0468 | LOSS: 1.8090 | ACC 0.4728\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0393/0468 | LOSS: 1.8066 | ACC 0.4735\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0394/0468 | LOSS: 1.8039 | ACC 0.4745\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0395/0468 | LOSS: 1.8013 | ACC 0.4753\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0396/0468 | LOSS: 1.7988 | ACC 0.4761\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0397/0468 | LOSS: 1.7967 | ACC 0.4767\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0398/0468 | LOSS: 1.7941 | ACC 0.4774\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0399/0468 | LOSS: 1.7916 | ACC 0.4781\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0400/0468 | LOSS: 1.7891 | ACC 0.4790\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0401/0468 | LOSS: 1.7864 | ACC 0.4798\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0402/0468 | LOSS: 1.7838 | ACC 0.4805\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0403/0468 | LOSS: 1.7812 | ACC 0.4812\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0404/0468 | LOSS: 1.7788 | ACC 0.4820\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0405/0468 | LOSS: 1.7767 | ACC 0.4827\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0406/0468 | LOSS: 1.7743 | ACC 0.4834\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0407/0468 | LOSS: 1.7718 | ACC 0.4842\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0408/0468 | LOSS: 1.7691 | ACC 0.4850\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0409/0468 | LOSS: 1.7667 | ACC 0.4857\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0410/0468 | LOSS: 1.7642 | ACC 0.4864\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0411/0468 | LOSS: 1.7617 | ACC 0.4872\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0412/0468 | LOSS: 1.7593 | ACC 0.4879\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0413/0468 | LOSS: 1.7568 | ACC 0.4885\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0414/0468 | LOSS: 1.7543 | ACC 0.4893\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0415/0468 | LOSS: 1.7517 | ACC 0.4902\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0416/0468 | LOSS: 1.7492 | ACC 0.4908\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0417/0468 | LOSS: 1.7472 | ACC 0.4914\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0418/0468 | LOSS: 1.7448 | ACC 0.4921\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0419/0468 | LOSS: 1.7426 | ACC 0.4927\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0420/0468 | LOSS: 1.7403 | ACC 0.4934\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0421/0468 | LOSS: 1.7381 | ACC 0.4941\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0422/0468 | LOSS: 1.7353 | ACC 0.4950\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0423/0468 | LOSS: 1.7327 | ACC 0.4958\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0424/0468 | LOSS: 1.7302 | ACC 0.4966\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0425/0468 | LOSS: 1.7279 | ACC 0.4973\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0426/0468 | LOSS: 1.7254 | ACC 0.4981\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0427/0468 | LOSS: 1.7232 | ACC 0.4989\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0428/0468 | LOSS: 1.7208 | ACC 0.4996\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0429/0468 | LOSS: 1.7185 | ACC 0.5003\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0430/0468 | LOSS: 1.7159 | ACC 0.5010\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0431/0468 | LOSS: 1.7142 | ACC 0.5016\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0432/0468 | LOSS: 1.7118 | ACC 0.5023\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0433/0468 | LOSS: 1.7096 | ACC 0.5028\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0434/0468 | LOSS: 1.7072 | ACC 0.5034\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0435/0468 | LOSS: 1.7049 | ACC 0.5040\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0436/0468 | LOSS: 1.7029 | ACC 0.5046\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0437/0468 | LOSS: 1.7007 | ACC 0.5052\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0438/0468 | LOSS: 1.6984 | ACC 0.5060\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0439/0468 | LOSS: 1.6964 | ACC 0.5066\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0440/0468 | LOSS: 1.6941 | ACC 0.5073\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0441/0468 | LOSS: 1.6920 | ACC 0.5078\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0442/0468 | LOSS: 1.6895 | ACC 0.5086\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0443/0468 | LOSS: 1.6872 | ACC 0.5092\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0444/0468 | LOSS: 1.6849 | ACC 0.5098\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0445/0468 | LOSS: 1.6825 | ACC 0.5105\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0446/0468 | LOSS: 1.6801 | ACC 0.5112\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0447/0468 | LOSS: 1.6778 | ACC 0.5118\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0448/0468 | LOSS: 1.6755 | ACC 0.5126\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0449/0468 | LOSS: 1.6732 | ACC 0.5132\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0450/0468 | LOSS: 1.6708 | ACC 0.5140\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0451/0468 | LOSS: 1.6687 | ACC 0.5147\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0452/0468 | LOSS: 1.6666 | ACC 0.5153\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0453/0468 | LOSS: 1.6641 | ACC 0.5161\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0454/0468 | LOSS: 1.6617 | ACC 0.5167\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0455/0468 | LOSS: 1.6594 | ACC 0.5174\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0456/0468 | LOSS: 1.6574 | ACC 0.5180\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0457/0468 | LOSS: 1.6552 | ACC 0.5186\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0458/0468 | LOSS: 1.6532 | ACC 0.5191\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0459/0468 | LOSS: 1.6512 | ACC 0.5197\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0460/0468 | LOSS: 1.6490 | ACC 0.5203\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0461/0468 | LOSS: 1.6471 | ACC 0.5207\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0462/0468 | LOSS: 1.6452 | ACC 0.5213\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0463/0468 | LOSS: 1.6434 | ACC 0.5218\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0464/0468 | LOSS: 1.6412 | ACC 0.5224\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0465/0468 | LOSS: 1.6388 | ACC 0.5231\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0466/0468 | LOSS: 1.6366 | ACC 0.5237\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0467/0468 | LOSS: 1.6344 | ACC 0.5243\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0468/0468 | LOSS: 1.6322 | ACC 0.5250\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0000/0468 | LOSS: 0.6154 | ACC 0.7969\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0001/0468 | LOSS: 0.7200 | ACC 0.7734\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0002/0468 | LOSS: 0.6978 | ACC 0.7969\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0003/0468 | LOSS: 0.6691 | ACC 0.8145\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0004/0468 | LOSS: 0.6530 | ACC 0.8125\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0005/0468 | LOSS: 0.6362 | ACC 0.8125\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0006/0468 | LOSS: 0.6301 | ACC 0.8114\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0007/0468 | LOSS: 0.6109 | ACC 0.8174\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0008/0468 | LOSS: 0.6061 | ACC 0.8212\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0009/0468 | LOSS: 0.6197 | ACC 0.8211\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0010/0468 | LOSS: 0.6125 | ACC 0.8260\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0011/0468 | LOSS: 0.6145 | ACC 0.8197\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0012/0468 | LOSS: 0.6171 | ACC 0.8173\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0013/0468 | LOSS: 0.6109 | ACC 0.8214\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0014/0468 | LOSS: 0.6121 | ACC 0.8187\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0015/0468 | LOSS: 0.6105 | ACC 0.8188\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0016/0468 | LOSS: 0.6105 | ACC 0.8217\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0017/0468 | LOSS: 0.6125 | ACC 0.8212\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0018/0468 | LOSS: 0.6091 | ACC 0.8220\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0019/0468 | LOSS: 0.6091 | ACC 0.8238\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0020/0468 | LOSS: 0.6147 | ACC 0.8214\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0021/0468 | LOSS: 0.6148 | ACC 0.8200\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0022/0468 | LOSS: 0.6130 | ACC 0.8220\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0023/0468 | LOSS: 0.6071 | ACC 0.8242\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0024/0468 | LOSS: 0.6086 | ACC 0.8225\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0025/0468 | LOSS: 0.6048 | ACC 0.8242\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0026/0468 | LOSS: 0.6024 | ACC 0.8258\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0027/0468 | LOSS: 0.6029 | ACC 0.8259\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0028/0468 | LOSS: 0.6009 | ACC 0.8270\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0029/0468 | LOSS: 0.6016 | ACC 0.8266\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0030/0468 | LOSS: 0.6024 | ACC 0.8266\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0031/0468 | LOSS: 0.6025 | ACC 0.8262\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0032/0468 | LOSS: 0.6007 | ACC 0.8262\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0033/0468 | LOSS: 0.5999 | ACC 0.8274\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0034/0468 | LOSS: 0.6030 | ACC 0.8261\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0035/0468 | LOSS: 0.6056 | ACC 0.8249\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0036/0468 | LOSS: 0.6015 | ACC 0.8264\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0037/0468 | LOSS: 0.5999 | ACC 0.8269\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0038/0468 | LOSS: 0.5972 | ACC 0.8279\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0039/0468 | LOSS: 0.5950 | ACC 0.8291\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0040/0468 | LOSS: 0.5966 | ACC 0.8281\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0041/0468 | LOSS: 0.5969 | ACC 0.8278\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0042/0468 | LOSS: 0.5974 | ACC 0.8270\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0043/0468 | LOSS: 0.5974 | ACC 0.8269\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0044/0468 | LOSS: 0.5982 | ACC 0.8271\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0045/0468 | LOSS: 0.5988 | ACC 0.8269\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0046/0468 | LOSS: 0.5992 | ACC 0.8273\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0047/0468 | LOSS: 0.5965 | ACC 0.8281\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0048/0468 | LOSS: 0.5949 | ACC 0.8288\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0049/0468 | LOSS: 0.5933 | ACC 0.8292\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0050/0468 | LOSS: 0.5943 | ACC 0.8283\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0051/0468 | LOSS: 0.5913 | ACC 0.8293\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0052/0468 | LOSS: 0.5901 | ACC 0.8290\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0053/0468 | LOSS: 0.5882 | ACC 0.8293\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0054/0468 | LOSS: 0.5893 | ACC 0.8287\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0055/0468 | LOSS: 0.5905 | ACC 0.8284\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0056/0468 | LOSS: 0.5894 | ACC 0.8294\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0057/0468 | LOSS: 0.5893 | ACC 0.8287\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0058/0468 | LOSS: 0.5887 | ACC 0.8291\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0059/0468 | LOSS: 0.5890 | ACC 0.8289\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0060/0468 | LOSS: 0.5890 | ACC 0.8283\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0061/0468 | LOSS: 0.5892 | ACC 0.8286\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0062/0468 | LOSS: 0.5900 | ACC 0.8289\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0063/0468 | LOSS: 0.5890 | ACC 0.8289\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0064/0468 | LOSS: 0.5886 | ACC 0.8292\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0065/0468 | LOSS: 0.5860 | ACC 0.8304\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0066/0468 | LOSS: 0.5842 | ACC 0.8309\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0067/0468 | LOSS: 0.5837 | ACC 0.8310\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0068/0468 | LOSS: 0.5836 | ACC 0.8310\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0069/0468 | LOSS: 0.5838 | ACC 0.8302\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0070/0468 | LOSS: 0.5814 | ACC 0.8313\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0071/0468 | LOSS: 0.5799 | ACC 0.8317\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0072/0468 | LOSS: 0.5805 | ACC 0.8312\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0073/0468 | LOSS: 0.5807 | ACC 0.8314\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0074/0468 | LOSS: 0.5813 | ACC 0.8313\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0075/0468 | LOSS: 0.5797 | ACC 0.8318\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0076/0468 | LOSS: 0.5788 | ACC 0.8321\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0077/0468 | LOSS: 0.5780 | ACC 0.8323\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0078/0468 | LOSS: 0.5773 | ACC 0.8326\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0079/0468 | LOSS: 0.5748 | ACC 0.8337\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0080/0468 | LOSS: 0.5734 | ACC 0.8340\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0081/0468 | LOSS: 0.5733 | ACC 0.8340\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0082/0468 | LOSS: 0.5728 | ACC 0.8341\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0083/0468 | LOSS: 0.5719 | ACC 0.8347\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0084/0468 | LOSS: 0.5717 | ACC 0.8347\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0085/0468 | LOSS: 0.5716 | ACC 0.8347\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0086/0468 | LOSS: 0.5729 | ACC 0.8342\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0087/0468 | LOSS: 0.5734 | ACC 0.8343\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0088/0468 | LOSS: 0.5731 | ACC 0.8349\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0089/0468 | LOSS: 0.5708 | ACC 0.8359\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0090/0468 | LOSS: 0.5703 | ACC 0.8359\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0091/0468 | LOSS: 0.5698 | ACC 0.8361\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0092/0468 | LOSS: 0.5704 | ACC 0.8356\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0093/0468 | LOSS: 0.5688 | ACC 0.8359\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0094/0468 | LOSS: 0.5685 | ACC 0.8359\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0095/0468 | LOSS: 0.5682 | ACC 0.8362\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0096/0468 | LOSS: 0.5679 | ACC 0.8359\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0097/0468 | LOSS: 0.5678 | ACC 0.8355\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0098/0468 | LOSS: 0.5694 | ACC 0.8348\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0099/0468 | LOSS: 0.5683 | ACC 0.8353\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0100/0468 | LOSS: 0.5690 | ACC 0.8353\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0101/0468 | LOSS: 0.5686 | ACC 0.8352\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0102/0468 | LOSS: 0.5683 | ACC 0.8355\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0103/0468 | LOSS: 0.5663 | ACC 0.8365\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0104/0468 | LOSS: 0.5663 | ACC 0.8363\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0105/0468 | LOSS: 0.5645 | ACC 0.8363\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0106/0468 | LOSS: 0.5638 | ACC 0.8363\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0107/0468 | LOSS: 0.5631 | ACC 0.8364\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0108/0468 | LOSS: 0.5631 | ACC 0.8363\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0109/0468 | LOSS: 0.5626 | ACC 0.8363\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0110/0468 | LOSS: 0.5617 | ACC 0.8366\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0111/0468 | LOSS: 0.5604 | ACC 0.8373\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0112/0468 | LOSS: 0.5598 | ACC 0.8373\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0113/0468 | LOSS: 0.5587 | ACC 0.8375\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0114/0468 | LOSS: 0.5577 | ACC 0.8380\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0115/0468 | LOSS: 0.5573 | ACC 0.8381\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0116/0468 | LOSS: 0.5571 | ACC 0.8383\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0117/0468 | LOSS: 0.5572 | ACC 0.8385\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0118/0468 | LOSS: 0.5564 | ACC 0.8386\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0119/0468 | LOSS: 0.5568 | ACC 0.8380\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0120/0468 | LOSS: 0.5564 | ACC 0.8379\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0121/0468 | LOSS: 0.5557 | ACC 0.8384\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0122/0468 | LOSS: 0.5557 | ACC 0.8385\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0123/0468 | LOSS: 0.5552 | ACC 0.8387\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0124/0468 | LOSS: 0.5547 | ACC 0.8386\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0125/0468 | LOSS: 0.5531 | ACC 0.8393\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0126/0468 | LOSS: 0.5525 | ACC 0.8393\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0127/0468 | LOSS: 0.5526 | ACC 0.8394\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0128/0468 | LOSS: 0.5521 | ACC 0.8395\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0129/0468 | LOSS: 0.5513 | ACC 0.8398\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0130/0468 | LOSS: 0.5504 | ACC 0.8400\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0131/0468 | LOSS: 0.5492 | ACC 0.8401\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0132/0468 | LOSS: 0.5489 | ACC 0.8402\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0133/0468 | LOSS: 0.5489 | ACC 0.8404\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0134/0468 | LOSS: 0.5492 | ACC 0.8403\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0135/0468 | LOSS: 0.5489 | ACC 0.8404\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0136/0468 | LOSS: 0.5482 | ACC 0.8405\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0137/0468 | LOSS: 0.5476 | ACC 0.8406\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0138/0468 | LOSS: 0.5462 | ACC 0.8410\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0139/0468 | LOSS: 0.5455 | ACC 0.8413\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0140/0468 | LOSS: 0.5455 | ACC 0.8414\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0141/0468 | LOSS: 0.5449 | ACC 0.8415\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0142/0468 | LOSS: 0.5448 | ACC 0.8416\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0143/0468 | LOSS: 0.5442 | ACC 0.8418\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0144/0468 | LOSS: 0.5435 | ACC 0.8421\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0145/0468 | LOSS: 0.5425 | ACC 0.8425\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0146/0468 | LOSS: 0.5420 | ACC 0.8425\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0147/0468 | LOSS: 0.5421 | ACC 0.8424\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0148/0468 | LOSS: 0.5410 | ACC 0.8428\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0149/0468 | LOSS: 0.5405 | ACC 0.8428\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0150/0468 | LOSS: 0.5397 | ACC 0.8431\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0151/0468 | LOSS: 0.5387 | ACC 0.8433\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0152/0468 | LOSS: 0.5385 | ACC 0.8435\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0153/0468 | LOSS: 0.5377 | ACC 0.8440\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0154/0468 | LOSS: 0.5365 | ACC 0.8445\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0155/0468 | LOSS: 0.5351 | ACC 0.8449\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0156/0468 | LOSS: 0.5341 | ACC 0.8450\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0157/0468 | LOSS: 0.5330 | ACC 0.8453\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0158/0468 | LOSS: 0.5332 | ACC 0.8451\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0159/0468 | LOSS: 0.5331 | ACC 0.8452\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0160/0468 | LOSS: 0.5330 | ACC 0.8450\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0161/0468 | LOSS: 0.5325 | ACC 0.8450\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0162/0468 | LOSS: 0.5328 | ACC 0.8450\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0163/0468 | LOSS: 0.5320 | ACC 0.8451\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0164/0468 | LOSS: 0.5314 | ACC 0.8453\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0165/0468 | LOSS: 0.5315 | ACC 0.8452\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0166/0468 | LOSS: 0.5315 | ACC 0.8451\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0167/0468 | LOSS: 0.5307 | ACC 0.8451\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0168/0468 | LOSS: 0.5315 | ACC 0.8448\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0169/0468 | LOSS: 0.5308 | ACC 0.8449\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0170/0468 | LOSS: 0.5304 | ACC 0.8451\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0171/0468 | LOSS: 0.5296 | ACC 0.8454\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0172/0468 | LOSS: 0.5296 | ACC 0.8452\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0173/0468 | LOSS: 0.5291 | ACC 0.8451\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0174/0468 | LOSS: 0.5279 | ACC 0.8455\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0175/0468 | LOSS: 0.5278 | ACC 0.8454\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0176/0468 | LOSS: 0.5272 | ACC 0.8454\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0177/0468 | LOSS: 0.5266 | ACC 0.8455\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0178/0468 | LOSS: 0.5254 | ACC 0.8457\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0179/0468 | LOSS: 0.5252 | ACC 0.8460\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0180/0468 | LOSS: 0.5245 | ACC 0.8462\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0181/0468 | LOSS: 0.5250 | ACC 0.8459\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0182/0468 | LOSS: 0.5245 | ACC 0.8461\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0183/0468 | LOSS: 0.5240 | ACC 0.8460\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0184/0468 | LOSS: 0.5235 | ACC 0.8462\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0185/0468 | LOSS: 0.5233 | ACC 0.8465\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0186/0468 | LOSS: 0.5224 | ACC 0.8467\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0187/0468 | LOSS: 0.5215 | ACC 0.8471\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0188/0468 | LOSS: 0.5209 | ACC 0.8473\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0189/0468 | LOSS: 0.5205 | ACC 0.8475\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0190/0468 | LOSS: 0.5199 | ACC 0.8477\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0191/0468 | LOSS: 0.5191 | ACC 0.8477\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0192/0468 | LOSS: 0.5181 | ACC 0.8478\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0193/0468 | LOSS: 0.5175 | ACC 0.8482\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0194/0468 | LOSS: 0.5169 | ACC 0.8485\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0195/0468 | LOSS: 0.5162 | ACC 0.8488\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0196/0468 | LOSS: 0.5156 | ACC 0.8490\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0197/0468 | LOSS: 0.5150 | ACC 0.8491\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0198/0468 | LOSS: 0.5145 | ACC 0.8491\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0199/0468 | LOSS: 0.5137 | ACC 0.8495\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0200/0468 | LOSS: 0.5131 | ACC 0.8497\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0201/0468 | LOSS: 0.5127 | ACC 0.8499\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0202/0468 | LOSS: 0.5119 | ACC 0.8500\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0203/0468 | LOSS: 0.5113 | ACC 0.8502\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0204/0468 | LOSS: 0.5111 | ACC 0.8502\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0205/0468 | LOSS: 0.5106 | ACC 0.8502\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0206/0468 | LOSS: 0.5107 | ACC 0.8502\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0207/0468 | LOSS: 0.5106 | ACC 0.8502\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0208/0468 | LOSS: 0.5099 | ACC 0.8504\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0209/0468 | LOSS: 0.5092 | ACC 0.8507\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0210/0468 | LOSS: 0.5091 | ACC 0.8507\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0211/0468 | LOSS: 0.5085 | ACC 0.8509\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0212/0468 | LOSS: 0.5079 | ACC 0.8510\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0213/0468 | LOSS: 0.5072 | ACC 0.8513\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0214/0468 | LOSS: 0.5072 | ACC 0.8513\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0215/0468 | LOSS: 0.5068 | ACC 0.8515\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0216/0468 | LOSS: 0.5063 | ACC 0.8516\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0217/0468 | LOSS: 0.5058 | ACC 0.8516\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0218/0468 | LOSS: 0.5051 | ACC 0.8518\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0219/0468 | LOSS: 0.5048 | ACC 0.8519\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0220/0468 | LOSS: 0.5052 | ACC 0.8518\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0221/0468 | LOSS: 0.5045 | ACC 0.8520\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0222/0468 | LOSS: 0.5042 | ACC 0.8521\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0223/0468 | LOSS: 0.5038 | ACC 0.8523\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0224/0468 | LOSS: 0.5031 | ACC 0.8526\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0225/0468 | LOSS: 0.5020 | ACC 0.8531\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0226/0468 | LOSS: 0.5017 | ACC 0.8531\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0227/0468 | LOSS: 0.5020 | ACC 0.8530\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0228/0468 | LOSS: 0.5020 | ACC 0.8530\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0229/0468 | LOSS: 0.5020 | ACC 0.8530\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0230/0468 | LOSS: 0.5011 | ACC 0.8534\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0231/0468 | LOSS: 0.5010 | ACC 0.8534\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0232/0468 | LOSS: 0.5006 | ACC 0.8537\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0233/0468 | LOSS: 0.4998 | ACC 0.8539\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0234/0468 | LOSS: 0.4991 | ACC 0.8542\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0235/0468 | LOSS: 0.4990 | ACC 0.8541\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0236/0468 | LOSS: 0.4989 | ACC 0.8541\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0237/0468 | LOSS: 0.4987 | ACC 0.8543\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0238/0468 | LOSS: 0.4986 | ACC 0.8543\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0239/0468 | LOSS: 0.4987 | ACC 0.8543\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0240/0468 | LOSS: 0.4986 | ACC 0.8544\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0241/0468 | LOSS: 0.4984 | ACC 0.8545\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0242/0468 | LOSS: 0.4981 | ACC 0.8546\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0243/0468 | LOSS: 0.4977 | ACC 0.8547\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0244/0468 | LOSS: 0.4968 | ACC 0.8548\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0245/0468 | LOSS: 0.4962 | ACC 0.8549\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0246/0468 | LOSS: 0.4960 | ACC 0.8549\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0247/0468 | LOSS: 0.4962 | ACC 0.8548\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0248/0468 | LOSS: 0.4957 | ACC 0.8549\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0249/0468 | LOSS: 0.4953 | ACC 0.8549\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0250/0468 | LOSS: 0.4949 | ACC 0.8550\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0251/0468 | LOSS: 0.4944 | ACC 0.8551\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0252/0468 | LOSS: 0.4937 | ACC 0.8553\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0253/0468 | LOSS: 0.4938 | ACC 0.8554\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0254/0468 | LOSS: 0.4933 | ACC 0.8556\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0255/0468 | LOSS: 0.4928 | ACC 0.8557\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0256/0468 | LOSS: 0.4928 | ACC 0.8556\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0257/0468 | LOSS: 0.4926 | ACC 0.8557\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0258/0468 | LOSS: 0.4921 | ACC 0.8558\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0259/0468 | LOSS: 0.4916 | ACC 0.8559\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0260/0468 | LOSS: 0.4916 | ACC 0.8559\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0261/0468 | LOSS: 0.4908 | ACC 0.8560\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0262/0468 | LOSS: 0.4904 | ACC 0.8562\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0263/0468 | LOSS: 0.4903 | ACC 0.8562\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0264/0468 | LOSS: 0.4896 | ACC 0.8565\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0265/0468 | LOSS: 0.4895 | ACC 0.8566\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0266/0468 | LOSS: 0.4894 | ACC 0.8566\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0267/0468 | LOSS: 0.4893 | ACC 0.8566\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0268/0468 | LOSS: 0.4888 | ACC 0.8568\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0269/0468 | LOSS: 0.4881 | ACC 0.8570\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0270/0468 | LOSS: 0.4873 | ACC 0.8573\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0271/0468 | LOSS: 0.4872 | ACC 0.8574\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0272/0468 | LOSS: 0.4864 | ACC 0.8576\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0273/0468 | LOSS: 0.4861 | ACC 0.8578\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0274/0468 | LOSS: 0.4859 | ACC 0.8579\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0275/0468 | LOSS: 0.4858 | ACC 0.8580\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0276/0468 | LOSS: 0.4857 | ACC 0.8578\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0277/0468 | LOSS: 0.4853 | ACC 0.8579\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0278/0468 | LOSS: 0.4847 | ACC 0.8581\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0279/0468 | LOSS: 0.4846 | ACC 0.8581\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0280/0468 | LOSS: 0.4840 | ACC 0.8582\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0281/0468 | LOSS: 0.4838 | ACC 0.8582\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0282/0468 | LOSS: 0.4836 | ACC 0.8582\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0283/0468 | LOSS: 0.4829 | ACC 0.8583\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0284/0468 | LOSS: 0.4826 | ACC 0.8584\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0285/0468 | LOSS: 0.4823 | ACC 0.8585\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0286/0468 | LOSS: 0.4820 | ACC 0.8586\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0287/0468 | LOSS: 0.4815 | ACC 0.8587\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0288/0468 | LOSS: 0.4810 | ACC 0.8589\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0289/0468 | LOSS: 0.4806 | ACC 0.8591\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0290/0468 | LOSS: 0.4800 | ACC 0.8593\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0291/0468 | LOSS: 0.4794 | ACC 0.8596\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0292/0468 | LOSS: 0.4787 | ACC 0.8597\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0293/0468 | LOSS: 0.4781 | ACC 0.8599\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0294/0468 | LOSS: 0.4778 | ACC 0.8600\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0295/0468 | LOSS: 0.4776 | ACC 0.8600\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0296/0468 | LOSS: 0.4773 | ACC 0.8600\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0297/0468 | LOSS: 0.4766 | ACC 0.8602\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0298/0468 | LOSS: 0.4762 | ACC 0.8603\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0299/0468 | LOSS: 0.4757 | ACC 0.8604\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0300/0468 | LOSS: 0.4748 | ACC 0.8608\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0301/0468 | LOSS: 0.4746 | ACC 0.8609\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0302/0468 | LOSS: 0.4746 | ACC 0.8610\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0303/0468 | LOSS: 0.4744 | ACC 0.8611\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0304/0468 | LOSS: 0.4741 | ACC 0.8611\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0305/0468 | LOSS: 0.4738 | ACC 0.8613\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0306/0468 | LOSS: 0.4737 | ACC 0.8613\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0307/0468 | LOSS: 0.4731 | ACC 0.8614\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0308/0468 | LOSS: 0.4728 | ACC 0.8614\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0309/0468 | LOSS: 0.4725 | ACC 0.8615\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0310/0468 | LOSS: 0.4720 | ACC 0.8616\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0311/0468 | LOSS: 0.4718 | ACC 0.8616\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0312/0468 | LOSS: 0.4714 | ACC 0.8617\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0313/0468 | LOSS: 0.4712 | ACC 0.8617\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0314/0468 | LOSS: 0.4708 | ACC 0.8618\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0315/0468 | LOSS: 0.4702 | ACC 0.8620\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0316/0468 | LOSS: 0.4702 | ACC 0.8620\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0317/0468 | LOSS: 0.4699 | ACC 0.8622\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0318/0468 | LOSS: 0.4697 | ACC 0.8623\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0319/0468 | LOSS: 0.4701 | ACC 0.8622\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0320/0468 | LOSS: 0.4696 | ACC 0.8623\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0321/0468 | LOSS: 0.4693 | ACC 0.8624\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0322/0468 | LOSS: 0.4690 | ACC 0.8623\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0323/0468 | LOSS: 0.4688 | ACC 0.8624\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0324/0468 | LOSS: 0.4686 | ACC 0.8625\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0325/0468 | LOSS: 0.4681 | ACC 0.8626\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0326/0468 | LOSS: 0.4678 | ACC 0.8628\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0327/0468 | LOSS: 0.4672 | ACC 0.8630\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0328/0468 | LOSS: 0.4669 | ACC 0.8630\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0329/0468 | LOSS: 0.4665 | ACC 0.8632\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0330/0468 | LOSS: 0.4666 | ACC 0.8631\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0331/0468 | LOSS: 0.4662 | ACC 0.8633\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0332/0468 | LOSS: 0.4660 | ACC 0.8632\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0333/0468 | LOSS: 0.4656 | ACC 0.8634\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0334/0468 | LOSS: 0.4652 | ACC 0.8635\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0335/0468 | LOSS: 0.4650 | ACC 0.8636\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0336/0468 | LOSS: 0.4648 | ACC 0.8636\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0337/0468 | LOSS: 0.4647 | ACC 0.8637\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0338/0468 | LOSS: 0.4641 | ACC 0.8638\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0339/0468 | LOSS: 0.4640 | ACC 0.8639\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0340/0468 | LOSS: 0.4637 | ACC 0.8640\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0341/0468 | LOSS: 0.4632 | ACC 0.8641\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0342/0468 | LOSS: 0.4624 | ACC 0.8644\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0343/0468 | LOSS: 0.4621 | ACC 0.8644\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0344/0468 | LOSS: 0.4617 | ACC 0.8646\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0345/0468 | LOSS: 0.4614 | ACC 0.8647\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0346/0468 | LOSS: 0.4609 | ACC 0.8649\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0347/0468 | LOSS: 0.4602 | ACC 0.8651\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0348/0468 | LOSS: 0.4599 | ACC 0.8651\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0349/0468 | LOSS: 0.4595 | ACC 0.8653\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0350/0468 | LOSS: 0.4591 | ACC 0.8654\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0351/0468 | LOSS: 0.4588 | ACC 0.8653\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0352/0468 | LOSS: 0.4586 | ACC 0.8654\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0353/0468 | LOSS: 0.4582 | ACC 0.8656\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0354/0468 | LOSS: 0.4582 | ACC 0.8656\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0355/0468 | LOSS: 0.4578 | ACC 0.8657\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0356/0468 | LOSS: 0.4575 | ACC 0.8658\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0357/0468 | LOSS: 0.4572 | ACC 0.8659\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0358/0468 | LOSS: 0.4570 | ACC 0.8661\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0359/0468 | LOSS: 0.4566 | ACC 0.8661\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0360/0468 | LOSS: 0.4559 | ACC 0.8663\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0361/0468 | LOSS: 0.4555 | ACC 0.8664\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0362/0468 | LOSS: 0.4550 | ACC 0.8666\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0363/0468 | LOSS: 0.4547 | ACC 0.8667\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0364/0468 | LOSS: 0.4546 | ACC 0.8667\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0365/0468 | LOSS: 0.4542 | ACC 0.8669\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0366/0468 | LOSS: 0.4541 | ACC 0.8668\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0367/0468 | LOSS: 0.4536 | ACC 0.8669\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0368/0468 | LOSS: 0.4532 | ACC 0.8670\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0369/0468 | LOSS: 0.4529 | ACC 0.8671\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0370/0468 | LOSS: 0.4527 | ACC 0.8672\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0371/0468 | LOSS: 0.4524 | ACC 0.8673\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0372/0468 | LOSS: 0.4521 | ACC 0.8674\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0373/0468 | LOSS: 0.4518 | ACC 0.8675\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0374/0468 | LOSS: 0.4513 | ACC 0.8676\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0375/0468 | LOSS: 0.4510 | ACC 0.8677\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0376/0468 | LOSS: 0.4505 | ACC 0.8679\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0377/0468 | LOSS: 0.4502 | ACC 0.8680\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0378/0468 | LOSS: 0.4500 | ACC 0.8680\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0379/0468 | LOSS: 0.4498 | ACC 0.8681\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0380/0468 | LOSS: 0.4495 | ACC 0.8682\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0381/0468 | LOSS: 0.4494 | ACC 0.8683\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0382/0468 | LOSS: 0.4490 | ACC 0.8684\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0383/0468 | LOSS: 0.4488 | ACC 0.8684\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0384/0468 | LOSS: 0.4482 | ACC 0.8686\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0385/0468 | LOSS: 0.4479 | ACC 0.8688\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0386/0468 | LOSS: 0.4477 | ACC 0.8689\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0387/0468 | LOSS: 0.4475 | ACC 0.8689\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0388/0468 | LOSS: 0.4471 | ACC 0.8691\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0389/0468 | LOSS: 0.4468 | ACC 0.8692\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0390/0468 | LOSS: 0.4466 | ACC 0.8692\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0391/0468 | LOSS: 0.4465 | ACC 0.8693\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0392/0468 | LOSS: 0.4463 | ACC 0.8694\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0393/0468 | LOSS: 0.4457 | ACC 0.8696\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0394/0468 | LOSS: 0.4453 | ACC 0.8696\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0395/0468 | LOSS: 0.4454 | ACC 0.8696\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0396/0468 | LOSS: 0.4451 | ACC 0.8697\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0397/0468 | LOSS: 0.4448 | ACC 0.8699\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0398/0468 | LOSS: 0.4443 | ACC 0.8699\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0399/0468 | LOSS: 0.4440 | ACC 0.8700\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0400/0468 | LOSS: 0.4439 | ACC 0.8700\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0401/0468 | LOSS: 0.4435 | ACC 0.8701\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0402/0468 | LOSS: 0.4437 | ACC 0.8700\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0403/0468 | LOSS: 0.4433 | ACC 0.8701\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0404/0468 | LOSS: 0.4430 | ACC 0.8702\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0405/0468 | LOSS: 0.4428 | ACC 0.8702\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0406/0468 | LOSS: 0.4424 | ACC 0.8703\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0407/0468 | LOSS: 0.4418 | ACC 0.8704\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0408/0468 | LOSS: 0.4414 | ACC 0.8706\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0409/0468 | LOSS: 0.4412 | ACC 0.8706\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0410/0468 | LOSS: 0.4406 | ACC 0.8707\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0411/0468 | LOSS: 0.4405 | ACC 0.8708\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0412/0468 | LOSS: 0.4401 | ACC 0.8709\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0413/0468 | LOSS: 0.4400 | ACC 0.8710\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0414/0468 | LOSS: 0.4398 | ACC 0.8711\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0415/0468 | LOSS: 0.4395 | ACC 0.8712\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0416/0468 | LOSS: 0.4392 | ACC 0.8713\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0417/0468 | LOSS: 0.4388 | ACC 0.8714\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0418/0468 | LOSS: 0.4387 | ACC 0.8716\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0419/0468 | LOSS: 0.4384 | ACC 0.8716\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0420/0468 | LOSS: 0.4378 | ACC 0.8718\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0421/0468 | LOSS: 0.4378 | ACC 0.8717\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0422/0468 | LOSS: 0.4373 | ACC 0.8719\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0423/0468 | LOSS: 0.4371 | ACC 0.8719\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0424/0468 | LOSS: 0.4367 | ACC 0.8720\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0425/0468 | LOSS: 0.4362 | ACC 0.8722\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0426/0468 | LOSS: 0.4358 | ACC 0.8722\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0427/0468 | LOSS: 0.4356 | ACC 0.8723\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0428/0468 | LOSS: 0.4357 | ACC 0.8723\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0429/0468 | LOSS: 0.4354 | ACC 0.8723\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0430/0468 | LOSS: 0.4349 | ACC 0.8725\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0431/0468 | LOSS: 0.4348 | ACC 0.8725\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0432/0468 | LOSS: 0.4346 | ACC 0.8725\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0433/0468 | LOSS: 0.4343 | ACC 0.8726\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0434/0468 | LOSS: 0.4342 | ACC 0.8726\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0435/0468 | LOSS: 0.4339 | ACC 0.8728\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0436/0468 | LOSS: 0.4335 | ACC 0.8728\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0437/0468 | LOSS: 0.4333 | ACC 0.8728\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0438/0468 | LOSS: 0.4331 | ACC 0.8729\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0439/0468 | LOSS: 0.4329 | ACC 0.8730\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0440/0468 | LOSS: 0.4329 | ACC 0.8730\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0441/0468 | LOSS: 0.4326 | ACC 0.8731\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0442/0468 | LOSS: 0.4322 | ACC 0.8732\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0443/0468 | LOSS: 0.4323 | ACC 0.8732\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0444/0468 | LOSS: 0.4319 | ACC 0.8732\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0445/0468 | LOSS: 0.4316 | ACC 0.8733\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0446/0468 | LOSS: 0.4315 | ACC 0.8733\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0447/0468 | LOSS: 0.4312 | ACC 0.8734\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0448/0468 | LOSS: 0.4308 | ACC 0.8736\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0449/0468 | LOSS: 0.4304 | ACC 0.8737\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0450/0468 | LOSS: 0.4301 | ACC 0.8738\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0451/0468 | LOSS: 0.4299 | ACC 0.8738\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0452/0468 | LOSS: 0.4295 | ACC 0.8739\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0453/0468 | LOSS: 0.4290 | ACC 0.8741\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0454/0468 | LOSS: 0.4288 | ACC 0.8741\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0455/0468 | LOSS: 0.4285 | ACC 0.8743\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0456/0468 | LOSS: 0.4284 | ACC 0.8743\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0457/0468 | LOSS: 0.4280 | ACC 0.8745\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0458/0468 | LOSS: 0.4278 | ACC 0.8746\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0459/0468 | LOSS: 0.4276 | ACC 0.8746\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0460/0468 | LOSS: 0.4274 | ACC 0.8747\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0461/0468 | LOSS: 0.4269 | ACC 0.8749\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0462/0468 | LOSS: 0.4267 | ACC 0.8750\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0463/0468 | LOSS: 0.4264 | ACC 0.8751\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0464/0468 | LOSS: 0.4260 | ACC 0.8752\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0465/0468 | LOSS: 0.4257 | ACC 0.8753\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0466/0468 | LOSS: 0.4253 | ACC 0.8754\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0467/0468 | LOSS: 0.4251 | ACC 0.8754\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0468/0468 | LOSS: 0.4250 | ACC 0.8754\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0000/0468 | LOSS: 0.2940 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0001/0468 | LOSS: 0.3133 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0002/0468 | LOSS: 0.3135 | ACC 0.9245\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0003/0468 | LOSS: 0.3472 | ACC 0.9062\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0004/0468 | LOSS: 0.3472 | ACC 0.8969\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0005/0468 | LOSS: 0.3281 | ACC 0.9049\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0006/0468 | LOSS: 0.3217 | ACC 0.9085\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0007/0468 | LOSS: 0.3197 | ACC 0.9072\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0008/0468 | LOSS: 0.3132 | ACC 0.9071\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0009/0468 | LOSS: 0.3058 | ACC 0.9094\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0010/0468 | LOSS: 0.3055 | ACC 0.9098\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0011/0468 | LOSS: 0.3034 | ACC 0.9108\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0012/0468 | LOSS: 0.2969 | ACC 0.9123\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0013/0468 | LOSS: 0.2939 | ACC 0.9118\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0014/0468 | LOSS: 0.2916 | ACC 0.9120\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0015/0468 | LOSS: 0.2911 | ACC 0.9121\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0016/0468 | LOSS: 0.2932 | ACC 0.9108\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0017/0468 | LOSS: 0.2959 | ACC 0.9093\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0018/0468 | LOSS: 0.3003 | ACC 0.9079\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0019/0468 | LOSS: 0.2984 | ACC 0.9090\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0020/0468 | LOSS: 0.2973 | ACC 0.9089\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0021/0468 | LOSS: 0.2948 | ACC 0.9091\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0022/0468 | LOSS: 0.2919 | ACC 0.9096\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0023/0468 | LOSS: 0.2970 | ACC 0.9079\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0024/0468 | LOSS: 0.2929 | ACC 0.9094\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0025/0468 | LOSS: 0.2918 | ACC 0.9093\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0026/0468 | LOSS: 0.2904 | ACC 0.9094\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0027/0468 | LOSS: 0.2902 | ACC 0.9104\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0028/0468 | LOSS: 0.2925 | ACC 0.9098\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0029/0468 | LOSS: 0.2902 | ACC 0.9115\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0030/0468 | LOSS: 0.2878 | ACC 0.9128\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0031/0468 | LOSS: 0.2862 | ACC 0.9131\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0032/0468 | LOSS: 0.2883 | ACC 0.9131\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0033/0468 | LOSS: 0.2877 | ACC 0.9136\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0034/0468 | LOSS: 0.2896 | ACC 0.9134\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0035/0468 | LOSS: 0.2908 | ACC 0.9138\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0036/0468 | LOSS: 0.2899 | ACC 0.9136\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0037/0468 | LOSS: 0.2910 | ACC 0.9128\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0038/0468 | LOSS: 0.2907 | ACC 0.9131\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0039/0468 | LOSS: 0.2893 | ACC 0.9139\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0040/0468 | LOSS: 0.2890 | ACC 0.9139\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0041/0468 | LOSS: 0.2906 | ACC 0.9142\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0042/0468 | LOSS: 0.2904 | ACC 0.9142\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0043/0468 | LOSS: 0.2909 | ACC 0.9134\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0044/0468 | LOSS: 0.2910 | ACC 0.9127\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0045/0468 | LOSS: 0.2920 | ACC 0.9124\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0046/0468 | LOSS: 0.2919 | ACC 0.9124\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0047/0468 | LOSS: 0.2921 | ACC 0.9124\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0048/0468 | LOSS: 0.2932 | ACC 0.9128\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0049/0468 | LOSS: 0.2929 | ACC 0.9122\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0050/0468 | LOSS: 0.2916 | ACC 0.9127\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0051/0468 | LOSS: 0.2915 | ACC 0.9127\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0052/0468 | LOSS: 0.2899 | ACC 0.9135\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0053/0468 | LOSS: 0.2912 | ACC 0.9136\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0054/0468 | LOSS: 0.2912 | ACC 0.9138\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0055/0468 | LOSS: 0.2934 | ACC 0.9129\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0056/0468 | LOSS: 0.2926 | ACC 0.9132\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0057/0468 | LOSS: 0.2902 | ACC 0.9141\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0058/0468 | LOSS: 0.2893 | ACC 0.9142\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0059/0468 | LOSS: 0.2905 | ACC 0.9133\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0060/0468 | LOSS: 0.2900 | ACC 0.9137\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0061/0468 | LOSS: 0.2905 | ACC 0.9139\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0062/0468 | LOSS: 0.2904 | ACC 0.9139\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0063/0468 | LOSS: 0.2887 | ACC 0.9148\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0064/0468 | LOSS: 0.2874 | ACC 0.9153\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0065/0468 | LOSS: 0.2879 | ACC 0.9149\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0066/0468 | LOSS: 0.2877 | ACC 0.9149\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0067/0468 | LOSS: 0.2880 | ACC 0.9152\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0068/0468 | LOSS: 0.2881 | ACC 0.9154\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0069/0468 | LOSS: 0.2885 | ACC 0.9152\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0070/0468 | LOSS: 0.2888 | ACC 0.9155\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0071/0468 | LOSS: 0.2869 | ACC 0.9161\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0072/0468 | LOSS: 0.2861 | ACC 0.9162\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0073/0468 | LOSS: 0.2856 | ACC 0.9160\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0074/0468 | LOSS: 0.2863 | ACC 0.9159\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0075/0468 | LOSS: 0.2855 | ACC 0.9162\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0076/0468 | LOSS: 0.2840 | ACC 0.9168\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0077/0468 | LOSS: 0.2834 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0078/0468 | LOSS: 0.2829 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0079/0468 | LOSS: 0.2824 | ACC 0.9176\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0080/0468 | LOSS: 0.2825 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0081/0468 | LOSS: 0.2821 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0082/0468 | LOSS: 0.2823 | ACC 0.9173\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0083/0468 | LOSS: 0.2827 | ACC 0.9173\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0084/0468 | LOSS: 0.2826 | ACC 0.9175\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0085/0468 | LOSS: 0.2822 | ACC 0.9175\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0086/0468 | LOSS: 0.2839 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0087/0468 | LOSS: 0.2838 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0088/0468 | LOSS: 0.2834 | ACC 0.9171\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0089/0468 | LOSS: 0.2847 | ACC 0.9168\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0090/0468 | LOSS: 0.2843 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0091/0468 | LOSS: 0.2841 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0092/0468 | LOSS: 0.2850 | ACC 0.9165\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0093/0468 | LOSS: 0.2849 | ACC 0.9163\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0094/0468 | LOSS: 0.2846 | ACC 0.9165\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0095/0468 | LOSS: 0.2842 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0096/0468 | LOSS: 0.2840 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0097/0468 | LOSS: 0.2836 | ACC 0.9167\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0098/0468 | LOSS: 0.2837 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0099/0468 | LOSS: 0.2838 | ACC 0.9167\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0100/0468 | LOSS: 0.2851 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0101/0468 | LOSS: 0.2844 | ACC 0.9169\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0102/0468 | LOSS: 0.2842 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0103/0468 | LOSS: 0.2838 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0104/0468 | LOSS: 0.2843 | ACC 0.9168\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0105/0468 | LOSS: 0.2835 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0106/0468 | LOSS: 0.2832 | ACC 0.9171\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0107/0468 | LOSS: 0.2827 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0108/0468 | LOSS: 0.2820 | ACC 0.9174\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0109/0468 | LOSS: 0.2816 | ACC 0.9175\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0110/0468 | LOSS: 0.2825 | ACC 0.9173\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0111/0468 | LOSS: 0.2819 | ACC 0.9173\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0112/0468 | LOSS: 0.2822 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0113/0468 | LOSS: 0.2817 | ACC 0.9176\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0114/0468 | LOSS: 0.2815 | ACC 0.9174\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0115/0468 | LOSS: 0.2819 | ACC 0.9176\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0116/0468 | LOSS: 0.2815 | ACC 0.9177\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0117/0468 | LOSS: 0.2814 | ACC 0.9177\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0118/0468 | LOSS: 0.2809 | ACC 0.9179\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0119/0468 | LOSS: 0.2802 | ACC 0.9181\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0120/0468 | LOSS: 0.2792 | ACC 0.9185\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0121/0468 | LOSS: 0.2790 | ACC 0.9185\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0122/0468 | LOSS: 0.2782 | ACC 0.9189\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0123/0468 | LOSS: 0.2780 | ACC 0.9192\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0124/0468 | LOSS: 0.2795 | ACC 0.9188\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0125/0468 | LOSS: 0.2794 | ACC 0.9190\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0126/0468 | LOSS: 0.2796 | ACC 0.9189\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0127/0468 | LOSS: 0.2791 | ACC 0.9189\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0128/0468 | LOSS: 0.2796 | ACC 0.9189\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0129/0468 | LOSS: 0.2798 | ACC 0.9187\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0130/0468 | LOSS: 0.2799 | ACC 0.9187\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0131/0468 | LOSS: 0.2792 | ACC 0.9190\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0132/0468 | LOSS: 0.2788 | ACC 0.9191\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0133/0468 | LOSS: 0.2782 | ACC 0.9192\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0134/0468 | LOSS: 0.2777 | ACC 0.9194\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0135/0468 | LOSS: 0.2771 | ACC 0.9197\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0136/0468 | LOSS: 0.2763 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0137/0468 | LOSS: 0.2761 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0138/0468 | LOSS: 0.2760 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0139/0468 | LOSS: 0.2762 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0140/0468 | LOSS: 0.2760 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0141/0468 | LOSS: 0.2759 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0142/0468 | LOSS: 0.2756 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0143/0468 | LOSS: 0.2758 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0144/0468 | LOSS: 0.2759 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0145/0468 | LOSS: 0.2755 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0146/0468 | LOSS: 0.2757 | ACC 0.9205\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0147/0468 | LOSS: 0.2750 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0148/0468 | LOSS: 0.2750 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0149/0468 | LOSS: 0.2748 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0150/0468 | LOSS: 0.2756 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0151/0468 | LOSS: 0.2757 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0152/0468 | LOSS: 0.2755 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0153/0468 | LOSS: 0.2753 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0154/0468 | LOSS: 0.2754 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0155/0468 | LOSS: 0.2749 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0156/0468 | LOSS: 0.2747 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0157/0468 | LOSS: 0.2743 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0158/0468 | LOSS: 0.2744 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0159/0468 | LOSS: 0.2739 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0160/0468 | LOSS: 0.2740 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0161/0468 | LOSS: 0.2736 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0162/0468 | LOSS: 0.2731 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0163/0468 | LOSS: 0.2729 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0164/0468 | LOSS: 0.2727 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0165/0468 | LOSS: 0.2727 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0166/0468 | LOSS: 0.2727 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0167/0468 | LOSS: 0.2725 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0168/0468 | LOSS: 0.2723 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0169/0468 | LOSS: 0.2725 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0170/0468 | LOSS: 0.2729 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0171/0468 | LOSS: 0.2731 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0172/0468 | LOSS: 0.2727 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0173/0468 | LOSS: 0.2732 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0174/0468 | LOSS: 0.2729 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0175/0468 | LOSS: 0.2724 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0176/0468 | LOSS: 0.2719 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0177/0468 | LOSS: 0.2716 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0178/0468 | LOSS: 0.2716 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0179/0468 | LOSS: 0.2715 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0180/0468 | LOSS: 0.2711 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0181/0468 | LOSS: 0.2708 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0182/0468 | LOSS: 0.2707 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0183/0468 | LOSS: 0.2714 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0184/0468 | LOSS: 0.2713 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0185/0468 | LOSS: 0.2713 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0186/0468 | LOSS: 0.2716 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0187/0468 | LOSS: 0.2719 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0188/0468 | LOSS: 0.2717 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0189/0468 | LOSS: 0.2715 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0190/0468 | LOSS: 0.2719 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0191/0468 | LOSS: 0.2721 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0192/0468 | LOSS: 0.2720 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0193/0468 | LOSS: 0.2719 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0194/0468 | LOSS: 0.2719 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0195/0468 | LOSS: 0.2720 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0196/0468 | LOSS: 0.2716 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0197/0468 | LOSS: 0.2721 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0198/0468 | LOSS: 0.2725 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0199/0468 | LOSS: 0.2727 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0200/0468 | LOSS: 0.2727 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0201/0468 | LOSS: 0.2726 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0202/0468 | LOSS: 0.2727 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0203/0468 | LOSS: 0.2728 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0204/0468 | LOSS: 0.2728 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0205/0468 | LOSS: 0.2725 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0206/0468 | LOSS: 0.2723 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0207/0468 | LOSS: 0.2723 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0208/0468 | LOSS: 0.2720 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0209/0468 | LOSS: 0.2719 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0210/0468 | LOSS: 0.2715 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0211/0468 | LOSS: 0.2713 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0212/0468 | LOSS: 0.2710 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0213/0468 | LOSS: 0.2706 | ACC 0.9205\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0214/0468 | LOSS: 0.2704 | ACC 0.9205\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0215/0468 | LOSS: 0.2706 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0216/0468 | LOSS: 0.2704 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0217/0468 | LOSS: 0.2700 | ACC 0.9205\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0218/0468 | LOSS: 0.2702 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0219/0468 | LOSS: 0.2712 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0220/0468 | LOSS: 0.2710 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0221/0468 | LOSS: 0.2709 | ACC 0.9199\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0222/0468 | LOSS: 0.2708 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0223/0468 | LOSS: 0.2705 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0224/0468 | LOSS: 0.2713 | ACC 0.9197\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0225/0468 | LOSS: 0.2711 | ACC 0.9197\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0226/0468 | LOSS: 0.2712 | ACC 0.9196\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0227/0468 | LOSS: 0.2710 | ACC 0.9196\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0228/0468 | LOSS: 0.2709 | ACC 0.9197\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0229/0468 | LOSS: 0.2710 | ACC 0.9198\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0230/0468 | LOSS: 0.2706 | ACC 0.9200\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0231/0468 | LOSS: 0.2704 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0232/0468 | LOSS: 0.2701 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0233/0468 | LOSS: 0.2698 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0234/0468 | LOSS: 0.2699 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0235/0468 | LOSS: 0.2696 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0236/0468 | LOSS: 0.2698 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0237/0468 | LOSS: 0.2697 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0238/0468 | LOSS: 0.2698 | ACC 0.9202\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0239/0468 | LOSS: 0.2695 | ACC 0.9203\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0240/0468 | LOSS: 0.2691 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0241/0468 | LOSS: 0.2688 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0242/0468 | LOSS: 0.2686 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0243/0468 | LOSS: 0.2683 | ACC 0.9207\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0244/0468 | LOSS: 0.2678 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0245/0468 | LOSS: 0.2674 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0246/0468 | LOSS: 0.2674 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0247/0468 | LOSS: 0.2674 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0248/0468 | LOSS: 0.2672 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0249/0468 | LOSS: 0.2670 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0250/0468 | LOSS: 0.2673 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0251/0468 | LOSS: 0.2675 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0252/0468 | LOSS: 0.2674 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0253/0468 | LOSS: 0.2674 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0254/0468 | LOSS: 0.2671 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0255/0468 | LOSS: 0.2671 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0256/0468 | LOSS: 0.2666 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0257/0468 | LOSS: 0.2668 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0258/0468 | LOSS: 0.2666 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0259/0468 | LOSS: 0.2667 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0260/0468 | LOSS: 0.2664 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0261/0468 | LOSS: 0.2660 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0262/0468 | LOSS: 0.2661 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0263/0468 | LOSS: 0.2659 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0264/0468 | LOSS: 0.2660 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0265/0468 | LOSS: 0.2660 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0266/0468 | LOSS: 0.2656 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0267/0468 | LOSS: 0.2656 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0268/0468 | LOSS: 0.2653 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0269/0468 | LOSS: 0.2649 | ACC 0.9216\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0270/0468 | LOSS: 0.2653 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0271/0468 | LOSS: 0.2653 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0272/0468 | LOSS: 0.2654 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0273/0468 | LOSS: 0.2654 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0274/0468 | LOSS: 0.2658 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0275/0468 | LOSS: 0.2657 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0276/0468 | LOSS: 0.2656 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0277/0468 | LOSS: 0.2656 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0278/0468 | LOSS: 0.2657 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0279/0468 | LOSS: 0.2657 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0280/0468 | LOSS: 0.2654 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0281/0468 | LOSS: 0.2658 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0282/0468 | LOSS: 0.2656 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0283/0468 | LOSS: 0.2656 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0284/0468 | LOSS: 0.2653 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0285/0468 | LOSS: 0.2652 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0286/0468 | LOSS: 0.2652 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0287/0468 | LOSS: 0.2652 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0288/0468 | LOSS: 0.2650 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0289/0468 | LOSS: 0.2647 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0290/0468 | LOSS: 0.2643 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0291/0468 | LOSS: 0.2641 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0292/0468 | LOSS: 0.2639 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0293/0468 | LOSS: 0.2639 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0294/0468 | LOSS: 0.2642 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0295/0468 | LOSS: 0.2641 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0296/0468 | LOSS: 0.2643 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0297/0468 | LOSS: 0.2642 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0298/0468 | LOSS: 0.2639 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0299/0468 | LOSS: 0.2640 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0300/0468 | LOSS: 0.2637 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0301/0468 | LOSS: 0.2636 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0302/0468 | LOSS: 0.2633 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0303/0468 | LOSS: 0.2632 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0304/0468 | LOSS: 0.2632 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0305/0468 | LOSS: 0.2631 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0306/0468 | LOSS: 0.2626 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0307/0468 | LOSS: 0.2624 | ACC 0.9222\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0308/0468 | LOSS: 0.2622 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0309/0468 | LOSS: 0.2618 | ACC 0.9223\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0310/0468 | LOSS: 0.2619 | ACC 0.9223\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0311/0468 | LOSS: 0.2616 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0312/0468 | LOSS: 0.2617 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0313/0468 | LOSS: 0.2614 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0314/0468 | LOSS: 0.2615 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0315/0468 | LOSS: 0.2615 | ACC 0.9226\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0316/0468 | LOSS: 0.2615 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0317/0468 | LOSS: 0.2615 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0318/0468 | LOSS: 0.2611 | ACC 0.9227\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0319/0468 | LOSS: 0.2613 | ACC 0.9226\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0320/0468 | LOSS: 0.2611 | ACC 0.9227\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0321/0468 | LOSS: 0.2612 | ACC 0.9227\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0322/0468 | LOSS: 0.2607 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0323/0468 | LOSS: 0.2607 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0324/0468 | LOSS: 0.2609 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0325/0468 | LOSS: 0.2608 | ACC 0.9227\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0326/0468 | LOSS: 0.2608 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0327/0468 | LOSS: 0.2607 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0328/0468 | LOSS: 0.2606 | ACC 0.9229\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0329/0468 | LOSS: 0.2606 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0330/0468 | LOSS: 0.2606 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0331/0468 | LOSS: 0.2602 | ACC 0.9229\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0332/0468 | LOSS: 0.2601 | ACC 0.9229\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0333/0468 | LOSS: 0.2600 | ACC 0.9229\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0334/0468 | LOSS: 0.2599 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0335/0468 | LOSS: 0.2598 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0336/0468 | LOSS: 0.2601 | ACC 0.9229\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0337/0468 | LOSS: 0.2602 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0338/0468 | LOSS: 0.2600 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0339/0468 | LOSS: 0.2597 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0340/0468 | LOSS: 0.2594 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0341/0468 | LOSS: 0.2593 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0342/0468 | LOSS: 0.2596 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0343/0468 | LOSS: 0.2595 | ACC 0.9230\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0344/0468 | LOSS: 0.2593 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0345/0468 | LOSS: 0.2594 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0346/0468 | LOSS: 0.2593 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0347/0468 | LOSS: 0.2591 | ACC 0.9232\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0348/0468 | LOSS: 0.2589 | ACC 0.9232\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0349/0468 | LOSS: 0.2590 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0350/0468 | LOSS: 0.2590 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0351/0468 | LOSS: 0.2590 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0352/0468 | LOSS: 0.2588 | ACC 0.9232\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0353/0468 | LOSS: 0.2588 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0354/0468 | LOSS: 0.2586 | ACC 0.9232\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0355/0468 | LOSS: 0.2585 | ACC 0.9233\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0356/0468 | LOSS: 0.2583 | ACC 0.9233\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0357/0468 | LOSS: 0.2584 | ACC 0.9233\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0358/0468 | LOSS: 0.2583 | ACC 0.9234\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0359/0468 | LOSS: 0.2582 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0360/0468 | LOSS: 0.2581 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0361/0468 | LOSS: 0.2584 | ACC 0.9234\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0362/0468 | LOSS: 0.2583 | ACC 0.9234\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0363/0468 | LOSS: 0.2583 | ACC 0.9234\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0364/0468 | LOSS: 0.2581 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0365/0468 | LOSS: 0.2580 | ACC 0.9234\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0366/0468 | LOSS: 0.2579 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0367/0468 | LOSS: 0.2578 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0368/0468 | LOSS: 0.2574 | ACC 0.9236\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0369/0468 | LOSS: 0.2574 | ACC 0.9236\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0370/0468 | LOSS: 0.2572 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0371/0468 | LOSS: 0.2570 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0372/0468 | LOSS: 0.2569 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0373/0468 | LOSS: 0.2573 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0374/0468 | LOSS: 0.2573 | ACC 0.9236\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0375/0468 | LOSS: 0.2570 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0376/0468 | LOSS: 0.2567 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0377/0468 | LOSS: 0.2566 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0378/0468 | LOSS: 0.2565 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0379/0468 | LOSS: 0.2563 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0380/0468 | LOSS: 0.2566 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0381/0468 | LOSS: 0.2567 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0382/0468 | LOSS: 0.2567 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0383/0468 | LOSS: 0.2570 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0384/0468 | LOSS: 0.2569 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0385/0468 | LOSS: 0.2568 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0386/0468 | LOSS: 0.2564 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0387/0468 | LOSS: 0.2566 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0388/0468 | LOSS: 0.2563 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0389/0468 | LOSS: 0.2561 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0390/0468 | LOSS: 0.2560 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0391/0468 | LOSS: 0.2557 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0392/0468 | LOSS: 0.2557 | ACC 0.9242\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0393/0468 | LOSS: 0.2558 | ACC 0.9242\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0394/0468 | LOSS: 0.2556 | ACC 0.9242\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0395/0468 | LOSS: 0.2553 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0396/0468 | LOSS: 0.2551 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0397/0468 | LOSS: 0.2550 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0398/0468 | LOSS: 0.2552 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0399/0468 | LOSS: 0.2553 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0400/0468 | LOSS: 0.2553 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0401/0468 | LOSS: 0.2554 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0402/0468 | LOSS: 0.2554 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0403/0468 | LOSS: 0.2552 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0404/0468 | LOSS: 0.2551 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0405/0468 | LOSS: 0.2550 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0406/0468 | LOSS: 0.2549 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0407/0468 | LOSS: 0.2546 | ACC 0.9244\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0408/0468 | LOSS: 0.2543 | ACC 0.9245\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0409/0468 | LOSS: 0.2540 | ACC 0.9246\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0410/0468 | LOSS: 0.2539 | ACC 0.9247\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0411/0468 | LOSS: 0.2537 | ACC 0.9247\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0412/0468 | LOSS: 0.2535 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0413/0468 | LOSS: 0.2535 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0414/0468 | LOSS: 0.2534 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0415/0468 | LOSS: 0.2534 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0416/0468 | LOSS: 0.2536 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0417/0468 | LOSS: 0.2537 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0418/0468 | LOSS: 0.2535 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0419/0468 | LOSS: 0.2535 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0420/0468 | LOSS: 0.2537 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0421/0468 | LOSS: 0.2536 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0422/0468 | LOSS: 0.2536 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0423/0468 | LOSS: 0.2535 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0424/0468 | LOSS: 0.2533 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0425/0468 | LOSS: 0.2533 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0426/0468 | LOSS: 0.2532 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0427/0468 | LOSS: 0.2529 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0428/0468 | LOSS: 0.2530 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0429/0468 | LOSS: 0.2527 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0430/0468 | LOSS: 0.2528 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0431/0468 | LOSS: 0.2528 | ACC 0.9251\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0432/0468 | LOSS: 0.2526 | ACC 0.9251\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0433/0468 | LOSS: 0.2525 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0434/0468 | LOSS: 0.2524 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0435/0468 | LOSS: 0.2521 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0436/0468 | LOSS: 0.2522 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0437/0468 | LOSS: 0.2520 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0438/0468 | LOSS: 0.2517 | ACC 0.9255\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0439/0468 | LOSS: 0.2518 | ACC 0.9255\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0440/0468 | LOSS: 0.2518 | ACC 0.9255\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0441/0468 | LOSS: 0.2517 | ACC 0.9255\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0442/0468 | LOSS: 0.2514 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0443/0468 | LOSS: 0.2513 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0444/0468 | LOSS: 0.2512 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0445/0468 | LOSS: 0.2512 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0446/0468 | LOSS: 0.2511 | ACC 0.9257\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0447/0468 | LOSS: 0.2512 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0448/0468 | LOSS: 0.2512 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0449/0468 | LOSS: 0.2509 | ACC 0.9257\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0450/0468 | LOSS: 0.2507 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0451/0468 | LOSS: 0.2508 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0452/0468 | LOSS: 0.2510 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0453/0468 | LOSS: 0.2508 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0454/0468 | LOSS: 0.2508 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0455/0468 | LOSS: 0.2509 | ACC 0.9257\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0456/0468 | LOSS: 0.2508 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0457/0468 | LOSS: 0.2506 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0458/0468 | LOSS: 0.2505 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0459/0468 | LOSS: 0.2505 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0460/0468 | LOSS: 0.2504 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0461/0468 | LOSS: 0.2504 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0462/0468 | LOSS: 0.2504 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0463/0468 | LOSS: 0.2504 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0464/0468 | LOSS: 0.2506 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0465/0468 | LOSS: 0.2506 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0466/0468 | LOSS: 0.2505 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0467/0468 | LOSS: 0.2505 | ACC 0.9257\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0468/0468 | LOSS: 0.2503 | ACC 0.9258\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0000/0468 | LOSS: 0.1607 | ACC 0.9531\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0001/0468 | LOSS: 0.1893 | ACC 0.9453\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0002/0468 | LOSS: 0.1876 | ACC 0.9479\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0003/0468 | LOSS: 0.1734 | ACC 0.9551\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0004/0468 | LOSS: 0.1784 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0005/0468 | LOSS: 0.1785 | ACC 0.9466\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0006/0468 | LOSS: 0.1776 | ACC 0.9453\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0007/0468 | LOSS: 0.1744 | ACC 0.9463\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0008/0468 | LOSS: 0.1824 | ACC 0.9410\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0009/0468 | LOSS: 0.1785 | ACC 0.9430\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0010/0468 | LOSS: 0.1720 | ACC 0.9467\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0011/0468 | LOSS: 0.1739 | ACC 0.9447\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0012/0468 | LOSS: 0.1747 | ACC 0.9441\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0013/0468 | LOSS: 0.1763 | ACC 0.9425\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0014/0468 | LOSS: 0.1758 | ACC 0.9427\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0015/0468 | LOSS: 0.1768 | ACC 0.9409\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0016/0468 | LOSS: 0.1801 | ACC 0.9384\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0017/0468 | LOSS: 0.1790 | ACC 0.9384\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0018/0468 | LOSS: 0.1809 | ACC 0.9387\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0019/0468 | LOSS: 0.1888 | ACC 0.9379\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0020/0468 | LOSS: 0.1941 | ACC 0.9371\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0021/0468 | LOSS: 0.1966 | ACC 0.9371\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0022/0468 | LOSS: 0.1991 | ACC 0.9361\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0023/0468 | LOSS: 0.1983 | ACC 0.9372\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0024/0468 | LOSS: 0.1959 | ACC 0.9381\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0025/0468 | LOSS: 0.1976 | ACC 0.9375\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0026/0468 | LOSS: 0.1983 | ACC 0.9361\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0027/0468 | LOSS: 0.1976 | ACC 0.9358\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0028/0468 | LOSS: 0.1969 | ACC 0.9367\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0029/0468 | LOSS: 0.1977 | ACC 0.9375\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0030/0468 | LOSS: 0.1969 | ACC 0.9378\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0031/0468 | LOSS: 0.2001 | ACC 0.9370\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0032/0468 | LOSS: 0.2009 | ACC 0.9377\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0033/0468 | LOSS: 0.1991 | ACC 0.9382\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0034/0468 | LOSS: 0.2038 | ACC 0.9371\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0035/0468 | LOSS: 0.2020 | ACC 0.9377\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0036/0468 | LOSS: 0.2016 | ACC 0.9375\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0037/0468 | LOSS: 0.2024 | ACC 0.9375\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0038/0468 | LOSS: 0.2002 | ACC 0.9379\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0039/0468 | LOSS: 0.2025 | ACC 0.9371\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0040/0468 | LOSS: 0.2037 | ACC 0.9369\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0041/0468 | LOSS: 0.2025 | ACC 0.9379\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0042/0468 | LOSS: 0.2013 | ACC 0.9382\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0043/0468 | LOSS: 0.2004 | ACC 0.9387\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0044/0468 | LOSS: 0.1982 | ACC 0.9396\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0045/0468 | LOSS: 0.1989 | ACC 0.9385\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0046/0468 | LOSS: 0.1976 | ACC 0.9385\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0047/0468 | LOSS: 0.1989 | ACC 0.9378\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0048/0468 | LOSS: 0.1992 | ACC 0.9378\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0049/0468 | LOSS: 0.1986 | ACC 0.9378\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0050/0468 | LOSS: 0.1989 | ACC 0.9373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23912\\3041924972.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 그라디언트를 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# 그라디언트를 사용하여 파라미터를 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epoch + 1):\n",
    "    net.train() # net.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "\n",
    "    for batch, (input, label) in enumerate(loader):\n",
    "        input = input.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = net(input)     # net.forward(input)\n",
    "        pred = fn_pred(output)  # softmax\n",
    "\n",
    "        optimizer.zero_grad()   # G = 0 \n",
    "\n",
    "        loss = fn_loss(output, label)   # \n",
    "        acc = fn_accuracy(pred, label)\n",
    "\n",
    "        loss.backward() # 그라디언트를 계산\n",
    "\n",
    "        optimizer.step()    # 그라디언트를 사용하여 파라미터를 업데이트\n",
    "\n",
    "        loss_arr += [loss.item()]\n",
    "        acc_arr += [acc.item()]\n",
    "\n",
    "        print('TRAIN: EPOCH %04d/%04d | BATCH %04d/%04d | LOSS: %.4f | ACC %.4f' %\n",
    "              (epoch, num_epoch, batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))\n",
    "\n",
    "    save(ckpt_dir = ckpt_dir, net=net, optim=optimizer, epoch=epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
