{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch를 이용한 간단한 CNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    # Pytorch 기본 라이브러리\n",
    "import torch.nn as nn   # Neural Network (nn)에 관련된 모듈들\n",
    "from torch.utils.data import DataLoader # Database의 데이터들을 불러오기 위한 라이브러리\n",
    "\n",
    "from torchvision import transforms, datasets    # 불러온 입력을 처리하는 모듈들\n",
    "\n",
    "import os   # 경로 탐색, 접근 등을 처리하기 위한 라이브러리\n",
    "import numpy as np # 행렬 연산에 사용하는 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 학습을 위한 하이퍼 파리미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 128\n",
    "num_epoch = 5\n",
    "\n",
    "ckpt_dir = './checkpoint'   # 학습된 파라미터를 저장할 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):       # nn.Module를 상속받아서 사용.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # 상속해준 부모 클래스의 Method를 사용할 때 super(하위클래스, 하위클래스의 객체)\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=10, \n",
    "                               kernel_size=5, \n",
    "                               stride=1, \n",
    "                               padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)    # cf. AvgPool2d\n",
    "\n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, \n",
    "                               out_channels=20, \n",
    "                               kernel_size=5, \n",
    "                               stride=1, \n",
    "                               padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)    # H * W * C = 320\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=320, \n",
    "                             out_features=50)             # Fully-Connected Layer\n",
    "        self.relu1_fc1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):   # x: 입력영상 (torch.Tensor())\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 320)     # x: B=128, C=20, H=4, W=4\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1_fc1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습된 파라미터를 저장하거나 불러오는 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(ckpt_dir, net, optim, epoch):      # ckpt_dir: checkpoint를 저장할 경로, net, optim, epoch\n",
    "    if not os.path.exists(ckpt_dir):    # ckpt_dir이 존재하는지 확인하는 함수\n",
    "        os.makedirs(ckpt_dir)           # 디렉토리를 만들어주는 함수\n",
    "\n",
    "    torch.save({'net': net.state_dict(),        # 네트워크에 있는 변수들\n",
    "                'optim': optim.state_dict()},   # optimizer에 있는 변수들\n",
    "               './%s/model_epoch%d.pth' % (ckpt_dir, epoch))\n",
    "\n",
    "def load(ckpt_dir, net, optim):\n",
    "    ckpt_lst = os.listdir(ckpt_dir)  # 입력한 디렉토리 내의 모든 파일과 디렉토리의 리스트를 반환\n",
    "    ckpt_lst.sort()  # 정렬\n",
    "\n",
    "    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "\n",
    "    return net, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서식문자를 이용한 문자열 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat is animal.\n",
      "2 + 3 is 5\n"
     ]
    }
   ],
   "source": [
    "print('%s is %s.' % ('Cat', 'animal'))\n",
    "print('%d + %d is %d' % (2, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "a = ['b', 'a', 'c']\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Database 불러오기\n",
    "### Database가 없을 경우 Pytorch에 내장되어 있는 MNIST 다운로드 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "83.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "dataset = datasets.MNIST(download=True, \n",
    "                         root='./', \n",
    "                         train=True, \n",
    "                         transform=transform)   # 개별 데이터 파일의 처리를 담당\n",
    "loader = DataLoader(dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, \n",
    "                    num_workers=0)  # 미니배치 처리를 담당\n",
    "\n",
    "num_data = len(loader.dataset)\n",
    "num_batch = num_data // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 네트워크 및 손실함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "params = net.parameters()\n",
    "\n",
    "fn_loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def fn_pred(output):\n",
    "    return torch.softmax(output, dim=1)     # B=128 (dim=0), 10 (dim=1)\n",
    "\n",
    "def fn_accuracy(pred, label):\n",
    "    return (pred.max(dim=1)[1] == label).type(torch.float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimizer 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0001/0005 | BATCH 0000/0468 | LOSS: 2.3044 | ACC 0.1172\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0001/0468 | LOSS: 2.3026 | ACC 0.1172\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0002/0468 | LOSS: 2.3030 | ACC 0.1224\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0003/0468 | LOSS: 2.3020 | ACC 0.1484\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0004/0468 | LOSS: 2.3016 | ACC 0.1578\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0005/0468 | LOSS: 2.3022 | ACC 0.1523\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0006/0468 | LOSS: 2.3023 | ACC 0.1551\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0007/0468 | LOSS: 2.3014 | ACC 0.1562\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0008/0468 | LOSS: 2.3015 | ACC 0.1528\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0009/0468 | LOSS: 2.3013 | ACC 0.1547\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0010/0468 | LOSS: 2.3017 | ACC 0.1577\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0011/0468 | LOSS: 2.3015 | ACC 0.1589\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0012/0468 | LOSS: 2.3013 | ACC 0.1593\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0013/0468 | LOSS: 2.3011 | ACC 0.1641\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0014/0468 | LOSS: 2.3006 | ACC 0.1651\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0015/0468 | LOSS: 2.2999 | ACC 0.1650\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0016/0468 | LOSS: 2.2994 | ACC 0.1673\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0017/0468 | LOSS: 2.2995 | ACC 0.1697\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0018/0468 | LOSS: 2.2995 | ACC 0.1690\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0019/0468 | LOSS: 2.2993 | ACC 0.1711\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0020/0468 | LOSS: 2.2992 | ACC 0.1719\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0021/0468 | LOSS: 2.2993 | ACC 0.1687\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0022/0468 | LOSS: 2.2993 | ACC 0.1709\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0023/0468 | LOSS: 2.2990 | ACC 0.1702\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0024/0468 | LOSS: 2.2989 | ACC 0.1709\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0025/0468 | LOSS: 2.2987 | ACC 0.1716\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0026/0468 | LOSS: 2.2982 | ACC 0.1739\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0027/0468 | LOSS: 2.2980 | ACC 0.1744\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0028/0468 | LOSS: 2.2978 | ACC 0.1756\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0029/0468 | LOSS: 2.2978 | ACC 0.1753\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0030/0468 | LOSS: 2.2975 | ACC 0.1736\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0031/0468 | LOSS: 2.2972 | ACC 0.1753\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0032/0468 | LOSS: 2.2969 | ACC 0.1768\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0033/0468 | LOSS: 2.2968 | ACC 0.1760\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0034/0468 | LOSS: 2.2968 | ACC 0.1759\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0035/0468 | LOSS: 2.2968 | ACC 0.1760\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0036/0468 | LOSS: 2.2966 | ACC 0.1763\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0037/0468 | LOSS: 2.2966 | ACC 0.1768\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0038/0468 | LOSS: 2.2964 | ACC 0.1771\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0039/0468 | LOSS: 2.2961 | ACC 0.1783\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0040/0468 | LOSS: 2.2960 | ACC 0.1784\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0041/0468 | LOSS: 2.2957 | ACC 0.1797\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0042/0468 | LOSS: 2.2955 | ACC 0.1811\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0043/0468 | LOSS: 2.2953 | ACC 0.1822\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0044/0468 | LOSS: 2.2950 | ACC 0.1835\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0045/0468 | LOSS: 2.2948 | ACC 0.1846\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0046/0468 | LOSS: 2.2945 | ACC 0.1860\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0047/0468 | LOSS: 2.2942 | ACC 0.1865\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0048/0468 | LOSS: 2.2941 | ACC 0.1857\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0049/0468 | LOSS: 2.2941 | ACC 0.1858\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0050/0468 | LOSS: 2.2939 | ACC 0.1869\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0051/0468 | LOSS: 2.2937 | ACC 0.1890\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0052/0468 | LOSS: 2.2936 | ACC 0.1897\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0053/0468 | LOSS: 2.2934 | ACC 0.1916\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0054/0468 | LOSS: 2.2931 | ACC 0.1928\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0055/0468 | LOSS: 2.2928 | ACC 0.1939\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0056/0468 | LOSS: 2.2928 | ACC 0.1931\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0057/0468 | LOSS: 2.2926 | ACC 0.1942\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0058/0468 | LOSS: 2.2925 | ACC 0.1956\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0059/0468 | LOSS: 2.2923 | ACC 0.1960\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0060/0468 | LOSS: 2.2923 | ACC 0.1967\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0061/0468 | LOSS: 2.2921 | ACC 0.1976\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0062/0468 | LOSS: 2.2920 | ACC 0.1973\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0063/0468 | LOSS: 2.2919 | ACC 0.1979\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0064/0468 | LOSS: 2.2918 | ACC 0.1980\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0065/0468 | LOSS: 2.2919 | ACC 0.1976\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0066/0468 | LOSS: 2.2917 | ACC 0.1983\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0067/0468 | LOSS: 2.2915 | ACC 0.1985\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0068/0468 | LOSS: 2.2914 | ACC 0.1985\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0069/0468 | LOSS: 2.2911 | ACC 0.1994\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0070/0468 | LOSS: 2.2910 | ACC 0.1996\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0071/0468 | LOSS: 2.2907 | ACC 0.1999\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0072/0468 | LOSS: 2.2907 | ACC 0.1995\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0073/0468 | LOSS: 2.2905 | ACC 0.2007\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0074/0468 | LOSS: 2.2904 | ACC 0.2007\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0075/0468 | LOSS: 2.2902 | ACC 0.2008\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0076/0468 | LOSS: 2.2901 | ACC 0.2010\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0077/0468 | LOSS: 2.2900 | ACC 0.2010\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0078/0468 | LOSS: 2.2900 | ACC 0.2007\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0079/0468 | LOSS: 2.2898 | ACC 0.2015\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0080/0468 | LOSS: 2.2896 | ACC 0.2022\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0081/0468 | LOSS: 2.2896 | ACC 0.2017\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0082/0468 | LOSS: 2.2896 | ACC 0.2017\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0083/0468 | LOSS: 2.2894 | ACC 0.2035\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0084/0468 | LOSS: 2.2892 | ACC 0.2040\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0085/0468 | LOSS: 2.2891 | ACC 0.2041\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0086/0468 | LOSS: 2.2889 | ACC 0.2050\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0087/0468 | LOSS: 2.2888 | ACC 0.2054\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0088/0468 | LOSS: 2.2886 | ACC 0.2051\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0089/0468 | LOSS: 2.2885 | ACC 0.2053\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0090/0468 | LOSS: 2.2883 | ACC 0.2063\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0091/0468 | LOSS: 2.2881 | ACC 0.2068\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0092/0468 | LOSS: 2.2880 | ACC 0.2067\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0093/0468 | LOSS: 2.2879 | ACC 0.2073\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0094/0468 | LOSS: 2.2876 | ACC 0.2081\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0095/0468 | LOSS: 2.2875 | ACC 0.2091\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0096/0468 | LOSS: 2.2874 | ACC 0.2092\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0097/0468 | LOSS: 2.2873 | ACC 0.2097\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0098/0468 | LOSS: 2.2870 | ACC 0.2108\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0099/0468 | LOSS: 2.2868 | ACC 0.2119\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0100/0468 | LOSS: 2.2866 | ACC 0.2133\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0101/0468 | LOSS: 2.2864 | ACC 0.2139\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0102/0468 | LOSS: 2.2862 | ACC 0.2142\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0103/0468 | LOSS: 2.2861 | ACC 0.2141\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0104/0468 | LOSS: 2.2859 | ACC 0.2144\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0105/0468 | LOSS: 2.2858 | ACC 0.2143\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0106/0468 | LOSS: 2.2856 | ACC 0.2147\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0107/0468 | LOSS: 2.2855 | ACC 0.2154\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0108/0468 | LOSS: 2.2853 | ACC 0.2158\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0109/0468 | LOSS: 2.2852 | ACC 0.2163\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0110/0468 | LOSS: 2.2850 | ACC 0.2171\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0111/0468 | LOSS: 2.2847 | ACC 0.2182\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0112/0468 | LOSS: 2.2845 | ACC 0.2192\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0113/0468 | LOSS: 2.2843 | ACC 0.2201\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0114/0468 | LOSS: 2.2840 | ACC 0.2213\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0115/0468 | LOSS: 2.2838 | ACC 0.2214\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0116/0468 | LOSS: 2.2836 | ACC 0.2223\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0117/0468 | LOSS: 2.2834 | ACC 0.2229\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0118/0468 | LOSS: 2.2831 | ACC 0.2237\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0119/0468 | LOSS: 2.2830 | ACC 0.2237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0001/0005 | BATCH 0120/0468 | LOSS: 2.2828 | ACC 0.2241\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0121/0468 | LOSS: 2.2826 | ACC 0.2246\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0122/0468 | LOSS: 2.2823 | ACC 0.2250\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0123/0468 | LOSS: 2.2821 | ACC 0.2257\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0124/0468 | LOSS: 2.2820 | ACC 0.2259\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0125/0468 | LOSS: 2.2818 | ACC 0.2263\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0126/0468 | LOSS: 2.2817 | ACC 0.2266\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0127/0468 | LOSS: 2.2815 | ACC 0.2270\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0128/0468 | LOSS: 2.2813 | ACC 0.2273\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0129/0468 | LOSS: 2.2811 | ACC 0.2276\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0130/0468 | LOSS: 2.2809 | ACC 0.2281\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0131/0468 | LOSS: 2.2808 | ACC 0.2286\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0132/0468 | LOSS: 2.2805 | ACC 0.2296\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0133/0468 | LOSS: 2.2802 | ACC 0.2299\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0134/0468 | LOSS: 2.2800 | ACC 0.2304\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0135/0468 | LOSS: 2.2799 | ACC 0.2309\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0136/0468 | LOSS: 2.2797 | ACC 0.2319\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0137/0468 | LOSS: 2.2795 | ACC 0.2323\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0138/0468 | LOSS: 2.2792 | ACC 0.2333\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0139/0468 | LOSS: 2.2790 | ACC 0.2342\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0140/0468 | LOSS: 2.2788 | ACC 0.2352\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0141/0468 | LOSS: 2.2785 | ACC 0.2360\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0142/0468 | LOSS: 2.2783 | ACC 0.2371\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0143/0468 | LOSS: 2.2780 | ACC 0.2377\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0144/0468 | LOSS: 2.2778 | ACC 0.2385\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0145/0468 | LOSS: 2.2776 | ACC 0.2399\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0146/0468 | LOSS: 2.2773 | ACC 0.2408\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0147/0468 | LOSS: 2.2770 | ACC 0.2413\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0148/0468 | LOSS: 2.2768 | ACC 0.2419\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0149/0468 | LOSS: 2.2766 | ACC 0.2432\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0150/0468 | LOSS: 2.2763 | ACC 0.2443\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0151/0468 | LOSS: 2.2761 | ACC 0.2449\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0152/0468 | LOSS: 2.2760 | ACC 0.2458\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0153/0468 | LOSS: 2.2757 | ACC 0.2464\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0154/0468 | LOSS: 2.2755 | ACC 0.2470\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0155/0468 | LOSS: 2.2753 | ACC 0.2480\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0156/0468 | LOSS: 2.2751 | ACC 0.2487\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0157/0468 | LOSS: 2.2749 | ACC 0.2495\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0158/0468 | LOSS: 2.2747 | ACC 0.2506\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0159/0468 | LOSS: 2.2744 | ACC 0.2517\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0160/0468 | LOSS: 2.2741 | ACC 0.2527\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0161/0468 | LOSS: 2.2739 | ACC 0.2537\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0162/0468 | LOSS: 2.2736 | ACC 0.2547\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0163/0468 | LOSS: 2.2734 | ACC 0.2560\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0164/0468 | LOSS: 2.2731 | ACC 0.2569\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0165/0468 | LOSS: 2.2729 | ACC 0.2580\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0166/0468 | LOSS: 2.2725 | ACC 0.2597\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0167/0468 | LOSS: 2.2723 | ACC 0.2605\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0168/0468 | LOSS: 2.2720 | ACC 0.2616\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0169/0468 | LOSS: 2.2718 | ACC 0.2624\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0170/0468 | LOSS: 2.2716 | ACC 0.2637\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0171/0468 | LOSS: 2.2714 | ACC 0.2646\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0172/0468 | LOSS: 2.2711 | ACC 0.2658\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0173/0468 | LOSS: 2.2709 | ACC 0.2664\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0174/0468 | LOSS: 2.2707 | ACC 0.2672\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0175/0468 | LOSS: 2.2705 | ACC 0.2677\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0176/0468 | LOSS: 2.2702 | ACC 0.2687\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0177/0468 | LOSS: 2.2699 | ACC 0.2697\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0178/0468 | LOSS: 2.2697 | ACC 0.2703\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0179/0468 | LOSS: 2.2694 | ACC 0.2714\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0180/0468 | LOSS: 2.2691 | ACC 0.2721\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0181/0468 | LOSS: 2.2687 | ACC 0.2735\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0182/0468 | LOSS: 2.2684 | ACC 0.2744\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0183/0468 | LOSS: 2.2681 | ACC 0.2752\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0184/0468 | LOSS: 2.2678 | ACC 0.2761\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0185/0468 | LOSS: 2.2676 | ACC 0.2766\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0186/0468 | LOSS: 2.2674 | ACC 0.2773\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0187/0468 | LOSS: 2.2670 | ACC 0.2783\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0188/0468 | LOSS: 2.2666 | ACC 0.2793\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0189/0468 | LOSS: 2.2663 | ACC 0.2804\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0190/0468 | LOSS: 2.2659 | ACC 0.2816\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0191/0468 | LOSS: 2.2656 | ACC 0.2825\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0192/0468 | LOSS: 2.2653 | ACC 0.2834\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0193/0468 | LOSS: 2.2649 | ACC 0.2845\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0194/0468 | LOSS: 2.2645 | ACC 0.2856\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0195/0468 | LOSS: 2.2642 | ACC 0.2861\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0196/0468 | LOSS: 2.2638 | ACC 0.2870\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0197/0468 | LOSS: 2.2635 | ACC 0.2876\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0198/0468 | LOSS: 2.2631 | ACC 0.2886\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0199/0468 | LOSS: 2.2628 | ACC 0.2895\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0200/0468 | LOSS: 2.2625 | ACC 0.2905\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0201/0468 | LOSS: 2.2621 | ACC 0.2913\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0202/0468 | LOSS: 2.2618 | ACC 0.2921\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0203/0468 | LOSS: 2.2615 | ACC 0.2926\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0204/0468 | LOSS: 2.2610 | ACC 0.2933\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0205/0468 | LOSS: 2.2607 | ACC 0.2938\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0206/0468 | LOSS: 2.2602 | ACC 0.2948\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0207/0468 | LOSS: 2.2598 | ACC 0.2959\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0208/0468 | LOSS: 2.2594 | ACC 0.2965\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0209/0468 | LOSS: 2.2590 | ACC 0.2972\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0210/0468 | LOSS: 2.2586 | ACC 0.2979\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0211/0468 | LOSS: 2.2581 | ACC 0.2989\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0212/0468 | LOSS: 2.2575 | ACC 0.2999\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0213/0468 | LOSS: 2.2572 | ACC 0.3004\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0214/0468 | LOSS: 2.2567 | ACC 0.3013\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0215/0468 | LOSS: 2.2562 | ACC 0.3023\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0216/0468 | LOSS: 2.2559 | ACC 0.3029\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0217/0468 | LOSS: 2.2556 | ACC 0.3034\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0218/0468 | LOSS: 2.2552 | ACC 0.3041\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0219/0468 | LOSS: 2.2548 | ACC 0.3044\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0220/0468 | LOSS: 2.2543 | ACC 0.3051\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0221/0468 | LOSS: 2.2540 | ACC 0.3055\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0222/0468 | LOSS: 2.2536 | ACC 0.3059\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0223/0468 | LOSS: 2.2533 | ACC 0.3064\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0224/0468 | LOSS: 2.2528 | ACC 0.3072\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0225/0468 | LOSS: 2.2523 | ACC 0.3078\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0226/0468 | LOSS: 2.2518 | ACC 0.3086\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0227/0468 | LOSS: 2.2514 | ACC 0.3090\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0228/0468 | LOSS: 2.2510 | ACC 0.3090\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0229/0468 | LOSS: 2.2506 | ACC 0.3096\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0230/0468 | LOSS: 2.2500 | ACC 0.3102\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0231/0468 | LOSS: 2.2495 | ACC 0.3109\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0232/0468 | LOSS: 2.2490 | ACC 0.3112\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0233/0468 | LOSS: 2.2485 | ACC 0.3118\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0234/0468 | LOSS: 2.2479 | ACC 0.3126\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0235/0468 | LOSS: 2.2474 | ACC 0.3130\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0236/0468 | LOSS: 2.2468 | ACC 0.3135\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0237/0468 | LOSS: 2.2463 | ACC 0.3140\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0238/0468 | LOSS: 2.2457 | ACC 0.3143\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0239/0468 | LOSS: 2.2452 | ACC 0.3148\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0240/0468 | LOSS: 2.2447 | ACC 0.3153\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0241/0468 | LOSS: 2.2441 | ACC 0.3157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0001/0005 | BATCH 0242/0468 | LOSS: 2.2435 | ACC 0.3164\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0243/0468 | LOSS: 2.2428 | ACC 0.3175\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0244/0468 | LOSS: 2.2424 | ACC 0.3178\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0245/0468 | LOSS: 2.2418 | ACC 0.3182\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0246/0468 | LOSS: 2.2414 | ACC 0.3184\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0247/0468 | LOSS: 2.2408 | ACC 0.3189\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0248/0468 | LOSS: 2.2402 | ACC 0.3194\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0249/0468 | LOSS: 2.2396 | ACC 0.3200\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0250/0468 | LOSS: 2.2391 | ACC 0.3203\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0251/0468 | LOSS: 2.2386 | ACC 0.3205\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0252/0468 | LOSS: 2.2381 | ACC 0.3209\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0253/0468 | LOSS: 2.2375 | ACC 0.3216\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0254/0468 | LOSS: 2.2368 | ACC 0.3222\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0255/0468 | LOSS: 2.2361 | ACC 0.3229\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0256/0468 | LOSS: 2.2355 | ACC 0.3232\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0257/0468 | LOSS: 2.2349 | ACC 0.3236\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0258/0468 | LOSS: 2.2343 | ACC 0.3238\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0259/0468 | LOSS: 2.2338 | ACC 0.3239\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0260/0468 | LOSS: 2.2331 | ACC 0.3245\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0261/0468 | LOSS: 2.2324 | ACC 0.3249\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0262/0468 | LOSS: 2.2315 | ACC 0.3254\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0263/0468 | LOSS: 2.2308 | ACC 0.3257\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0264/0468 | LOSS: 2.2301 | ACC 0.3260\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0265/0468 | LOSS: 2.2295 | ACC 0.3260\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0266/0468 | LOSS: 2.2288 | ACC 0.3262\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0267/0468 | LOSS: 2.2282 | ACC 0.3263\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0268/0468 | LOSS: 2.2275 | ACC 0.3266\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0269/0468 | LOSS: 2.2266 | ACC 0.3271\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0270/0468 | LOSS: 2.2259 | ACC 0.3273\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0271/0468 | LOSS: 2.2251 | ACC 0.3278\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0272/0468 | LOSS: 2.2244 | ACC 0.3280\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0273/0468 | LOSS: 2.2236 | ACC 0.3285\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0274/0468 | LOSS: 2.2229 | ACC 0.3287\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0275/0468 | LOSS: 2.2219 | ACC 0.3293\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0276/0468 | LOSS: 2.2212 | ACC 0.3293\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0277/0468 | LOSS: 2.2207 | ACC 0.3295\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0278/0468 | LOSS: 2.2198 | ACC 0.3301\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0279/0468 | LOSS: 2.2190 | ACC 0.3302\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0280/0468 | LOSS: 2.2180 | ACC 0.3306\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0281/0468 | LOSS: 2.2172 | ACC 0.3308\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0282/0468 | LOSS: 2.2163 | ACC 0.3312\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0283/0468 | LOSS: 2.2156 | ACC 0.3315\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0284/0468 | LOSS: 2.2148 | ACC 0.3317\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0285/0468 | LOSS: 2.2139 | ACC 0.3320\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0286/0468 | LOSS: 2.2130 | ACC 0.3325\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0287/0468 | LOSS: 2.2119 | ACC 0.3329\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0288/0468 | LOSS: 2.2111 | ACC 0.3329\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0289/0468 | LOSS: 2.2102 | ACC 0.3332\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0290/0468 | LOSS: 2.2093 | ACC 0.3333\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0291/0468 | LOSS: 2.2086 | ACC 0.3334\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0292/0468 | LOSS: 2.2076 | ACC 0.3338\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0293/0468 | LOSS: 2.2067 | ACC 0.3340\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0294/0468 | LOSS: 2.2057 | ACC 0.3345\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0295/0468 | LOSS: 2.2050 | ACC 0.3344\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0296/0468 | LOSS: 2.2041 | ACC 0.3346\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0297/0468 | LOSS: 2.2031 | ACC 0.3349\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0298/0468 | LOSS: 2.2021 | ACC 0.3353\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0299/0468 | LOSS: 2.2011 | ACC 0.3358\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0300/0468 | LOSS: 2.2001 | ACC 0.3362\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0301/0468 | LOSS: 2.1989 | ACC 0.3366\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0302/0468 | LOSS: 2.1980 | ACC 0.3366\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0303/0468 | LOSS: 2.1971 | ACC 0.3368\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0304/0468 | LOSS: 2.1958 | ACC 0.3375\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0305/0468 | LOSS: 2.1947 | ACC 0.3380\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0306/0468 | LOSS: 2.1938 | ACC 0.3382\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0307/0468 | LOSS: 2.1925 | ACC 0.3387\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0308/0468 | LOSS: 2.1914 | ACC 0.3393\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0309/0468 | LOSS: 2.1903 | ACC 0.3396\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0310/0468 | LOSS: 2.1891 | ACC 0.3402\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0311/0468 | LOSS: 2.1881 | ACC 0.3404\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0312/0468 | LOSS: 2.1869 | ACC 0.3408\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0313/0468 | LOSS: 2.1860 | ACC 0.3408\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0314/0468 | LOSS: 2.1847 | ACC 0.3413\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0315/0468 | LOSS: 2.1836 | ACC 0.3415\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0316/0468 | LOSS: 2.1827 | ACC 0.3415\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0317/0468 | LOSS: 2.1814 | ACC 0.3419\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0318/0468 | LOSS: 2.1802 | ACC 0.3422\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0319/0468 | LOSS: 2.1792 | ACC 0.3423\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0320/0468 | LOSS: 2.1781 | ACC 0.3426\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0321/0468 | LOSS: 2.1769 | ACC 0.3430\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0322/0468 | LOSS: 2.1757 | ACC 0.3434\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0323/0468 | LOSS: 2.1742 | ACC 0.3441\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0324/0468 | LOSS: 2.1731 | ACC 0.3443\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0325/0468 | LOSS: 2.1717 | ACC 0.3447\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0326/0468 | LOSS: 2.1704 | ACC 0.3449\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0327/0468 | LOSS: 2.1690 | ACC 0.3453\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0328/0468 | LOSS: 2.1677 | ACC 0.3459\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0329/0468 | LOSS: 2.1667 | ACC 0.3461\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0330/0468 | LOSS: 2.1650 | ACC 0.3467\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0331/0468 | LOSS: 2.1637 | ACC 0.3470\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0332/0468 | LOSS: 2.1623 | ACC 0.3475\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0333/0468 | LOSS: 2.1609 | ACC 0.3477\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0334/0468 | LOSS: 2.1593 | ACC 0.3483\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0335/0468 | LOSS: 2.1579 | ACC 0.3487\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0336/0468 | LOSS: 2.1564 | ACC 0.3490\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0337/0468 | LOSS: 2.1551 | ACC 0.3496\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0338/0468 | LOSS: 2.1537 | ACC 0.3499\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0339/0468 | LOSS: 2.1522 | ACC 0.3503\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0340/0468 | LOSS: 2.1508 | ACC 0.3507\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0341/0468 | LOSS: 2.1493 | ACC 0.3511\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0342/0468 | LOSS: 2.1478 | ACC 0.3515\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0343/0468 | LOSS: 2.1463 | ACC 0.3519\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0344/0468 | LOSS: 2.1450 | ACC 0.3522\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0345/0468 | LOSS: 2.1435 | ACC 0.3526\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0346/0468 | LOSS: 2.1419 | ACC 0.3530\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0347/0468 | LOSS: 2.1403 | ACC 0.3534\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0348/0468 | LOSS: 2.1386 | ACC 0.3539\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0349/0468 | LOSS: 2.1371 | ACC 0.3542\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0350/0468 | LOSS: 2.1355 | ACC 0.3547\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0351/0468 | LOSS: 2.1341 | ACC 0.3549\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0352/0468 | LOSS: 2.1327 | ACC 0.3553\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0353/0468 | LOSS: 2.1309 | ACC 0.3561\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0354/0468 | LOSS: 2.1292 | ACC 0.3565\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0355/0468 | LOSS: 2.1277 | ACC 0.3570\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0356/0468 | LOSS: 2.1258 | ACC 0.3575\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0357/0468 | LOSS: 2.1240 | ACC 0.3581\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0358/0468 | LOSS: 2.1222 | ACC 0.3587\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0359/0468 | LOSS: 2.1205 | ACC 0.3593\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0360/0468 | LOSS: 2.1190 | ACC 0.3597\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0361/0468 | LOSS: 2.1171 | ACC 0.3604\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0362/0468 | LOSS: 2.1152 | ACC 0.3609\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0363/0468 | LOSS: 2.1132 | ACC 0.3615\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0364/0468 | LOSS: 2.1114 | ACC 0.3622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0001/0005 | BATCH 0365/0468 | LOSS: 2.1098 | ACC 0.3625\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0366/0468 | LOSS: 2.1078 | ACC 0.3631\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0367/0468 | LOSS: 2.1062 | ACC 0.3635\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0368/0468 | LOSS: 2.1043 | ACC 0.3641\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0369/0468 | LOSS: 2.1026 | ACC 0.3645\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0370/0468 | LOSS: 2.1010 | ACC 0.3650\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0371/0468 | LOSS: 2.0989 | ACC 0.3657\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0372/0468 | LOSS: 2.0973 | ACC 0.3661\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0373/0468 | LOSS: 2.0956 | ACC 0.3665\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0374/0468 | LOSS: 2.0941 | ACC 0.3668\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0375/0468 | LOSS: 2.0924 | ACC 0.3672\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0376/0468 | LOSS: 2.0909 | ACC 0.3677\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0377/0468 | LOSS: 2.0891 | ACC 0.3682\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0378/0468 | LOSS: 2.0872 | ACC 0.3689\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0379/0468 | LOSS: 2.0853 | ACC 0.3695\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0380/0468 | LOSS: 2.0835 | ACC 0.3701\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0381/0468 | LOSS: 2.0818 | ACC 0.3708\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0382/0468 | LOSS: 2.0800 | ACC 0.3713\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0383/0468 | LOSS: 2.0782 | ACC 0.3720\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0384/0468 | LOSS: 2.0765 | ACC 0.3726\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0385/0468 | LOSS: 2.0748 | ACC 0.3731\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0386/0468 | LOSS: 2.0730 | ACC 0.3737\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0387/0468 | LOSS: 2.0712 | ACC 0.3743\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0388/0468 | LOSS: 2.0694 | ACC 0.3750\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0389/0468 | LOSS: 2.0676 | ACC 0.3755\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0390/0468 | LOSS: 2.0658 | ACC 0.3762\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0391/0468 | LOSS: 2.0641 | ACC 0.3767\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0392/0468 | LOSS: 2.0621 | ACC 0.3774\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0393/0468 | LOSS: 2.0604 | ACC 0.3780\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0394/0468 | LOSS: 2.0587 | ACC 0.3785\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0395/0468 | LOSS: 2.0564 | ACC 0.3795\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0396/0468 | LOSS: 2.0545 | ACC 0.3802\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0397/0468 | LOSS: 2.0524 | ACC 0.3809\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0398/0468 | LOSS: 2.0508 | ACC 0.3814\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0399/0468 | LOSS: 2.0489 | ACC 0.3819\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0400/0468 | LOSS: 2.0468 | ACC 0.3828\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0401/0468 | LOSS: 2.0449 | ACC 0.3832\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0402/0468 | LOSS: 2.0430 | ACC 0.3838\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0403/0468 | LOSS: 2.0413 | ACC 0.3843\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0404/0468 | LOSS: 2.0392 | ACC 0.3849\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0405/0468 | LOSS: 2.0375 | ACC 0.3854\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0406/0468 | LOSS: 2.0352 | ACC 0.3862\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0407/0468 | LOSS: 2.0333 | ACC 0.3869\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0408/0468 | LOSS: 2.0314 | ACC 0.3876\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0409/0468 | LOSS: 2.0294 | ACC 0.3882\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0410/0468 | LOSS: 2.0276 | ACC 0.3887\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0411/0468 | LOSS: 2.0255 | ACC 0.3894\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0412/0468 | LOSS: 2.0236 | ACC 0.3901\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0413/0468 | LOSS: 2.0213 | ACC 0.3910\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0414/0468 | LOSS: 2.0189 | ACC 0.3919\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0415/0468 | LOSS: 2.0170 | ACC 0.3924\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0416/0468 | LOSS: 2.0152 | ACC 0.3929\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0417/0468 | LOSS: 2.0133 | ACC 0.3936\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0418/0468 | LOSS: 2.0113 | ACC 0.3944\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0419/0468 | LOSS: 2.0092 | ACC 0.3950\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0420/0468 | LOSS: 2.0073 | ACC 0.3957\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0421/0468 | LOSS: 2.0053 | ACC 0.3963\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0422/0468 | LOSS: 2.0030 | ACC 0.3970\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0423/0468 | LOSS: 2.0010 | ACC 0.3977\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0424/0468 | LOSS: 1.9989 | ACC 0.3985\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0425/0468 | LOSS: 1.9966 | ACC 0.3992\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0426/0468 | LOSS: 1.9944 | ACC 0.3999\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0427/0468 | LOSS: 1.9924 | ACC 0.4005\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0428/0468 | LOSS: 1.9903 | ACC 0.4012\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0429/0468 | LOSS: 1.9879 | ACC 0.4021\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0430/0468 | LOSS: 1.9862 | ACC 0.4026\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0431/0468 | LOSS: 1.9842 | ACC 0.4032\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0432/0468 | LOSS: 1.9824 | ACC 0.4037\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0433/0468 | LOSS: 1.9802 | ACC 0.4045\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0434/0468 | LOSS: 1.9780 | ACC 0.4052\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0435/0468 | LOSS: 1.9759 | ACC 0.4059\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0436/0468 | LOSS: 1.9738 | ACC 0.4067\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0437/0468 | LOSS: 1.9716 | ACC 0.4073\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0438/0468 | LOSS: 1.9696 | ACC 0.4080\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0439/0468 | LOSS: 1.9676 | ACC 0.4086\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0440/0468 | LOSS: 1.9656 | ACC 0.4093\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0441/0468 | LOSS: 1.9632 | ACC 0.4100\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0442/0468 | LOSS: 1.9613 | ACC 0.4106\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0443/0468 | LOSS: 1.9595 | ACC 0.4110\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0444/0468 | LOSS: 1.9575 | ACC 0.4115\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0445/0468 | LOSS: 1.9553 | ACC 0.4122\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0446/0468 | LOSS: 1.9529 | ACC 0.4130\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0447/0468 | LOSS: 1.9508 | ACC 0.4136\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0448/0468 | LOSS: 1.9487 | ACC 0.4143\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0449/0468 | LOSS: 1.9467 | ACC 0.4149\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0450/0468 | LOSS: 1.9446 | ACC 0.4156\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0451/0468 | LOSS: 1.9424 | ACC 0.4163\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0452/0468 | LOSS: 1.9401 | ACC 0.4170\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0453/0468 | LOSS: 1.9381 | ACC 0.4177\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0454/0468 | LOSS: 1.9360 | ACC 0.4184\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0455/0468 | LOSS: 1.9339 | ACC 0.4191\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0456/0468 | LOSS: 1.9318 | ACC 0.4199\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0457/0468 | LOSS: 1.9300 | ACC 0.4205\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0458/0468 | LOSS: 1.9278 | ACC 0.4211\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0459/0468 | LOSS: 1.9255 | ACC 0.4219\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0460/0468 | LOSS: 1.9237 | ACC 0.4225\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0461/0468 | LOSS: 1.9215 | ACC 0.4233\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0462/0468 | LOSS: 1.9193 | ACC 0.4239\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0463/0468 | LOSS: 1.9174 | ACC 0.4245\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0464/0468 | LOSS: 1.9155 | ACC 0.4251\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0465/0468 | LOSS: 1.9132 | ACC 0.4258\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0466/0468 | LOSS: 1.9111 | ACC 0.4264\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0467/0468 | LOSS: 1.9089 | ACC 0.4272\n",
      "TRAIN: EPOCH 0001/0005 | BATCH 0468/0468 | LOSS: 1.9068 | ACC 0.4278\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0000/0468 | LOSS: 0.9495 | ACC 0.7500\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0001/0468 | LOSS: 0.9102 | ACC 0.7695\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0002/0468 | LOSS: 0.9523 | ACC 0.7448\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0003/0468 | LOSS: 0.9613 | ACC 0.7344\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0004/0468 | LOSS: 0.9598 | ACC 0.7375\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0005/0468 | LOSS: 0.9674 | ACC 0.7318\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0006/0468 | LOSS: 0.9853 | ACC 0.7210\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0007/0468 | LOSS: 0.9778 | ACC 0.7227\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0008/0468 | LOSS: 0.9715 | ACC 0.7257\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0009/0468 | LOSS: 0.9559 | ACC 0.7328\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0010/0468 | LOSS: 0.9648 | ACC 0.7330\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0011/0468 | LOSS: 0.9581 | ACC 0.7337\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0012/0468 | LOSS: 0.9535 | ACC 0.7314\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0013/0468 | LOSS: 0.9521 | ACC 0.7294\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0014/0468 | LOSS: 0.9519 | ACC 0.7297\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0015/0468 | LOSS: 0.9421 | ACC 0.7349\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0016/0468 | LOSS: 0.9365 | ACC 0.7335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0002/0005 | BATCH 0017/0468 | LOSS: 0.9299 | ACC 0.7352\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0018/0468 | LOSS: 0.9271 | ACC 0.7360\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0019/0468 | LOSS: 0.9254 | ACC 0.7352\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0020/0468 | LOSS: 0.9206 | ACC 0.7370\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0021/0468 | LOSS: 0.9196 | ACC 0.7376\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0022/0468 | LOSS: 0.9208 | ACC 0.7371\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0023/0468 | LOSS: 0.9184 | ACC 0.7367\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0024/0468 | LOSS: 0.9135 | ACC 0.7381\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0025/0468 | LOSS: 0.9132 | ACC 0.7377\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0026/0468 | LOSS: 0.9109 | ACC 0.7378\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0027/0468 | LOSS: 0.9116 | ACC 0.7374\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0028/0468 | LOSS: 0.9106 | ACC 0.7371\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0029/0468 | LOSS: 0.9131 | ACC 0.7365\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0030/0468 | LOSS: 0.9095 | ACC 0.7361\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0031/0468 | LOSS: 0.9066 | ACC 0.7358\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0032/0468 | LOSS: 0.9026 | ACC 0.7372\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0033/0468 | LOSS: 0.9006 | ACC 0.7378\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0034/0468 | LOSS: 0.8987 | ACC 0.7388\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0035/0468 | LOSS: 0.8984 | ACC 0.7407\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0036/0468 | LOSS: 0.8953 | ACC 0.7413\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0037/0468 | LOSS: 0.8891 | ACC 0.7444\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0038/0468 | LOSS: 0.8878 | ACC 0.7448\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0039/0468 | LOSS: 0.8847 | ACC 0.7447\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0040/0468 | LOSS: 0.8821 | ACC 0.7462\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0041/0468 | LOSS: 0.8802 | ACC 0.7470\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0042/0468 | LOSS: 0.8770 | ACC 0.7480\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0043/0468 | LOSS: 0.8734 | ACC 0.7495\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0044/0468 | LOSS: 0.8707 | ACC 0.7495\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0045/0468 | LOSS: 0.8685 | ACC 0.7505\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0046/0468 | LOSS: 0.8655 | ACC 0.7525\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0047/0468 | LOSS: 0.8626 | ACC 0.7534\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0048/0468 | LOSS: 0.8599 | ACC 0.7546\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0049/0468 | LOSS: 0.8594 | ACC 0.7553\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0050/0468 | LOSS: 0.8578 | ACC 0.7560\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0051/0468 | LOSS: 0.8580 | ACC 0.7559\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0052/0468 | LOSS: 0.8577 | ACC 0.7555\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0053/0468 | LOSS: 0.8555 | ACC 0.7559\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0054/0468 | LOSS: 0.8550 | ACC 0.7553\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0055/0468 | LOSS: 0.8519 | ACC 0.7556\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0056/0468 | LOSS: 0.8519 | ACC 0.7547\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0057/0468 | LOSS: 0.8505 | ACC 0.7554\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0058/0468 | LOSS: 0.8493 | ACC 0.7562\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0059/0468 | LOSS: 0.8487 | ACC 0.7568\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0060/0468 | LOSS: 0.8460 | ACC 0.7578\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0061/0468 | LOSS: 0.8453 | ACC 0.7574\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0062/0468 | LOSS: 0.8452 | ACC 0.7574\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0063/0468 | LOSS: 0.8436 | ACC 0.7577\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0064/0468 | LOSS: 0.8411 | ACC 0.7588\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0065/0468 | LOSS: 0.8408 | ACC 0.7586\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0066/0468 | LOSS: 0.8376 | ACC 0.7598\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0067/0468 | LOSS: 0.8367 | ACC 0.7602\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0068/0468 | LOSS: 0.8366 | ACC 0.7597\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0069/0468 | LOSS: 0.8354 | ACC 0.7603\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0070/0468 | LOSS: 0.8344 | ACC 0.7608\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0071/0468 | LOSS: 0.8312 | ACC 0.7619\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0072/0468 | LOSS: 0.8322 | ACC 0.7615\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0073/0468 | LOSS: 0.8317 | ACC 0.7619\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0074/0468 | LOSS: 0.8315 | ACC 0.7615\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0075/0468 | LOSS: 0.8312 | ACC 0.7615\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0076/0468 | LOSS: 0.8303 | ACC 0.7612\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0077/0468 | LOSS: 0.8278 | ACC 0.7618\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0078/0468 | LOSS: 0.8274 | ACC 0.7617\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0079/0468 | LOSS: 0.8272 | ACC 0.7619\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0080/0468 | LOSS: 0.8264 | ACC 0.7622\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0081/0468 | LOSS: 0.8255 | ACC 0.7624\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0082/0468 | LOSS: 0.8249 | ACC 0.7620\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0083/0468 | LOSS: 0.8232 | ACC 0.7625\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0084/0468 | LOSS: 0.8214 | ACC 0.7629\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0085/0468 | LOSS: 0.8202 | ACC 0.7632\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0086/0468 | LOSS: 0.8192 | ACC 0.7636\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0087/0468 | LOSS: 0.8172 | ACC 0.7643\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0088/0468 | LOSS: 0.8169 | ACC 0.7640\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0089/0468 | LOSS: 0.8160 | ACC 0.7639\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0090/0468 | LOSS: 0.8144 | ACC 0.7643\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0091/0468 | LOSS: 0.8121 | ACC 0.7649\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0092/0468 | LOSS: 0.8120 | ACC 0.7650\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0093/0468 | LOSS: 0.8096 | ACC 0.7658\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0094/0468 | LOSS: 0.8083 | ACC 0.7661\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0095/0468 | LOSS: 0.8078 | ACC 0.7661\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0096/0468 | LOSS: 0.8072 | ACC 0.7662\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0097/0468 | LOSS: 0.8054 | ACC 0.7667\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0098/0468 | LOSS: 0.8033 | ACC 0.7669\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0099/0468 | LOSS: 0.8019 | ACC 0.7673\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0100/0468 | LOSS: 0.8006 | ACC 0.7675\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0101/0468 | LOSS: 0.7998 | ACC 0.7675\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0102/0468 | LOSS: 0.7981 | ACC 0.7678\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0103/0468 | LOSS: 0.7972 | ACC 0.7681\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0104/0468 | LOSS: 0.7966 | ACC 0.7682\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0105/0468 | LOSS: 0.7952 | ACC 0.7686\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0106/0468 | LOSS: 0.7948 | ACC 0.7689\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0107/0468 | LOSS: 0.7943 | ACC 0.7691\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0108/0468 | LOSS: 0.7939 | ACC 0.7693\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0109/0468 | LOSS: 0.7929 | ACC 0.7695\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0110/0468 | LOSS: 0.7913 | ACC 0.7702\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0111/0468 | LOSS: 0.7922 | ACC 0.7695\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0112/0468 | LOSS: 0.7899 | ACC 0.7703\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0113/0468 | LOSS: 0.7889 | ACC 0.7708\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0114/0468 | LOSS: 0.7879 | ACC 0.7710\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0115/0468 | LOSS: 0.7883 | ACC 0.7710\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0116/0468 | LOSS: 0.7863 | ACC 0.7716\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0117/0468 | LOSS: 0.7854 | ACC 0.7722\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0118/0468 | LOSS: 0.7838 | ACC 0.7727\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0119/0468 | LOSS: 0.7815 | ACC 0.7734\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0120/0468 | LOSS: 0.7818 | ACC 0.7731\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0121/0468 | LOSS: 0.7819 | ACC 0.7736\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0122/0468 | LOSS: 0.7813 | ACC 0.7741\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0123/0468 | LOSS: 0.7800 | ACC 0.7744\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0124/0468 | LOSS: 0.7794 | ACC 0.7744\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0125/0468 | LOSS: 0.7784 | ACC 0.7744\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0126/0468 | LOSS: 0.7775 | ACC 0.7744\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0127/0468 | LOSS: 0.7768 | ACC 0.7747\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0128/0468 | LOSS: 0.7759 | ACC 0.7752\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0129/0468 | LOSS: 0.7749 | ACC 0.7756\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0130/0468 | LOSS: 0.7738 | ACC 0.7761\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0131/0468 | LOSS: 0.7727 | ACC 0.7763\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0132/0468 | LOSS: 0.7718 | ACC 0.7764\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0133/0468 | LOSS: 0.7705 | ACC 0.7769\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0134/0468 | LOSS: 0.7697 | ACC 0.7774\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0135/0468 | LOSS: 0.7689 | ACC 0.7772\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0136/0468 | LOSS: 0.7672 | ACC 0.7779\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0137/0468 | LOSS: 0.7660 | ACC 0.7783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0002/0005 | BATCH 0138/0468 | LOSS: 0.7653 | ACC 0.7783\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0139/0468 | LOSS: 0.7658 | ACC 0.7781\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0140/0468 | LOSS: 0.7655 | ACC 0.7782\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0141/0468 | LOSS: 0.7643 | ACC 0.7784\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0142/0468 | LOSS: 0.7641 | ACC 0.7782\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0143/0468 | LOSS: 0.7628 | ACC 0.7788\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0144/0468 | LOSS: 0.7630 | ACC 0.7792\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0145/0468 | LOSS: 0.7622 | ACC 0.7798\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0146/0468 | LOSS: 0.7620 | ACC 0.7799\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0147/0468 | LOSS: 0.7609 | ACC 0.7803\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0148/0468 | LOSS: 0.7594 | ACC 0.7803\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0149/0468 | LOSS: 0.7585 | ACC 0.7804\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0150/0468 | LOSS: 0.7577 | ACC 0.7808\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0151/0468 | LOSS: 0.7565 | ACC 0.7810\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0152/0468 | LOSS: 0.7554 | ACC 0.7813\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0153/0468 | LOSS: 0.7532 | ACC 0.7819\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0154/0468 | LOSS: 0.7524 | ACC 0.7822\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0155/0468 | LOSS: 0.7520 | ACC 0.7824\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0156/0468 | LOSS: 0.7511 | ACC 0.7827\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0157/0468 | LOSS: 0.7500 | ACC 0.7833\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0158/0468 | LOSS: 0.7486 | ACC 0.7837\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0159/0468 | LOSS: 0.7482 | ACC 0.7839\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0160/0468 | LOSS: 0.7473 | ACC 0.7842\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0161/0468 | LOSS: 0.7459 | ACC 0.7845\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0162/0468 | LOSS: 0.7458 | ACC 0.7847\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0163/0468 | LOSS: 0.7448 | ACC 0.7851\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0164/0468 | LOSS: 0.7437 | ACC 0.7850\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0165/0468 | LOSS: 0.7437 | ACC 0.7851\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0166/0468 | LOSS: 0.7432 | ACC 0.7852\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0167/0468 | LOSS: 0.7428 | ACC 0.7852\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0168/0468 | LOSS: 0.7421 | ACC 0.7854\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0169/0468 | LOSS: 0.7417 | ACC 0.7858\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0170/0468 | LOSS: 0.7404 | ACC 0.7861\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0171/0468 | LOSS: 0.7395 | ACC 0.7862\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0172/0468 | LOSS: 0.7386 | ACC 0.7865\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0173/0468 | LOSS: 0.7380 | ACC 0.7868\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0174/0468 | LOSS: 0.7366 | ACC 0.7871\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0175/0468 | LOSS: 0.7360 | ACC 0.7873\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0176/0468 | LOSS: 0.7352 | ACC 0.7876\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0177/0468 | LOSS: 0.7338 | ACC 0.7879\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0178/0468 | LOSS: 0.7335 | ACC 0.7878\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0179/0468 | LOSS: 0.7322 | ACC 0.7882\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0180/0468 | LOSS: 0.7309 | ACC 0.7883\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0181/0468 | LOSS: 0.7301 | ACC 0.7883\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0182/0468 | LOSS: 0.7294 | ACC 0.7885\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0183/0468 | LOSS: 0.7279 | ACC 0.7890\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0184/0468 | LOSS: 0.7268 | ACC 0.7890\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0185/0468 | LOSS: 0.7256 | ACC 0.7896\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0186/0468 | LOSS: 0.7247 | ACC 0.7899\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0187/0468 | LOSS: 0.7235 | ACC 0.7905\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0188/0468 | LOSS: 0.7223 | ACC 0.7908\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0189/0468 | LOSS: 0.7212 | ACC 0.7913\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0190/0468 | LOSS: 0.7206 | ACC 0.7912\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0191/0468 | LOSS: 0.7199 | ACC 0.7912\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0192/0468 | LOSS: 0.7191 | ACC 0.7914\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0193/0468 | LOSS: 0.7190 | ACC 0.7914\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0194/0468 | LOSS: 0.7180 | ACC 0.7916\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0195/0468 | LOSS: 0.7169 | ACC 0.7919\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0196/0468 | LOSS: 0.7157 | ACC 0.7922\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0197/0468 | LOSS: 0.7148 | ACC 0.7926\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0198/0468 | LOSS: 0.7139 | ACC 0.7929\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0199/0468 | LOSS: 0.7129 | ACC 0.7932\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0200/0468 | LOSS: 0.7120 | ACC 0.7936\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0201/0468 | LOSS: 0.7106 | ACC 0.7940\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0202/0468 | LOSS: 0.7095 | ACC 0.7943\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0203/0468 | LOSS: 0.7084 | ACC 0.7947\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0204/0468 | LOSS: 0.7076 | ACC 0.7949\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0205/0468 | LOSS: 0.7067 | ACC 0.7951\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0206/0468 | LOSS: 0.7055 | ACC 0.7955\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0207/0468 | LOSS: 0.7044 | ACC 0.7959\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0208/0468 | LOSS: 0.7037 | ACC 0.7962\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0209/0468 | LOSS: 0.7025 | ACC 0.7964\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0210/0468 | LOSS: 0.7013 | ACC 0.7968\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0211/0468 | LOSS: 0.7006 | ACC 0.7969\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0212/0468 | LOSS: 0.6996 | ACC 0.7972\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0213/0468 | LOSS: 0.6987 | ACC 0.7975\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0214/0468 | LOSS: 0.6980 | ACC 0.7976\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0215/0468 | LOSS: 0.6974 | ACC 0.7978\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0216/0468 | LOSS: 0.6967 | ACC 0.7979\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0217/0468 | LOSS: 0.6954 | ACC 0.7983\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0218/0468 | LOSS: 0.6948 | ACC 0.7983\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0219/0468 | LOSS: 0.6942 | ACC 0.7983\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0220/0468 | LOSS: 0.6935 | ACC 0.7985\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0221/0468 | LOSS: 0.6921 | ACC 0.7988\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0222/0468 | LOSS: 0.6915 | ACC 0.7989\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0223/0468 | LOSS: 0.6913 | ACC 0.7989\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0224/0468 | LOSS: 0.6903 | ACC 0.7992\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0225/0468 | LOSS: 0.6894 | ACC 0.7993\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0226/0468 | LOSS: 0.6887 | ACC 0.7995\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0227/0468 | LOSS: 0.6880 | ACC 0.7998\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0228/0468 | LOSS: 0.6867 | ACC 0.8001\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0229/0468 | LOSS: 0.6858 | ACC 0.8003\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0230/0468 | LOSS: 0.6849 | ACC 0.8007\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0231/0468 | LOSS: 0.6841 | ACC 0.8008\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0232/0468 | LOSS: 0.6833 | ACC 0.8011\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0233/0468 | LOSS: 0.6823 | ACC 0.8013\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0234/0468 | LOSS: 0.6819 | ACC 0.8015\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0235/0468 | LOSS: 0.6812 | ACC 0.8017\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0236/0468 | LOSS: 0.6804 | ACC 0.8019\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0237/0468 | LOSS: 0.6799 | ACC 0.8019\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0238/0468 | LOSS: 0.6791 | ACC 0.8022\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0239/0468 | LOSS: 0.6787 | ACC 0.8023\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0240/0468 | LOSS: 0.6780 | ACC 0.8025\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0241/0468 | LOSS: 0.6776 | ACC 0.8026\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0242/0468 | LOSS: 0.6768 | ACC 0.8030\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0243/0468 | LOSS: 0.6764 | ACC 0.8032\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0244/0468 | LOSS: 0.6753 | ACC 0.8035\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0245/0468 | LOSS: 0.6745 | ACC 0.8037\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0246/0468 | LOSS: 0.6738 | ACC 0.8038\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0247/0468 | LOSS: 0.6734 | ACC 0.8039\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0248/0468 | LOSS: 0.6726 | ACC 0.8042\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0249/0468 | LOSS: 0.6715 | ACC 0.8044\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0250/0468 | LOSS: 0.6707 | ACC 0.8047\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0251/0468 | LOSS: 0.6696 | ACC 0.8049\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0252/0468 | LOSS: 0.6691 | ACC 0.8051\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0253/0468 | LOSS: 0.6682 | ACC 0.8054\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0254/0468 | LOSS: 0.6672 | ACC 0.8057\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0255/0468 | LOSS: 0.6666 | ACC 0.8060\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0256/0468 | LOSS: 0.6658 | ACC 0.8062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0002/0005 | BATCH 0257/0468 | LOSS: 0.6649 | ACC 0.8064\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0258/0468 | LOSS: 0.6639 | ACC 0.8069\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0259/0468 | LOSS: 0.6635 | ACC 0.8071\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0260/0468 | LOSS: 0.6625 | ACC 0.8074\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0261/0468 | LOSS: 0.6620 | ACC 0.8075\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0262/0468 | LOSS: 0.6614 | ACC 0.8077\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0263/0468 | LOSS: 0.6611 | ACC 0.8079\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0264/0468 | LOSS: 0.6604 | ACC 0.8079\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0265/0468 | LOSS: 0.6593 | ACC 0.8084\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0266/0468 | LOSS: 0.6591 | ACC 0.8085\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0267/0468 | LOSS: 0.6582 | ACC 0.8087\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0268/0468 | LOSS: 0.6574 | ACC 0.8090\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0269/0468 | LOSS: 0.6570 | ACC 0.8091\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0270/0468 | LOSS: 0.6562 | ACC 0.8094\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0271/0468 | LOSS: 0.6555 | ACC 0.8096\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0272/0468 | LOSS: 0.6550 | ACC 0.8097\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0273/0468 | LOSS: 0.6541 | ACC 0.8100\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0274/0468 | LOSS: 0.6534 | ACC 0.8102\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0275/0468 | LOSS: 0.6536 | ACC 0.8101\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0276/0468 | LOSS: 0.6525 | ACC 0.8105\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0277/0468 | LOSS: 0.6515 | ACC 0.8108\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0278/0468 | LOSS: 0.6508 | ACC 0.8109\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0279/0468 | LOSS: 0.6503 | ACC 0.8110\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0280/0468 | LOSS: 0.6497 | ACC 0.8111\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0281/0468 | LOSS: 0.6488 | ACC 0.8115\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0282/0468 | LOSS: 0.6482 | ACC 0.8118\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0283/0468 | LOSS: 0.6480 | ACC 0.8118\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0284/0468 | LOSS: 0.6472 | ACC 0.8121\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0285/0468 | LOSS: 0.6463 | ACC 0.8125\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0286/0468 | LOSS: 0.6455 | ACC 0.8126\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0287/0468 | LOSS: 0.6456 | ACC 0.8126\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0288/0468 | LOSS: 0.6448 | ACC 0.8128\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0289/0468 | LOSS: 0.6443 | ACC 0.8129\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0290/0468 | LOSS: 0.6432 | ACC 0.8133\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0291/0468 | LOSS: 0.6424 | ACC 0.8135\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0292/0468 | LOSS: 0.6415 | ACC 0.8137\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0293/0468 | LOSS: 0.6405 | ACC 0.8141\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0294/0468 | LOSS: 0.6398 | ACC 0.8142\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0295/0468 | LOSS: 0.6390 | ACC 0.8145\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0296/0468 | LOSS: 0.6385 | ACC 0.8146\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0297/0468 | LOSS: 0.6377 | ACC 0.8148\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0298/0468 | LOSS: 0.6370 | ACC 0.8149\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0299/0468 | LOSS: 0.6365 | ACC 0.8150\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0300/0468 | LOSS: 0.6356 | ACC 0.8151\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0301/0468 | LOSS: 0.6344 | ACC 0.8154\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0302/0468 | LOSS: 0.6336 | ACC 0.8157\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0303/0468 | LOSS: 0.6326 | ACC 0.8161\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0304/0468 | LOSS: 0.6324 | ACC 0.8161\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0305/0468 | LOSS: 0.6318 | ACC 0.8163\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0306/0468 | LOSS: 0.6313 | ACC 0.8165\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0307/0468 | LOSS: 0.6304 | ACC 0.8168\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0308/0468 | LOSS: 0.6299 | ACC 0.8170\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0309/0468 | LOSS: 0.6296 | ACC 0.8171\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0310/0468 | LOSS: 0.6287 | ACC 0.8174\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0311/0468 | LOSS: 0.6280 | ACC 0.8176\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0312/0468 | LOSS: 0.6272 | ACC 0.8178\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0313/0468 | LOSS: 0.6267 | ACC 0.8180\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0314/0468 | LOSS: 0.6259 | ACC 0.8182\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0315/0468 | LOSS: 0.6254 | ACC 0.8184\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0316/0468 | LOSS: 0.6247 | ACC 0.8186\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0317/0468 | LOSS: 0.6241 | ACC 0.8188\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0318/0468 | LOSS: 0.6236 | ACC 0.8190\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0319/0468 | LOSS: 0.6224 | ACC 0.8194\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0320/0468 | LOSS: 0.6218 | ACC 0.8195\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0321/0468 | LOSS: 0.6214 | ACC 0.8196\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0322/0468 | LOSS: 0.6205 | ACC 0.8199\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0323/0468 | LOSS: 0.6199 | ACC 0.8200\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0324/0468 | LOSS: 0.6192 | ACC 0.8201\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0325/0468 | LOSS: 0.6187 | ACC 0.8203\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0326/0468 | LOSS: 0.6179 | ACC 0.8206\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0327/0468 | LOSS: 0.6172 | ACC 0.8208\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0328/0468 | LOSS: 0.6164 | ACC 0.8211\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0329/0468 | LOSS: 0.6157 | ACC 0.8214\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0330/0468 | LOSS: 0.6150 | ACC 0.8216\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0331/0468 | LOSS: 0.6149 | ACC 0.8216\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0332/0468 | LOSS: 0.6142 | ACC 0.8219\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0333/0468 | LOSS: 0.6132 | ACC 0.8221\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0334/0468 | LOSS: 0.6125 | ACC 0.8223\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0335/0468 | LOSS: 0.6119 | ACC 0.8224\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0336/0468 | LOSS: 0.6111 | ACC 0.8227\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0337/0468 | LOSS: 0.6108 | ACC 0.8230\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0338/0468 | LOSS: 0.6099 | ACC 0.8232\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0339/0468 | LOSS: 0.6093 | ACC 0.8234\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0340/0468 | LOSS: 0.6091 | ACC 0.8235\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0341/0468 | LOSS: 0.6086 | ACC 0.8236\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0342/0468 | LOSS: 0.6080 | ACC 0.8237\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0343/0468 | LOSS: 0.6074 | ACC 0.8239\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0344/0468 | LOSS: 0.6067 | ACC 0.8241\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0345/0468 | LOSS: 0.6062 | ACC 0.8243\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0346/0468 | LOSS: 0.6054 | ACC 0.8245\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0347/0468 | LOSS: 0.6046 | ACC 0.8247\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0348/0468 | LOSS: 0.6042 | ACC 0.8249\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0349/0468 | LOSS: 0.6034 | ACC 0.8252\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0350/0468 | LOSS: 0.6026 | ACC 0.8254\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0351/0468 | LOSS: 0.6020 | ACC 0.8256\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0352/0468 | LOSS: 0.6013 | ACC 0.8259\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0353/0468 | LOSS: 0.6005 | ACC 0.8261\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0354/0468 | LOSS: 0.6000 | ACC 0.8262\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0355/0468 | LOSS: 0.5994 | ACC 0.8263\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0356/0468 | LOSS: 0.5991 | ACC 0.8264\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0357/0468 | LOSS: 0.5984 | ACC 0.8266\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0358/0468 | LOSS: 0.5978 | ACC 0.8269\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0359/0468 | LOSS: 0.5974 | ACC 0.8271\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0360/0468 | LOSS: 0.5967 | ACC 0.8272\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0361/0468 | LOSS: 0.5961 | ACC 0.8275\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0362/0468 | LOSS: 0.5954 | ACC 0.8277\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0363/0468 | LOSS: 0.5947 | ACC 0.8279\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0364/0468 | LOSS: 0.5939 | ACC 0.8282\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0365/0468 | LOSS: 0.5931 | ACC 0.8285\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0366/0468 | LOSS: 0.5924 | ACC 0.8287\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0367/0468 | LOSS: 0.5915 | ACC 0.8290\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0368/0468 | LOSS: 0.5907 | ACC 0.8292\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0369/0468 | LOSS: 0.5902 | ACC 0.8293\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0370/0468 | LOSS: 0.5895 | ACC 0.8294\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0371/0468 | LOSS: 0.5889 | ACC 0.8296\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0372/0468 | LOSS: 0.5883 | ACC 0.8298\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0373/0468 | LOSS: 0.5873 | ACC 0.8301\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0374/0468 | LOSS: 0.5868 | ACC 0.8303\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0375/0468 | LOSS: 0.5863 | ACC 0.8305\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0376/0468 | LOSS: 0.5857 | ACC 0.8307\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0377/0468 | LOSS: 0.5853 | ACC 0.8307\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0378/0468 | LOSS: 0.5846 | ACC 0.8309\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0379/0468 | LOSS: 0.5839 | ACC 0.8311\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0380/0468 | LOSS: 0.5833 | ACC 0.8314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0002/0005 | BATCH 0381/0468 | LOSS: 0.5828 | ACC 0.8316\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0382/0468 | LOSS: 0.5821 | ACC 0.8318\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0383/0468 | LOSS: 0.5816 | ACC 0.8320\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0384/0468 | LOSS: 0.5810 | ACC 0.8321\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0385/0468 | LOSS: 0.5804 | ACC 0.8323\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0386/0468 | LOSS: 0.5798 | ACC 0.8324\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0387/0468 | LOSS: 0.5792 | ACC 0.8326\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0388/0468 | LOSS: 0.5786 | ACC 0.8328\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0389/0468 | LOSS: 0.5781 | ACC 0.8329\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0390/0468 | LOSS: 0.5778 | ACC 0.8329\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0391/0468 | LOSS: 0.5772 | ACC 0.8332\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0392/0468 | LOSS: 0.5767 | ACC 0.8333\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0393/0468 | LOSS: 0.5762 | ACC 0.8334\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0394/0468 | LOSS: 0.5758 | ACC 0.8335\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0395/0468 | LOSS: 0.5752 | ACC 0.8336\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0396/0468 | LOSS: 0.5748 | ACC 0.8337\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0397/0468 | LOSS: 0.5742 | ACC 0.8338\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0398/0468 | LOSS: 0.5736 | ACC 0.8340\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0399/0468 | LOSS: 0.5732 | ACC 0.8340\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0400/0468 | LOSS: 0.5727 | ACC 0.8341\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0401/0468 | LOSS: 0.5720 | ACC 0.8344\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0402/0468 | LOSS: 0.5714 | ACC 0.8346\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0403/0468 | LOSS: 0.5707 | ACC 0.8348\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0404/0468 | LOSS: 0.5700 | ACC 0.8350\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0405/0468 | LOSS: 0.5696 | ACC 0.8351\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0406/0468 | LOSS: 0.5689 | ACC 0.8354\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0407/0468 | LOSS: 0.5681 | ACC 0.8357\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0408/0468 | LOSS: 0.5674 | ACC 0.8359\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0409/0468 | LOSS: 0.5669 | ACC 0.8361\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0410/0468 | LOSS: 0.5665 | ACC 0.8362\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0411/0468 | LOSS: 0.5663 | ACC 0.8362\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0412/0468 | LOSS: 0.5659 | ACC 0.8363\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0413/0468 | LOSS: 0.5652 | ACC 0.8365\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0414/0468 | LOSS: 0.5644 | ACC 0.8368\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0415/0468 | LOSS: 0.5638 | ACC 0.8370\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0416/0468 | LOSS: 0.5633 | ACC 0.8371\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0417/0468 | LOSS: 0.5625 | ACC 0.8374\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0418/0468 | LOSS: 0.5619 | ACC 0.8375\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0419/0468 | LOSS: 0.5613 | ACC 0.8377\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0420/0468 | LOSS: 0.5609 | ACC 0.8378\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0421/0468 | LOSS: 0.5604 | ACC 0.8380\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0422/0468 | LOSS: 0.5595 | ACC 0.8383\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0423/0468 | LOSS: 0.5588 | ACC 0.8385\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0424/0468 | LOSS: 0.5583 | ACC 0.8387\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0425/0468 | LOSS: 0.5577 | ACC 0.8388\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0426/0468 | LOSS: 0.5572 | ACC 0.8390\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0427/0468 | LOSS: 0.5564 | ACC 0.8392\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0428/0468 | LOSS: 0.5556 | ACC 0.8394\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0429/0468 | LOSS: 0.5552 | ACC 0.8396\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0430/0468 | LOSS: 0.5545 | ACC 0.8397\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0431/0468 | LOSS: 0.5541 | ACC 0.8398\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0432/0468 | LOSS: 0.5535 | ACC 0.8400\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0433/0468 | LOSS: 0.5530 | ACC 0.8401\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0434/0468 | LOSS: 0.5523 | ACC 0.8403\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0435/0468 | LOSS: 0.5515 | ACC 0.8406\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0436/0468 | LOSS: 0.5513 | ACC 0.8406\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0437/0468 | LOSS: 0.5506 | ACC 0.8408\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0438/0468 | LOSS: 0.5499 | ACC 0.8410\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0439/0468 | LOSS: 0.5494 | ACC 0.8411\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0440/0468 | LOSS: 0.5487 | ACC 0.8413\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0441/0468 | LOSS: 0.5481 | ACC 0.8414\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0442/0468 | LOSS: 0.5474 | ACC 0.8417\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0443/0468 | LOSS: 0.5472 | ACC 0.8417\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0444/0468 | LOSS: 0.5466 | ACC 0.8419\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0445/0468 | LOSS: 0.5461 | ACC 0.8421\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0446/0468 | LOSS: 0.5455 | ACC 0.8423\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0447/0468 | LOSS: 0.5454 | ACC 0.8423\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0448/0468 | LOSS: 0.5449 | ACC 0.8425\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0449/0468 | LOSS: 0.5444 | ACC 0.8426\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0450/0468 | LOSS: 0.5439 | ACC 0.8427\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0451/0468 | LOSS: 0.5436 | ACC 0.8428\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0452/0468 | LOSS: 0.5430 | ACC 0.8430\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0453/0468 | LOSS: 0.5425 | ACC 0.8432\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0454/0468 | LOSS: 0.5419 | ACC 0.8433\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0455/0468 | LOSS: 0.5413 | ACC 0.8434\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0456/0468 | LOSS: 0.5407 | ACC 0.8437\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0457/0468 | LOSS: 0.5402 | ACC 0.8438\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0458/0468 | LOSS: 0.5397 | ACC 0.8441\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0459/0468 | LOSS: 0.5390 | ACC 0.8442\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0460/0468 | LOSS: 0.5385 | ACC 0.8444\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0461/0468 | LOSS: 0.5380 | ACC 0.8445\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0462/0468 | LOSS: 0.5375 | ACC 0.8446\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0463/0468 | LOSS: 0.5370 | ACC 0.8448\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0464/0468 | LOSS: 0.5364 | ACC 0.8450\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0465/0468 | LOSS: 0.5360 | ACC 0.8452\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0466/0468 | LOSS: 0.5353 | ACC 0.8454\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0467/0468 | LOSS: 0.5347 | ACC 0.8456\n",
      "TRAIN: EPOCH 0002/0005 | BATCH 0468/0468 | LOSS: 0.5342 | ACC 0.8458\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0000/0468 | LOSS: 0.1580 | ACC 0.9688\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0001/0468 | LOSS: 0.1958 | ACC 0.9492\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0002/0468 | LOSS: 0.2849 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0003/0468 | LOSS: 0.2745 | ACC 0.9277\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0004/0468 | LOSS: 0.2941 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0005/0468 | LOSS: 0.3046 | ACC 0.9141\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0006/0468 | LOSS: 0.3013 | ACC 0.9152\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0007/0468 | LOSS: 0.3065 | ACC 0.9121\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0008/0468 | LOSS: 0.3105 | ACC 0.9115\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0009/0468 | LOSS: 0.3055 | ACC 0.9117\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0010/0468 | LOSS: 0.3106 | ACC 0.9119\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0011/0468 | LOSS: 0.3086 | ACC 0.9115\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0012/0468 | LOSS: 0.3105 | ACC 0.9123\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0013/0468 | LOSS: 0.3196 | ACC 0.9102\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0014/0468 | LOSS: 0.3196 | ACC 0.9115\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0015/0468 | LOSS: 0.3133 | ACC 0.9146\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0016/0468 | LOSS: 0.3093 | ACC 0.9159\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0017/0468 | LOSS: 0.3119 | ACC 0.9167\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0018/0468 | LOSS: 0.3090 | ACC 0.9169\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0019/0468 | LOSS: 0.3090 | ACC 0.9164\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0020/0468 | LOSS: 0.3089 | ACC 0.9170\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0021/0468 | LOSS: 0.3067 | ACC 0.9173\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0022/0468 | LOSS: 0.3056 | ACC 0.9164\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0023/0468 | LOSS: 0.3074 | ACC 0.9154\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0024/0468 | LOSS: 0.3052 | ACC 0.9163\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0025/0468 | LOSS: 0.3049 | ACC 0.9168\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0026/0468 | LOSS: 0.3031 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0027/0468 | LOSS: 0.3027 | ACC 0.9171\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0028/0468 | LOSS: 0.3044 | ACC 0.9159\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0029/0468 | LOSS: 0.3028 | ACC 0.9169\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0030/0468 | LOSS: 0.3042 | ACC 0.9153\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0031/0468 | LOSS: 0.3022 | ACC 0.9163\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0032/0468 | LOSS: 0.2994 | ACC 0.9171\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0033/0468 | LOSS: 0.3004 | ACC 0.9166\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0034/0468 | LOSS: 0.2980 | ACC 0.9172\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0035/0468 | LOSS: 0.2965 | ACC 0.9173\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0036/0468 | LOSS: 0.2971 | ACC 0.9172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0003/0005 | BATCH 0037/0468 | LOSS: 0.2979 | ACC 0.9169\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0038/0468 | LOSS: 0.2943 | ACC 0.9181\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0039/0468 | LOSS: 0.2924 | ACC 0.9191\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0040/0468 | LOSS: 0.2911 | ACC 0.9194\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0041/0468 | LOSS: 0.2925 | ACC 0.9191\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0042/0468 | LOSS: 0.2920 | ACC 0.9197\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0043/0468 | LOSS: 0.2902 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0044/0468 | LOSS: 0.2894 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0045/0468 | LOSS: 0.2887 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0046/0468 | LOSS: 0.2892 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0047/0468 | LOSS: 0.2890 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0048/0468 | LOSS: 0.2911 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0049/0468 | LOSS: 0.2923 | ACC 0.9206\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0050/0468 | LOSS: 0.2914 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0051/0468 | LOSS: 0.2945 | ACC 0.9201\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0052/0468 | LOSS: 0.2915 | ACC 0.9210\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0053/0468 | LOSS: 0.2914 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0054/0468 | LOSS: 0.2915 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0055/0468 | LOSS: 0.2920 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0056/0468 | LOSS: 0.2912 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0057/0468 | LOSS: 0.2900 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0058/0468 | LOSS: 0.2884 | ACC 0.9220\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0059/0468 | LOSS: 0.2868 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0060/0468 | LOSS: 0.2871 | ACC 0.9223\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0061/0468 | LOSS: 0.2869 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0062/0468 | LOSS: 0.2854 | ACC 0.9226\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0063/0468 | LOSS: 0.2860 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0064/0468 | LOSS: 0.2868 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0065/0468 | LOSS: 0.2862 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0066/0468 | LOSS: 0.2853 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0067/0468 | LOSS: 0.2842 | ACC 0.9226\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0068/0468 | LOSS: 0.2832 | ACC 0.9226\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0069/0468 | LOSS: 0.2847 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0070/0468 | LOSS: 0.2840 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0071/0468 | LOSS: 0.2848 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0072/0468 | LOSS: 0.2848 | ACC 0.9216\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0073/0468 | LOSS: 0.2852 | ACC 0.9211\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0074/0468 | LOSS: 0.2850 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0075/0468 | LOSS: 0.2866 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0076/0468 | LOSS: 0.2869 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0077/0468 | LOSS: 0.2871 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0078/0468 | LOSS: 0.2865 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0079/0468 | LOSS: 0.2877 | ACC 0.9204\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0080/0468 | LOSS: 0.2874 | ACC 0.9205\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0081/0468 | LOSS: 0.2865 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0082/0468 | LOSS: 0.2866 | ACC 0.9208\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0083/0468 | LOSS: 0.2856 | ACC 0.9209\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0084/0468 | LOSS: 0.2852 | ACC 0.9212\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0085/0468 | LOSS: 0.2852 | ACC 0.9213\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0086/0468 | LOSS: 0.2844 | ACC 0.9215\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0087/0468 | LOSS: 0.2840 | ACC 0.9216\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0088/0468 | LOSS: 0.2841 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0089/0468 | LOSS: 0.2844 | ACC 0.9216\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0090/0468 | LOSS: 0.2836 | ACC 0.9217\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0091/0468 | LOSS: 0.2848 | ACC 0.9214\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0092/0468 | LOSS: 0.2835 | ACC 0.9219\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0093/0468 | LOSS: 0.2825 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0094/0468 | LOSS: 0.2815 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0095/0468 | LOSS: 0.2816 | ACC 0.9224\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0096/0468 | LOSS: 0.2817 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0097/0468 | LOSS: 0.2817 | ACC 0.9218\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0098/0468 | LOSS: 0.2806 | ACC 0.9222\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0099/0468 | LOSS: 0.2802 | ACC 0.9223\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0100/0468 | LOSS: 0.2805 | ACC 0.9221\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0101/0468 | LOSS: 0.2804 | ACC 0.9222\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0102/0468 | LOSS: 0.2801 | ACC 0.9222\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0103/0468 | LOSS: 0.2794 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0104/0468 | LOSS: 0.2788 | ACC 0.9225\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0105/0468 | LOSS: 0.2779 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0106/0468 | LOSS: 0.2777 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0107/0468 | LOSS: 0.2774 | ACC 0.9228\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0108/0468 | LOSS: 0.2774 | ACC 0.9231\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0109/0468 | LOSS: 0.2770 | ACC 0.9232\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0110/0468 | LOSS: 0.2767 | ACC 0.9234\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0111/0468 | LOSS: 0.2760 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0112/0468 | LOSS: 0.2754 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0113/0468 | LOSS: 0.2756 | ACC 0.9235\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0114/0468 | LOSS: 0.2750 | ACC 0.9236\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0115/0468 | LOSS: 0.2748 | ACC 0.9236\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0116/0468 | LOSS: 0.2750 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0117/0468 | LOSS: 0.2754 | ACC 0.9238\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0118/0468 | LOSS: 0.2751 | ACC 0.9236\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0119/0468 | LOSS: 0.2743 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0120/0468 | LOSS: 0.2743 | ACC 0.9237\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0121/0468 | LOSS: 0.2744 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0122/0468 | LOSS: 0.2745 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0123/0468 | LOSS: 0.2740 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0124/0468 | LOSS: 0.2739 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0125/0468 | LOSS: 0.2732 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0126/0468 | LOSS: 0.2737 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0127/0468 | LOSS: 0.2737 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0128/0468 | LOSS: 0.2737 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0129/0468 | LOSS: 0.2739 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0130/0468 | LOSS: 0.2733 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0131/0468 | LOSS: 0.2733 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0132/0468 | LOSS: 0.2732 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0133/0468 | LOSS: 0.2729 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0134/0468 | LOSS: 0.2729 | ACC 0.9242\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0135/0468 | LOSS: 0.2731 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0136/0468 | LOSS: 0.2730 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0137/0468 | LOSS: 0.2732 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0138/0468 | LOSS: 0.2731 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0139/0468 | LOSS: 0.2733 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0140/0468 | LOSS: 0.2731 | ACC 0.9239\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0141/0468 | LOSS: 0.2726 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0142/0468 | LOSS: 0.2722 | ACC 0.9242\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0143/0468 | LOSS: 0.2724 | ACC 0.9240\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0144/0468 | LOSS: 0.2723 | ACC 0.9241\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0145/0468 | LOSS: 0.2717 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0146/0468 | LOSS: 0.2716 | ACC 0.9243\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0147/0468 | LOSS: 0.2707 | ACC 0.9246\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0148/0468 | LOSS: 0.2706 | ACC 0.9245\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0149/0468 | LOSS: 0.2704 | ACC 0.9246\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0150/0468 | LOSS: 0.2700 | ACC 0.9247\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0151/0468 | LOSS: 0.2694 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0152/0468 | LOSS: 0.2692 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0153/0468 | LOSS: 0.2685 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0154/0468 | LOSS: 0.2683 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0155/0468 | LOSS: 0.2681 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0156/0468 | LOSS: 0.2683 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0157/0468 | LOSS: 0.2681 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0158/0468 | LOSS: 0.2676 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0159/0468 | LOSS: 0.2670 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0160/0468 | LOSS: 0.2663 | ACC 0.9254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0003/0005 | BATCH 0161/0468 | LOSS: 0.2667 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0162/0468 | LOSS: 0.2671 | ACC 0.9253\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0163/0468 | LOSS: 0.2673 | ACC 0.9251\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0164/0468 | LOSS: 0.2671 | ACC 0.9251\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0165/0468 | LOSS: 0.2667 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0166/0468 | LOSS: 0.2662 | ACC 0.9251\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0167/0468 | LOSS: 0.2659 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0168/0468 | LOSS: 0.2659 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0169/0468 | LOSS: 0.2660 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0170/0468 | LOSS: 0.2661 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0171/0468 | LOSS: 0.2658 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0172/0468 | LOSS: 0.2656 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0173/0468 | LOSS: 0.2651 | ACC 0.9253\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0174/0468 | LOSS: 0.2657 | ACC 0.9250\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0175/0468 | LOSS: 0.2660 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0176/0468 | LOSS: 0.2664 | ACC 0.9246\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0177/0468 | LOSS: 0.2661 | ACC 0.9247\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0178/0468 | LOSS: 0.2657 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0179/0468 | LOSS: 0.2658 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0180/0468 | LOSS: 0.2662 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0181/0468 | LOSS: 0.2663 | ACC 0.9248\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0182/0468 | LOSS: 0.2661 | ACC 0.9249\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0183/0468 | LOSS: 0.2655 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0184/0468 | LOSS: 0.2650 | ACC 0.9253\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0185/0468 | LOSS: 0.2648 | ACC 0.9256\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0186/0468 | LOSS: 0.2651 | ACC 0.9255\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0187/0468 | LOSS: 0.2655 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0188/0468 | LOSS: 0.2658 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0189/0468 | LOSS: 0.2657 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0190/0468 | LOSS: 0.2655 | ACC 0.9253\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0191/0468 | LOSS: 0.2657 | ACC 0.9252\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0192/0468 | LOSS: 0.2651 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0193/0468 | LOSS: 0.2651 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0194/0468 | LOSS: 0.2657 | ACC 0.9253\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0195/0468 | LOSS: 0.2659 | ACC 0.9253\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0196/0468 | LOSS: 0.2655 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0197/0468 | LOSS: 0.2651 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0198/0468 | LOSS: 0.2650 | ACC 0.9254\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0199/0468 | LOSS: 0.2652 | ACC 0.9255\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0200/0468 | LOSS: 0.2646 | ACC 0.9257\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0201/0468 | LOSS: 0.2643 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0202/0468 | LOSS: 0.2640 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0203/0468 | LOSS: 0.2643 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0204/0468 | LOSS: 0.2645 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0205/0468 | LOSS: 0.2647 | ACC 0.9258\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0206/0468 | LOSS: 0.2643 | ACC 0.9259\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0207/0468 | LOSS: 0.2638 | ACC 0.9261\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0208/0468 | LOSS: 0.2635 | ACC 0.9262\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0209/0468 | LOSS: 0.2630 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0210/0468 | LOSS: 0.2628 | ACC 0.9265\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0211/0468 | LOSS: 0.2630 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0212/0468 | LOSS: 0.2627 | ACC 0.9265\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0213/0468 | LOSS: 0.2628 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0214/0468 | LOSS: 0.2632 | ACC 0.9263\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0215/0468 | LOSS: 0.2632 | ACC 0.9263\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0216/0468 | LOSS: 0.2628 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0217/0468 | LOSS: 0.2625 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0218/0468 | LOSS: 0.2622 | ACC 0.9265\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0219/0468 | LOSS: 0.2619 | ACC 0.9266\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0220/0468 | LOSS: 0.2622 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0221/0468 | LOSS: 0.2619 | ACC 0.9265\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0222/0468 | LOSS: 0.2618 | ACC 0.9266\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0223/0468 | LOSS: 0.2620 | ACC 0.9265\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0224/0468 | LOSS: 0.2619 | ACC 0.9266\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0225/0468 | LOSS: 0.2626 | ACC 0.9264\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0226/0468 | LOSS: 0.2620 | ACC 0.9267\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0227/0468 | LOSS: 0.2618 | ACC 0.9267\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0228/0468 | LOSS: 0.2615 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0229/0468 | LOSS: 0.2613 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0230/0468 | LOSS: 0.2611 | ACC 0.9267\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0231/0468 | LOSS: 0.2611 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0232/0468 | LOSS: 0.2608 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0233/0468 | LOSS: 0.2612 | ACC 0.9266\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0234/0468 | LOSS: 0.2608 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0235/0468 | LOSS: 0.2608 | ACC 0.9267\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0236/0468 | LOSS: 0.2608 | ACC 0.9267\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0237/0468 | LOSS: 0.2603 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0238/0468 | LOSS: 0.2603 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0239/0468 | LOSS: 0.2601 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0240/0468 | LOSS: 0.2600 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0241/0468 | LOSS: 0.2600 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0242/0468 | LOSS: 0.2596 | ACC 0.9270\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0243/0468 | LOSS: 0.2595 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0244/0468 | LOSS: 0.2593 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0245/0468 | LOSS: 0.2591 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0246/0468 | LOSS: 0.2591 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0247/0468 | LOSS: 0.2590 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0248/0468 | LOSS: 0.2593 | ACC 0.9267\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0249/0468 | LOSS: 0.2591 | ACC 0.9268\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0250/0468 | LOSS: 0.2587 | ACC 0.9269\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0251/0468 | LOSS: 0.2584 | ACC 0.9270\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0252/0468 | LOSS: 0.2584 | ACC 0.9271\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0253/0468 | LOSS: 0.2582 | ACC 0.9272\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0254/0468 | LOSS: 0.2580 | ACC 0.9273\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0255/0468 | LOSS: 0.2576 | ACC 0.9274\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0256/0468 | LOSS: 0.2576 | ACC 0.9274\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0257/0468 | LOSS: 0.2575 | ACC 0.9276\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0258/0468 | LOSS: 0.2574 | ACC 0.9277\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0259/0468 | LOSS: 0.2571 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0260/0468 | LOSS: 0.2571 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0261/0468 | LOSS: 0.2571 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0262/0468 | LOSS: 0.2571 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0263/0468 | LOSS: 0.2569 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0264/0468 | LOSS: 0.2566 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0265/0468 | LOSS: 0.2564 | ACC 0.9278\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0266/0468 | LOSS: 0.2561 | ACC 0.9279\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0267/0468 | LOSS: 0.2561 | ACC 0.9279\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0268/0468 | LOSS: 0.2561 | ACC 0.9279\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0269/0468 | LOSS: 0.2558 | ACC 0.9280\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0270/0468 | LOSS: 0.2560 | ACC 0.9280\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0271/0468 | LOSS: 0.2560 | ACC 0.9280\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0272/0468 | LOSS: 0.2560 | ACC 0.9280\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0273/0468 | LOSS: 0.2557 | ACC 0.9281\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0274/0468 | LOSS: 0.2554 | ACC 0.9281\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0275/0468 | LOSS: 0.2553 | ACC 0.9280\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0276/0468 | LOSS: 0.2550 | ACC 0.9281\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0277/0468 | LOSS: 0.2551 | ACC 0.9281\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0278/0468 | LOSS: 0.2555 | ACC 0.9281\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0279/0468 | LOSS: 0.2552 | ACC 0.9282\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0280/0468 | LOSS: 0.2548 | ACC 0.9283\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0281/0468 | LOSS: 0.2547 | ACC 0.9283\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0282/0468 | LOSS: 0.2545 | ACC 0.9283\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0283/0468 | LOSS: 0.2543 | ACC 0.9284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0003/0005 | BATCH 0284/0468 | LOSS: 0.2539 | ACC 0.9285\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0285/0468 | LOSS: 0.2538 | ACC 0.9285\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0286/0468 | LOSS: 0.2535 | ACC 0.9286\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0287/0468 | LOSS: 0.2535 | ACC 0.9287\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0288/0468 | LOSS: 0.2533 | ACC 0.9287\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0289/0468 | LOSS: 0.2533 | ACC 0.9286\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0290/0468 | LOSS: 0.2534 | ACC 0.9287\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0291/0468 | LOSS: 0.2531 | ACC 0.9288\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0292/0468 | LOSS: 0.2530 | ACC 0.9289\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0293/0468 | LOSS: 0.2528 | ACC 0.9289\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0294/0468 | LOSS: 0.2526 | ACC 0.9290\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0295/0468 | LOSS: 0.2525 | ACC 0.9290\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0296/0468 | LOSS: 0.2525 | ACC 0.9290\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0297/0468 | LOSS: 0.2522 | ACC 0.9291\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0298/0468 | LOSS: 0.2520 | ACC 0.9292\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0299/0468 | LOSS: 0.2521 | ACC 0.9291\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0300/0468 | LOSS: 0.2523 | ACC 0.9291\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0301/0468 | LOSS: 0.2521 | ACC 0.9291\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0302/0468 | LOSS: 0.2516 | ACC 0.9293\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0303/0468 | LOSS: 0.2513 | ACC 0.9294\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0304/0468 | LOSS: 0.2508 | ACC 0.9296\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0305/0468 | LOSS: 0.2508 | ACC 0.9296\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0306/0468 | LOSS: 0.2508 | ACC 0.9296\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0307/0468 | LOSS: 0.2507 | ACC 0.9296\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0308/0468 | LOSS: 0.2504 | ACC 0.9297\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0309/0468 | LOSS: 0.2499 | ACC 0.9299\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0310/0468 | LOSS: 0.2497 | ACC 0.9299\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0311/0468 | LOSS: 0.2494 | ACC 0.9300\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0312/0468 | LOSS: 0.2492 | ACC 0.9301\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0313/0468 | LOSS: 0.2493 | ACC 0.9301\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0314/0468 | LOSS: 0.2490 | ACC 0.9302\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0315/0468 | LOSS: 0.2490 | ACC 0.9302\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0316/0468 | LOSS: 0.2488 | ACC 0.9303\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0317/0468 | LOSS: 0.2485 | ACC 0.9303\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0318/0468 | LOSS: 0.2485 | ACC 0.9303\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0319/0468 | LOSS: 0.2484 | ACC 0.9303\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0320/0468 | LOSS: 0.2483 | ACC 0.9303\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0321/0468 | LOSS: 0.2481 | ACC 0.9304\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0322/0468 | LOSS: 0.2478 | ACC 0.9305\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0323/0468 | LOSS: 0.2476 | ACC 0.9306\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0324/0468 | LOSS: 0.2476 | ACC 0.9305\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0325/0468 | LOSS: 0.2475 | ACC 0.9306\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0326/0468 | LOSS: 0.2473 | ACC 0.9305\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0327/0468 | LOSS: 0.2470 | ACC 0.9306\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0328/0468 | LOSS: 0.2466 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0329/0468 | LOSS: 0.2464 | ACC 0.9308\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0330/0468 | LOSS: 0.2467 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0331/0468 | LOSS: 0.2467 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0332/0468 | LOSS: 0.2465 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0333/0468 | LOSS: 0.2463 | ACC 0.9308\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0334/0468 | LOSS: 0.2463 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0335/0468 | LOSS: 0.2464 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0336/0468 | LOSS: 0.2464 | ACC 0.9307\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0337/0468 | LOSS: 0.2464 | ACC 0.9308\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0338/0468 | LOSS: 0.2463 | ACC 0.9308\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0339/0468 | LOSS: 0.2461 | ACC 0.9308\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0340/0468 | LOSS: 0.2459 | ACC 0.9308\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0341/0468 | LOSS: 0.2455 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0342/0468 | LOSS: 0.2454 | ACC 0.9310\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0343/0468 | LOSS: 0.2456 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0344/0468 | LOSS: 0.2455 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0345/0468 | LOSS: 0.2455 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0346/0468 | LOSS: 0.2454 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0347/0468 | LOSS: 0.2455 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0348/0468 | LOSS: 0.2454 | ACC 0.9309\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0349/0468 | LOSS: 0.2451 | ACC 0.9310\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0350/0468 | LOSS: 0.2448 | ACC 0.9310\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0351/0468 | LOSS: 0.2447 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0352/0468 | LOSS: 0.2446 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0353/0468 | LOSS: 0.2445 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0354/0468 | LOSS: 0.2444 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0355/0468 | LOSS: 0.2443 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0356/0468 | LOSS: 0.2442 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0357/0468 | LOSS: 0.2443 | ACC 0.9310\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0358/0468 | LOSS: 0.2442 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0359/0468 | LOSS: 0.2439 | ACC 0.9311\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0360/0468 | LOSS: 0.2437 | ACC 0.9312\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0361/0468 | LOSS: 0.2437 | ACC 0.9312\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0362/0468 | LOSS: 0.2435 | ACC 0.9312\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0363/0468 | LOSS: 0.2433 | ACC 0.9312\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0364/0468 | LOSS: 0.2433 | ACC 0.9313\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0365/0468 | LOSS: 0.2432 | ACC 0.9313\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0366/0468 | LOSS: 0.2429 | ACC 0.9314\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0367/0468 | LOSS: 0.2426 | ACC 0.9315\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0368/0468 | LOSS: 0.2424 | ACC 0.9315\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0369/0468 | LOSS: 0.2422 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0370/0468 | LOSS: 0.2422 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0371/0468 | LOSS: 0.2424 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0372/0468 | LOSS: 0.2422 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0373/0468 | LOSS: 0.2420 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0374/0468 | LOSS: 0.2420 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0375/0468 | LOSS: 0.2420 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0376/0468 | LOSS: 0.2419 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0377/0468 | LOSS: 0.2418 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0378/0468 | LOSS: 0.2416 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0379/0468 | LOSS: 0.2415 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0380/0468 | LOSS: 0.2414 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0381/0468 | LOSS: 0.2414 | ACC 0.9316\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0382/0468 | LOSS: 0.2411 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0383/0468 | LOSS: 0.2411 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0384/0468 | LOSS: 0.2409 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0385/0468 | LOSS: 0.2410 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0386/0468 | LOSS: 0.2407 | ACC 0.9318\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0387/0468 | LOSS: 0.2405 | ACC 0.9318\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0388/0468 | LOSS: 0.2404 | ACC 0.9318\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0389/0468 | LOSS: 0.2402 | ACC 0.9317\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0390/0468 | LOSS: 0.2399 | ACC 0.9318\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0391/0468 | LOSS: 0.2399 | ACC 0.9318\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0392/0468 | LOSS: 0.2398 | ACC 0.9319\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0393/0468 | LOSS: 0.2396 | ACC 0.9320\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0394/0468 | LOSS: 0.2392 | ACC 0.9321\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0395/0468 | LOSS: 0.2391 | ACC 0.9322\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0396/0468 | LOSS: 0.2388 | ACC 0.9323\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0397/0468 | LOSS: 0.2390 | ACC 0.9322\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0398/0468 | LOSS: 0.2389 | ACC 0.9322\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0399/0468 | LOSS: 0.2386 | ACC 0.9323\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0400/0468 | LOSS: 0.2385 | ACC 0.9323\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0401/0468 | LOSS: 0.2387 | ACC 0.9323\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0402/0468 | LOSS: 0.2384 | ACC 0.9324\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0403/0468 | LOSS: 0.2383 | ACC 0.9324\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0404/0468 | LOSS: 0.2382 | ACC 0.9324\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0405/0468 | LOSS: 0.2380 | ACC 0.9325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0003/0005 | BATCH 0406/0468 | LOSS: 0.2377 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0407/0468 | LOSS: 0.2377 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0408/0468 | LOSS: 0.2376 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0409/0468 | LOSS: 0.2376 | ACC 0.9325\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0410/0468 | LOSS: 0.2374 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0411/0468 | LOSS: 0.2374 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0412/0468 | LOSS: 0.2374 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0413/0468 | LOSS: 0.2373 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0414/0468 | LOSS: 0.2373 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0415/0468 | LOSS: 0.2373 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0416/0468 | LOSS: 0.2373 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0417/0468 | LOSS: 0.2373 | ACC 0.9325\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0418/0468 | LOSS: 0.2371 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0419/0468 | LOSS: 0.2370 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0420/0468 | LOSS: 0.2369 | ACC 0.9326\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0421/0468 | LOSS: 0.2366 | ACC 0.9327\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0422/0468 | LOSS: 0.2364 | ACC 0.9328\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0423/0468 | LOSS: 0.2363 | ACC 0.9329\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0424/0468 | LOSS: 0.2361 | ACC 0.9329\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0425/0468 | LOSS: 0.2358 | ACC 0.9330\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0426/0468 | LOSS: 0.2357 | ACC 0.9330\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0427/0468 | LOSS: 0.2356 | ACC 0.9330\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0428/0468 | LOSS: 0.2359 | ACC 0.9329\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0429/0468 | LOSS: 0.2359 | ACC 0.9329\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0430/0468 | LOSS: 0.2359 | ACC 0.9329\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0431/0468 | LOSS: 0.2360 | ACC 0.9329\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0432/0468 | LOSS: 0.2358 | ACC 0.9330\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0433/0468 | LOSS: 0.2356 | ACC 0.9330\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0434/0468 | LOSS: 0.2354 | ACC 0.9331\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0435/0468 | LOSS: 0.2351 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0436/0468 | LOSS: 0.2349 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0437/0468 | LOSS: 0.2351 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0438/0468 | LOSS: 0.2350 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0439/0468 | LOSS: 0.2351 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0440/0468 | LOSS: 0.2349 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0441/0468 | LOSS: 0.2349 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0442/0468 | LOSS: 0.2350 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0443/0468 | LOSS: 0.2350 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0444/0468 | LOSS: 0.2350 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0445/0468 | LOSS: 0.2348 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0446/0468 | LOSS: 0.2346 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0447/0468 | LOSS: 0.2345 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0448/0468 | LOSS: 0.2345 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0449/0468 | LOSS: 0.2344 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0450/0468 | LOSS: 0.2344 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0451/0468 | LOSS: 0.2343 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0452/0468 | LOSS: 0.2343 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0453/0468 | LOSS: 0.2342 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0454/0468 | LOSS: 0.2343 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0455/0468 | LOSS: 0.2341 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0456/0468 | LOSS: 0.2341 | ACC 0.9332\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0457/0468 | LOSS: 0.2339 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0458/0468 | LOSS: 0.2339 | ACC 0.9333\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0459/0468 | LOSS: 0.2338 | ACC 0.9334\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0460/0468 | LOSS: 0.2337 | ACC 0.9334\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0461/0468 | LOSS: 0.2334 | ACC 0.9335\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0462/0468 | LOSS: 0.2333 | ACC 0.9335\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0463/0468 | LOSS: 0.2333 | ACC 0.9335\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0464/0468 | LOSS: 0.2332 | ACC 0.9335\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0465/0468 | LOSS: 0.2330 | ACC 0.9336\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0466/0468 | LOSS: 0.2332 | ACC 0.9336\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0467/0468 | LOSS: 0.2329 | ACC 0.9337\n",
      "TRAIN: EPOCH 0003/0005 | BATCH 0468/0468 | LOSS: 0.2329 | ACC 0.9337\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0000/0468 | LOSS: 0.1151 | ACC 0.9688\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0001/0468 | LOSS: 0.1475 | ACC 0.9570\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0002/0468 | LOSS: 0.1536 | ACC 0.9531\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0003/0468 | LOSS: 0.1958 | ACC 0.9453\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0004/0468 | LOSS: 0.1893 | ACC 0.9469\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0005/0468 | LOSS: 0.2014 | ACC 0.9427\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0006/0468 | LOSS: 0.1989 | ACC 0.9464\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0007/0468 | LOSS: 0.2044 | ACC 0.9395\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0008/0468 | LOSS: 0.2012 | ACC 0.9410\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0009/0468 | LOSS: 0.1962 | ACC 0.9422\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0010/0468 | LOSS: 0.1941 | ACC 0.9446\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0011/0468 | LOSS: 0.1895 | ACC 0.9473\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0012/0468 | LOSS: 0.1886 | ACC 0.9459\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0013/0468 | LOSS: 0.1943 | ACC 0.9436\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0014/0468 | LOSS: 0.1949 | ACC 0.9422\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0015/0468 | LOSS: 0.1941 | ACC 0.9438\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0016/0468 | LOSS: 0.1901 | ACC 0.9444\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0017/0468 | LOSS: 0.1894 | ACC 0.9453\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0018/0468 | LOSS: 0.1862 | ACC 0.9465\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0019/0468 | LOSS: 0.1907 | ACC 0.9457\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0020/0468 | LOSS: 0.1918 | ACC 0.9449\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0021/0468 | LOSS: 0.1907 | ACC 0.9460\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0022/0468 | LOSS: 0.1875 | ACC 0.9474\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0023/0468 | LOSS: 0.1883 | ACC 0.9473\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0024/0468 | LOSS: 0.1878 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0025/0468 | LOSS: 0.1872 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0026/0468 | LOSS: 0.1868 | ACC 0.9473\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0027/0468 | LOSS: 0.1868 | ACC 0.9459\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0028/0468 | LOSS: 0.1841 | ACC 0.9469\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0029/0468 | LOSS: 0.1825 | ACC 0.9474\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0030/0468 | LOSS: 0.1807 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0031/0468 | LOSS: 0.1813 | ACC 0.9468\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0032/0468 | LOSS: 0.1837 | ACC 0.9460\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0033/0468 | LOSS: 0.1842 | ACC 0.9458\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0034/0468 | LOSS: 0.1822 | ACC 0.9467\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0035/0468 | LOSS: 0.1810 | ACC 0.9470\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0036/0468 | LOSS: 0.1815 | ACC 0.9468\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0037/0468 | LOSS: 0.1804 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0038/0468 | LOSS: 0.1809 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0039/0468 | LOSS: 0.1801 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0040/0468 | LOSS: 0.1822 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0041/0468 | LOSS: 0.1813 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0042/0468 | LOSS: 0.1787 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0043/0468 | LOSS: 0.1790 | ACC 0.9485\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0044/0468 | LOSS: 0.1808 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0045/0468 | LOSS: 0.1813 | ACC 0.9475\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0046/0468 | LOSS: 0.1803 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0047/0468 | LOSS: 0.1796 | ACC 0.9484\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0048/0468 | LOSS: 0.1791 | ACC 0.9485\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0049/0468 | LOSS: 0.1792 | ACC 0.9487\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0050/0468 | LOSS: 0.1771 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0051/0468 | LOSS: 0.1766 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0052/0468 | LOSS: 0.1766 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0053/0468 | LOSS: 0.1761 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0054/0468 | LOSS: 0.1770 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0055/0468 | LOSS: 0.1764 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0056/0468 | LOSS: 0.1776 | ACC 0.9493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0004/0005 | BATCH 0057/0468 | LOSS: 0.1771 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0058/0468 | LOSS: 0.1761 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0059/0468 | LOSS: 0.1752 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0060/0468 | LOSS: 0.1741 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0061/0468 | LOSS: 0.1739 | ACC 0.9505\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0062/0468 | LOSS: 0.1726 | ACC 0.9509\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0063/0468 | LOSS: 0.1730 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0064/0468 | LOSS: 0.1736 | ACC 0.9505\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0065/0468 | LOSS: 0.1743 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0066/0468 | LOSS: 0.1745 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0067/0468 | LOSS: 0.1751 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0068/0468 | LOSS: 0.1743 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0069/0468 | LOSS: 0.1744 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0070/0468 | LOSS: 0.1751 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0071/0468 | LOSS: 0.1746 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0072/0468 | LOSS: 0.1750 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0073/0468 | LOSS: 0.1744 | ACC 0.9505\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0074/0468 | LOSS: 0.1751 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0075/0468 | LOSS: 0.1753 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0076/0468 | LOSS: 0.1751 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0077/0468 | LOSS: 0.1752 | ACC 0.9505\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0078/0468 | LOSS: 0.1747 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0079/0468 | LOSS: 0.1759 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0080/0468 | LOSS: 0.1761 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0081/0468 | LOSS: 0.1758 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0082/0468 | LOSS: 0.1760 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0083/0468 | LOSS: 0.1757 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0084/0468 | LOSS: 0.1749 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0085/0468 | LOSS: 0.1750 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0086/0468 | LOSS: 0.1749 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0087/0468 | LOSS: 0.1763 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0088/0468 | LOSS: 0.1759 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0089/0468 | LOSS: 0.1758 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0090/0468 | LOSS: 0.1760 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0091/0468 | LOSS: 0.1757 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0092/0468 | LOSS: 0.1773 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0093/0468 | LOSS: 0.1767 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0094/0468 | LOSS: 0.1775 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0095/0468 | LOSS: 0.1766 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0096/0468 | LOSS: 0.1760 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0097/0468 | LOSS: 0.1755 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0098/0468 | LOSS: 0.1759 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0099/0468 | LOSS: 0.1753 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0100/0468 | LOSS: 0.1750 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0101/0468 | LOSS: 0.1757 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0102/0468 | LOSS: 0.1761 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0103/0468 | LOSS: 0.1760 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0104/0468 | LOSS: 0.1755 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0105/0468 | LOSS: 0.1759 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0106/0468 | LOSS: 0.1765 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0107/0468 | LOSS: 0.1764 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0108/0468 | LOSS: 0.1762 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0109/0468 | LOSS: 0.1764 | ACC 0.9485\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0110/0468 | LOSS: 0.1763 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0111/0468 | LOSS: 0.1764 | ACC 0.9485\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0112/0468 | LOSS: 0.1757 | ACC 0.9487\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0113/0468 | LOSS: 0.1760 | ACC 0.9485\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0114/0468 | LOSS: 0.1763 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0115/0468 | LOSS: 0.1761 | ACC 0.9487\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0116/0468 | LOSS: 0.1754 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0117/0468 | LOSS: 0.1761 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0118/0468 | LOSS: 0.1758 | ACC 0.9487\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0119/0468 | LOSS: 0.1763 | ACC 0.9484\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0120/0468 | LOSS: 0.1763 | ACC 0.9483\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0121/0468 | LOSS: 0.1761 | ACC 0.9484\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0122/0468 | LOSS: 0.1766 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0123/0468 | LOSS: 0.1781 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0124/0468 | LOSS: 0.1777 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0125/0468 | LOSS: 0.1782 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0126/0468 | LOSS: 0.1781 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0127/0468 | LOSS: 0.1778 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0128/0468 | LOSS: 0.1777 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0129/0468 | LOSS: 0.1778 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0130/0468 | LOSS: 0.1773 | ACC 0.9484\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0131/0468 | LOSS: 0.1774 | ACC 0.9483\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0132/0468 | LOSS: 0.1780 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0133/0468 | LOSS: 0.1774 | ACC 0.9483\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0134/0468 | LOSS: 0.1775 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0135/0468 | LOSS: 0.1777 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0136/0468 | LOSS: 0.1782 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0137/0468 | LOSS: 0.1782 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0138/0468 | LOSS: 0.1789 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0139/0468 | LOSS: 0.1783 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0140/0468 | LOSS: 0.1783 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0141/0468 | LOSS: 0.1786 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0142/0468 | LOSS: 0.1786 | ACC 0.9483\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0143/0468 | LOSS: 0.1786 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0144/0468 | LOSS: 0.1789 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0145/0468 | LOSS: 0.1787 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0146/0468 | LOSS: 0.1788 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0147/0468 | LOSS: 0.1783 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0148/0468 | LOSS: 0.1786 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0149/0468 | LOSS: 0.1787 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0150/0468 | LOSS: 0.1791 | ACC 0.9475\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0151/0468 | LOSS: 0.1785 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0152/0468 | LOSS: 0.1781 | ACC 0.9479\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0153/0468 | LOSS: 0.1786 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0154/0468 | LOSS: 0.1782 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0155/0468 | LOSS: 0.1786 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0156/0468 | LOSS: 0.1794 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0157/0468 | LOSS: 0.1804 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0158/0468 | LOSS: 0.1807 | ACC 0.9475\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0159/0468 | LOSS: 0.1811 | ACC 0.9473\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0160/0468 | LOSS: 0.1808 | ACC 0.9474\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0161/0468 | LOSS: 0.1803 | ACC 0.9475\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0162/0468 | LOSS: 0.1801 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0163/0468 | LOSS: 0.1801 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0164/0468 | LOSS: 0.1797 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0165/0468 | LOSS: 0.1798 | ACC 0.9479\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0166/0468 | LOSS: 0.1800 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0167/0468 | LOSS: 0.1805 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0168/0468 | LOSS: 0.1812 | ACC 0.9474\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0169/0468 | LOSS: 0.1815 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0170/0468 | LOSS: 0.1818 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0171/0468 | LOSS: 0.1819 | ACC 0.9471\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0172/0468 | LOSS: 0.1819 | ACC 0.9471\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0173/0468 | LOSS: 0.1819 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0174/0468 | LOSS: 0.1816 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0175/0468 | LOSS: 0.1818 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0176/0468 | LOSS: 0.1823 | ACC 0.9470\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0177/0468 | LOSS: 0.1820 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0178/0468 | LOSS: 0.1819 | ACC 0.9472\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0179/0468 | LOSS: 0.1816 | ACC 0.9474\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0180/0468 | LOSS: 0.1813 | ACC 0.9475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0004/0005 | BATCH 0181/0468 | LOSS: 0.1810 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0182/0468 | LOSS: 0.1808 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0183/0468 | LOSS: 0.1805 | ACC 0.9477\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0184/0468 | LOSS: 0.1808 | ACC 0.9476\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0185/0468 | LOSS: 0.1803 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0186/0468 | LOSS: 0.1799 | ACC 0.9479\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0187/0468 | LOSS: 0.1799 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0188/0468 | LOSS: 0.1798 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0189/0468 | LOSS: 0.1795 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0190/0468 | LOSS: 0.1792 | ACC 0.9478\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0191/0468 | LOSS: 0.1795 | ACC 0.9479\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0192/0468 | LOSS: 0.1791 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0193/0468 | LOSS: 0.1792 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0194/0468 | LOSS: 0.1793 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0195/0468 | LOSS: 0.1791 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0196/0468 | LOSS: 0.1791 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0197/0468 | LOSS: 0.1790 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0198/0468 | LOSS: 0.1792 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0199/0468 | LOSS: 0.1789 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0200/0468 | LOSS: 0.1788 | ACC 0.9481\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0201/0468 | LOSS: 0.1789 | ACC 0.9480\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0202/0468 | LOSS: 0.1784 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0203/0468 | LOSS: 0.1782 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0204/0468 | LOSS: 0.1784 | ACC 0.9482\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0205/0468 | LOSS: 0.1780 | ACC 0.9483\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0206/0468 | LOSS: 0.1777 | ACC 0.9484\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0207/0468 | LOSS: 0.1772 | ACC 0.9485\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0208/0468 | LOSS: 0.1768 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0209/0468 | LOSS: 0.1767 | ACC 0.9487\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0210/0468 | LOSS: 0.1766 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0211/0468 | LOSS: 0.1770 | ACC 0.9486\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0212/0468 | LOSS: 0.1767 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0213/0468 | LOSS: 0.1769 | ACC 0.9487\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0214/0468 | LOSS: 0.1768 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0215/0468 | LOSS: 0.1764 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0216/0468 | LOSS: 0.1763 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0217/0468 | LOSS: 0.1764 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0218/0468 | LOSS: 0.1760 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0219/0468 | LOSS: 0.1760 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0220/0468 | LOSS: 0.1759 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0221/0468 | LOSS: 0.1758 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0222/0468 | LOSS: 0.1759 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0223/0468 | LOSS: 0.1759 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0224/0468 | LOSS: 0.1759 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0225/0468 | LOSS: 0.1759 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0226/0468 | LOSS: 0.1760 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0227/0468 | LOSS: 0.1761 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0228/0468 | LOSS: 0.1766 | ACC 0.9488\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0229/0468 | LOSS: 0.1763 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0230/0468 | LOSS: 0.1760 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0231/0468 | LOSS: 0.1762 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0232/0468 | LOSS: 0.1758 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0233/0468 | LOSS: 0.1759 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0234/0468 | LOSS: 0.1756 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0235/0468 | LOSS: 0.1755 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0236/0468 | LOSS: 0.1752 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0237/0468 | LOSS: 0.1758 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0238/0468 | LOSS: 0.1756 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0239/0468 | LOSS: 0.1755 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0240/0468 | LOSS: 0.1754 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0241/0468 | LOSS: 0.1753 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0242/0468 | LOSS: 0.1753 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0243/0468 | LOSS: 0.1751 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0244/0468 | LOSS: 0.1758 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0245/0468 | LOSS: 0.1759 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0246/0468 | LOSS: 0.1760 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0247/0468 | LOSS: 0.1758 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0248/0468 | LOSS: 0.1758 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0249/0468 | LOSS: 0.1759 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0250/0468 | LOSS: 0.1756 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0251/0468 | LOSS: 0.1759 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0252/0468 | LOSS: 0.1763 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0253/0468 | LOSS: 0.1764 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0254/0468 | LOSS: 0.1765 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0255/0468 | LOSS: 0.1766 | ACC 0.9489\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0256/0468 | LOSS: 0.1764 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0257/0468 | LOSS: 0.1763 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0258/0468 | LOSS: 0.1764 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0259/0468 | LOSS: 0.1763 | ACC 0.9490\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0260/0468 | LOSS: 0.1765 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0261/0468 | LOSS: 0.1764 | ACC 0.9491\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0262/0468 | LOSS: 0.1760 | ACC 0.9492\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0263/0468 | LOSS: 0.1757 | ACC 0.9493\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0264/0468 | LOSS: 0.1754 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0265/0468 | LOSS: 0.1755 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0266/0468 | LOSS: 0.1755 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0267/0468 | LOSS: 0.1754 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0268/0468 | LOSS: 0.1758 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0269/0468 | LOSS: 0.1758 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0270/0468 | LOSS: 0.1757 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0271/0468 | LOSS: 0.1757 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0272/0468 | LOSS: 0.1754 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0273/0468 | LOSS: 0.1755 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0274/0468 | LOSS: 0.1751 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0275/0468 | LOSS: 0.1752 | ACC 0.9495\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0276/0468 | LOSS: 0.1752 | ACC 0.9494\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0277/0468 | LOSS: 0.1748 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0278/0468 | LOSS: 0.1746 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0279/0468 | LOSS: 0.1745 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0280/0468 | LOSS: 0.1745 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0281/0468 | LOSS: 0.1743 | ACC 0.9496\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0282/0468 | LOSS: 0.1741 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0283/0468 | LOSS: 0.1739 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0284/0468 | LOSS: 0.1738 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0285/0468 | LOSS: 0.1737 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0286/0468 | LOSS: 0.1737 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0287/0468 | LOSS: 0.1735 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0288/0468 | LOSS: 0.1734 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0289/0468 | LOSS: 0.1737 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0290/0468 | LOSS: 0.1733 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0291/0468 | LOSS: 0.1733 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0292/0468 | LOSS: 0.1734 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0293/0468 | LOSS: 0.1731 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0294/0468 | LOSS: 0.1733 | ACC 0.9497\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0295/0468 | LOSS: 0.1729 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0296/0468 | LOSS: 0.1729 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0297/0468 | LOSS: 0.1733 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0298/0468 | LOSS: 0.1733 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0299/0468 | LOSS: 0.1731 | ACC 0.9498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0004/0005 | BATCH 0300/0468 | LOSS: 0.1729 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0301/0468 | LOSS: 0.1729 | ACC 0.9498\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0302/0468 | LOSS: 0.1728 | ACC 0.9499\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0303/0468 | LOSS: 0.1727 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0304/0468 | LOSS: 0.1726 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0305/0468 | LOSS: 0.1725 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0306/0468 | LOSS: 0.1725 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0307/0468 | LOSS: 0.1725 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0308/0468 | LOSS: 0.1723 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0309/0468 | LOSS: 0.1722 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0310/0468 | LOSS: 0.1721 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0311/0468 | LOSS: 0.1720 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0312/0468 | LOSS: 0.1718 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0313/0468 | LOSS: 0.1718 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0314/0468 | LOSS: 0.1716 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0315/0468 | LOSS: 0.1714 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0316/0468 | LOSS: 0.1712 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0317/0468 | LOSS: 0.1710 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0318/0468 | LOSS: 0.1711 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0319/0468 | LOSS: 0.1710 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0320/0468 | LOSS: 0.1715 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0321/0468 | LOSS: 0.1714 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0322/0468 | LOSS: 0.1713 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0323/0468 | LOSS: 0.1713 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0324/0468 | LOSS: 0.1715 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0325/0468 | LOSS: 0.1713 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0326/0468 | LOSS: 0.1713 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0327/0468 | LOSS: 0.1711 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0328/0468 | LOSS: 0.1711 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0329/0468 | LOSS: 0.1711 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0330/0468 | LOSS: 0.1710 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0331/0468 | LOSS: 0.1711 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0332/0468 | LOSS: 0.1711 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0333/0468 | LOSS: 0.1709 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0334/0468 | LOSS: 0.1707 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0335/0468 | LOSS: 0.1705 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0336/0468 | LOSS: 0.1706 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0337/0468 | LOSS: 0.1706 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0338/0468 | LOSS: 0.1705 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0339/0468 | LOSS: 0.1707 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0340/0468 | LOSS: 0.1705 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0341/0468 | LOSS: 0.1703 | ACC 0.9500\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0342/0468 | LOSS: 0.1702 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0343/0468 | LOSS: 0.1700 | ACC 0.9501\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0344/0468 | LOSS: 0.1699 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0345/0468 | LOSS: 0.1699 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0346/0468 | LOSS: 0.1699 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0347/0468 | LOSS: 0.1698 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0348/0468 | LOSS: 0.1695 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0349/0468 | LOSS: 0.1693 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0350/0468 | LOSS: 0.1692 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0351/0468 | LOSS: 0.1693 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0352/0468 | LOSS: 0.1695 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0353/0468 | LOSS: 0.1695 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0354/0468 | LOSS: 0.1697 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0355/0468 | LOSS: 0.1699 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0356/0468 | LOSS: 0.1697 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0357/0468 | LOSS: 0.1700 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0358/0468 | LOSS: 0.1699 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0359/0468 | LOSS: 0.1698 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0360/0468 | LOSS: 0.1697 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0361/0468 | LOSS: 0.1695 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0362/0468 | LOSS: 0.1697 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0363/0468 | LOSS: 0.1696 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0364/0468 | LOSS: 0.1698 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0365/0468 | LOSS: 0.1700 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0366/0468 | LOSS: 0.1699 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0367/0468 | LOSS: 0.1697 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0368/0468 | LOSS: 0.1697 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0369/0468 | LOSS: 0.1699 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0370/0468 | LOSS: 0.1700 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0371/0468 | LOSS: 0.1698 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0372/0468 | LOSS: 0.1699 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0373/0468 | LOSS: 0.1699 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0374/0468 | LOSS: 0.1699 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0375/0468 | LOSS: 0.1698 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0376/0468 | LOSS: 0.1698 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0377/0468 | LOSS: 0.1698 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0378/0468 | LOSS: 0.1698 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0379/0468 | LOSS: 0.1700 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0380/0468 | LOSS: 0.1700 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0381/0468 | LOSS: 0.1701 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0382/0468 | LOSS: 0.1701 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0383/0468 | LOSS: 0.1701 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0384/0468 | LOSS: 0.1701 | ACC 0.9502\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0385/0468 | LOSS: 0.1699 | ACC 0.9503\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0386/0468 | LOSS: 0.1696 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0387/0468 | LOSS: 0.1695 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0388/0468 | LOSS: 0.1695 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0389/0468 | LOSS: 0.1695 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0390/0468 | LOSS: 0.1693 | ACC 0.9504\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0391/0468 | LOSS: 0.1692 | ACC 0.9505\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0392/0468 | LOSS: 0.1691 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0393/0468 | LOSS: 0.1691 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0394/0468 | LOSS: 0.1689 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0395/0468 | LOSS: 0.1690 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0396/0468 | LOSS: 0.1689 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0397/0468 | LOSS: 0.1689 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0398/0468 | LOSS: 0.1689 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0399/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0400/0468 | LOSS: 0.1686 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0401/0468 | LOSS: 0.1687 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0402/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0403/0468 | LOSS: 0.1688 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0404/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0405/0468 | LOSS: 0.1687 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0406/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0407/0468 | LOSS: 0.1689 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0408/0468 | LOSS: 0.1689 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0409/0468 | LOSS: 0.1687 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0410/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0411/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0412/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0413/0468 | LOSS: 0.1688 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0414/0468 | LOSS: 0.1687 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0415/0468 | LOSS: 0.1687 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0416/0468 | LOSS: 0.1687 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0417/0468 | LOSS: 0.1687 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0418/0468 | LOSS: 0.1686 | ACC 0.9506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0004/0005 | BATCH 0419/0468 | LOSS: 0.1685 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0420/0468 | LOSS: 0.1682 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0421/0468 | LOSS: 0.1684 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0422/0468 | LOSS: 0.1684 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0423/0468 | LOSS: 0.1683 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0424/0468 | LOSS: 0.1685 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0425/0468 | LOSS: 0.1685 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0426/0468 | LOSS: 0.1685 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0427/0468 | LOSS: 0.1685 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0428/0468 | LOSS: 0.1685 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0429/0468 | LOSS: 0.1685 | ACC 0.9505\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0430/0468 | LOSS: 0.1683 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0431/0468 | LOSS: 0.1683 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0432/0468 | LOSS: 0.1684 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0433/0468 | LOSS: 0.1683 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0434/0468 | LOSS: 0.1683 | ACC 0.9506\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0435/0468 | LOSS: 0.1682 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0436/0468 | LOSS: 0.1681 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0437/0468 | LOSS: 0.1681 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0438/0468 | LOSS: 0.1681 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0439/0468 | LOSS: 0.1680 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0440/0468 | LOSS: 0.1678 | ACC 0.9507\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0441/0468 | LOSS: 0.1678 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0442/0468 | LOSS: 0.1677 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0443/0468 | LOSS: 0.1678 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0444/0468 | LOSS: 0.1679 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0445/0468 | LOSS: 0.1679 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0446/0468 | LOSS: 0.1680 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0447/0468 | LOSS: 0.1679 | ACC 0.9508\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0448/0468 | LOSS: 0.1677 | ACC 0.9509\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0449/0468 | LOSS: 0.1676 | ACC 0.9510\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0450/0468 | LOSS: 0.1675 | ACC 0.9510\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0451/0468 | LOSS: 0.1677 | ACC 0.9510\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0452/0468 | LOSS: 0.1676 | ACC 0.9510\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0453/0468 | LOSS: 0.1674 | ACC 0.9511\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0454/0468 | LOSS: 0.1674 | ACC 0.9511\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0455/0468 | LOSS: 0.1674 | ACC 0.9511\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0456/0468 | LOSS: 0.1672 | ACC 0.9511\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0457/0468 | LOSS: 0.1671 | ACC 0.9512\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0458/0468 | LOSS: 0.1671 | ACC 0.9512\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0459/0468 | LOSS: 0.1670 | ACC 0.9512\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0460/0468 | LOSS: 0.1668 | ACC 0.9513\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0461/0468 | LOSS: 0.1667 | ACC 0.9513\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0462/0468 | LOSS: 0.1669 | ACC 0.9514\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0463/0468 | LOSS: 0.1668 | ACC 0.9513\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0464/0468 | LOSS: 0.1668 | ACC 0.9513\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0465/0468 | LOSS: 0.1667 | ACC 0.9513\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0466/0468 | LOSS: 0.1665 | ACC 0.9514\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0467/0468 | LOSS: 0.1666 | ACC 0.9514\n",
      "TRAIN: EPOCH 0004/0005 | BATCH 0468/0468 | LOSS: 0.1665 | ACC 0.9514\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0000/0468 | LOSS: 0.2326 | ACC 0.9375\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0001/0468 | LOSS: 0.1539 | ACC 0.9609\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0002/0468 | LOSS: 0.1439 | ACC 0.9609\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0003/0468 | LOSS: 0.1378 | ACC 0.9609\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0004/0468 | LOSS: 0.1447 | ACC 0.9547\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0005/0468 | LOSS: 0.1501 | ACC 0.9492\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0006/0468 | LOSS: 0.1412 | ACC 0.9520\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0007/0468 | LOSS: 0.1433 | ACC 0.9521\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0008/0468 | LOSS: 0.1439 | ACC 0.9505\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0009/0468 | LOSS: 0.1414 | ACC 0.9516\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0010/0468 | LOSS: 0.1363 | ACC 0.9545\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0011/0468 | LOSS: 0.1331 | ACC 0.9564\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0012/0468 | LOSS: 0.1327 | ACC 0.9561\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0013/0468 | LOSS: 0.1323 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0014/0468 | LOSS: 0.1390 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0015/0468 | LOSS: 0.1374 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0016/0468 | LOSS: 0.1356 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0017/0468 | LOSS: 0.1384 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0018/0468 | LOSS: 0.1366 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0019/0468 | LOSS: 0.1390 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0020/0468 | LOSS: 0.1438 | ACC 0.9561\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0021/0468 | LOSS: 0.1454 | ACC 0.9553\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0022/0468 | LOSS: 0.1465 | ACC 0.9545\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0023/0468 | LOSS: 0.1446 | ACC 0.9557\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0024/0468 | LOSS: 0.1433 | ACC 0.9563\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0025/0468 | LOSS: 0.1459 | ACC 0.9552\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0026/0468 | LOSS: 0.1434 | ACC 0.9560\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0027/0468 | LOSS: 0.1412 | ACC 0.9570\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0028/0468 | LOSS: 0.1399 | ACC 0.9574\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0029/0468 | LOSS: 0.1394 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0030/0468 | LOSS: 0.1381 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0031/0468 | LOSS: 0.1369 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0032/0468 | LOSS: 0.1382 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0033/0468 | LOSS: 0.1390 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0034/0468 | LOSS: 0.1370 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0035/0468 | LOSS: 0.1366 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0036/0468 | LOSS: 0.1374 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0037/0468 | LOSS: 0.1389 | ACC 0.9576\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0038/0468 | LOSS: 0.1380 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0039/0468 | LOSS: 0.1388 | ACC 0.9574\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0040/0468 | LOSS: 0.1388 | ACC 0.9571\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0041/0468 | LOSS: 0.1398 | ACC 0.9568\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0042/0468 | LOSS: 0.1410 | ACC 0.9568\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0043/0468 | LOSS: 0.1406 | ACC 0.9570\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0044/0468 | LOSS: 0.1401 | ACC 0.9571\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0045/0468 | LOSS: 0.1419 | ACC 0.9567\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0046/0468 | LOSS: 0.1413 | ACC 0.9571\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0047/0468 | LOSS: 0.1417 | ACC 0.9570\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0048/0468 | LOSS: 0.1426 | ACC 0.9565\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0049/0468 | LOSS: 0.1427 | ACC 0.9564\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0050/0468 | LOSS: 0.1423 | ACC 0.9566\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0051/0468 | LOSS: 0.1424 | ACC 0.9567\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0052/0468 | LOSS: 0.1429 | ACC 0.9565\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0053/0468 | LOSS: 0.1436 | ACC 0.9567\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0054/0468 | LOSS: 0.1438 | ACC 0.9564\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0055/0468 | LOSS: 0.1428 | ACC 0.9566\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0056/0468 | LOSS: 0.1419 | ACC 0.9568\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0057/0468 | LOSS: 0.1406 | ACC 0.9573\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0058/0468 | LOSS: 0.1400 | ACC 0.9576\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0059/0468 | LOSS: 0.1402 | ACC 0.9574\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0060/0468 | LOSS: 0.1411 | ACC 0.9571\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0061/0468 | LOSS: 0.1409 | ACC 0.9575\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0062/0468 | LOSS: 0.1405 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0063/0468 | LOSS: 0.1414 | ACC 0.9574\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0064/0468 | LOSS: 0.1414 | ACC 0.9573\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0065/0468 | LOSS: 0.1406 | ACC 0.9574\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0066/0468 | LOSS: 0.1413 | ACC 0.9574\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0067/0468 | LOSS: 0.1404 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0068/0468 | LOSS: 0.1419 | ACC 0.9575\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0069/0468 | LOSS: 0.1407 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0070/0468 | LOSS: 0.1424 | ACC 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0005/0005 | BATCH 0071/0468 | LOSS: 0.1424 | ACC 0.9576\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0072/0468 | LOSS: 0.1430 | ACC 0.9573\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0073/0468 | LOSS: 0.1417 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0074/0468 | LOSS: 0.1417 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0075/0468 | LOSS: 0.1424 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0076/0468 | LOSS: 0.1418 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0077/0468 | LOSS: 0.1410 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0078/0468 | LOSS: 0.1404 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0079/0468 | LOSS: 0.1398 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0080/0468 | LOSS: 0.1397 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0081/0468 | LOSS: 0.1403 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0082/0468 | LOSS: 0.1403 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0083/0468 | LOSS: 0.1397 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0084/0468 | LOSS: 0.1400 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0085/0468 | LOSS: 0.1400 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0086/0468 | LOSS: 0.1398 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0087/0468 | LOSS: 0.1393 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0088/0468 | LOSS: 0.1389 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0089/0468 | LOSS: 0.1389 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0090/0468 | LOSS: 0.1386 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0091/0468 | LOSS: 0.1383 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0092/0468 | LOSS: 0.1392 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0093/0468 | LOSS: 0.1390 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0094/0468 | LOSS: 0.1392 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0095/0468 | LOSS: 0.1388 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0096/0468 | LOSS: 0.1383 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0097/0468 | LOSS: 0.1382 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0098/0468 | LOSS: 0.1383 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0099/0468 | LOSS: 0.1387 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0100/0468 | LOSS: 0.1390 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0101/0468 | LOSS: 0.1395 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0102/0468 | LOSS: 0.1394 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0103/0468 | LOSS: 0.1391 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0104/0468 | LOSS: 0.1389 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0105/0468 | LOSS: 0.1386 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0106/0468 | LOSS: 0.1378 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0107/0468 | LOSS: 0.1387 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0108/0468 | LOSS: 0.1408 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0109/0468 | LOSS: 0.1408 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0110/0468 | LOSS: 0.1409 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0111/0468 | LOSS: 0.1401 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0112/0468 | LOSS: 0.1401 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0113/0468 | LOSS: 0.1412 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0114/0468 | LOSS: 0.1419 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0115/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0116/0468 | LOSS: 0.1422 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0117/0468 | LOSS: 0.1420 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0118/0468 | LOSS: 0.1416 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0119/0468 | LOSS: 0.1412 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0120/0468 | LOSS: 0.1407 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0121/0468 | LOSS: 0.1404 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0122/0468 | LOSS: 0.1411 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0123/0468 | LOSS: 0.1409 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0124/0468 | LOSS: 0.1415 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0125/0468 | LOSS: 0.1421 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0126/0468 | LOSS: 0.1421 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0127/0468 | LOSS: 0.1424 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0128/0468 | LOSS: 0.1423 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0129/0468 | LOSS: 0.1418 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0130/0468 | LOSS: 0.1415 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0131/0468 | LOSS: 0.1410 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0132/0468 | LOSS: 0.1413 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0133/0468 | LOSS: 0.1406 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0134/0468 | LOSS: 0.1404 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0135/0468 | LOSS: 0.1397 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0136/0468 | LOSS: 0.1409 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0137/0468 | LOSS: 0.1409 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0138/0468 | LOSS: 0.1408 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0139/0468 | LOSS: 0.1412 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0140/0468 | LOSS: 0.1412 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0141/0468 | LOSS: 0.1414 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0142/0468 | LOSS: 0.1419 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0143/0468 | LOSS: 0.1423 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0144/0468 | LOSS: 0.1421 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0145/0468 | LOSS: 0.1427 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0146/0468 | LOSS: 0.1432 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0147/0468 | LOSS: 0.1430 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0148/0468 | LOSS: 0.1437 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0149/0468 | LOSS: 0.1438 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0150/0468 | LOSS: 0.1433 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0151/0468 | LOSS: 0.1432 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0152/0468 | LOSS: 0.1431 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0153/0468 | LOSS: 0.1433 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0154/0468 | LOSS: 0.1445 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0155/0468 | LOSS: 0.1447 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0156/0468 | LOSS: 0.1446 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0157/0468 | LOSS: 0.1441 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0158/0468 | LOSS: 0.1437 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0159/0468 | LOSS: 0.1435 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0160/0468 | LOSS: 0.1439 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0161/0468 | LOSS: 0.1441 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0162/0468 | LOSS: 0.1442 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0163/0468 | LOSS: 0.1444 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0164/0468 | LOSS: 0.1442 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0165/0468 | LOSS: 0.1441 | ACC 0.9576\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0166/0468 | LOSS: 0.1437 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0167/0468 | LOSS: 0.1439 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0168/0468 | LOSS: 0.1439 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0169/0468 | LOSS: 0.1439 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0170/0468 | LOSS: 0.1445 | ACC 0.9575\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0171/0468 | LOSS: 0.1447 | ACC 0.9575\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0172/0468 | LOSS: 0.1443 | ACC 0.9576\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0173/0468 | LOSS: 0.1443 | ACC 0.9576\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0174/0468 | LOSS: 0.1443 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0175/0468 | LOSS: 0.1442 | ACC 0.9577\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0176/0468 | LOSS: 0.1443 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0177/0468 | LOSS: 0.1444 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0178/0468 | LOSS: 0.1443 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0179/0468 | LOSS: 0.1440 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0180/0468 | LOSS: 0.1441 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0181/0468 | LOSS: 0.1437 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0182/0468 | LOSS: 0.1438 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0183/0468 | LOSS: 0.1437 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0184/0468 | LOSS: 0.1434 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0185/0468 | LOSS: 0.1435 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0186/0468 | LOSS: 0.1432 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0187/0468 | LOSS: 0.1431 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0188/0468 | LOSS: 0.1431 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0189/0468 | LOSS: 0.1427 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0190/0468 | LOSS: 0.1427 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0191/0468 | LOSS: 0.1428 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0192/0468 | LOSS: 0.1424 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0193/0468 | LOSS: 0.1424 | ACC 0.9584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0005/0005 | BATCH 0194/0468 | LOSS: 0.1423 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0195/0468 | LOSS: 0.1424 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0196/0468 | LOSS: 0.1426 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0197/0468 | LOSS: 0.1429 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0198/0468 | LOSS: 0.1426 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0199/0468 | LOSS: 0.1425 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0200/0468 | LOSS: 0.1430 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0201/0468 | LOSS: 0.1428 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0202/0468 | LOSS: 0.1427 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0203/0468 | LOSS: 0.1425 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0204/0468 | LOSS: 0.1420 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0205/0468 | LOSS: 0.1421 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0206/0468 | LOSS: 0.1422 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0207/0468 | LOSS: 0.1421 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0208/0468 | LOSS: 0.1419 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0209/0468 | LOSS: 0.1417 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0210/0468 | LOSS: 0.1419 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0211/0468 | LOSS: 0.1417 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0212/0468 | LOSS: 0.1414 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0213/0468 | LOSS: 0.1413 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0214/0468 | LOSS: 0.1413 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0215/0468 | LOSS: 0.1411 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0216/0468 | LOSS: 0.1416 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0217/0468 | LOSS: 0.1414 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0218/0468 | LOSS: 0.1415 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0219/0468 | LOSS: 0.1414 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0220/0468 | LOSS: 0.1414 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0221/0468 | LOSS: 0.1411 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0222/0468 | LOSS: 0.1416 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0223/0468 | LOSS: 0.1413 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0224/0468 | LOSS: 0.1415 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0225/0468 | LOSS: 0.1422 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0226/0468 | LOSS: 0.1421 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0227/0468 | LOSS: 0.1419 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0228/0468 | LOSS: 0.1417 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0229/0468 | LOSS: 0.1417 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0230/0468 | LOSS: 0.1418 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0231/0468 | LOSS: 0.1418 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0232/0468 | LOSS: 0.1421 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0233/0468 | LOSS: 0.1418 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0234/0468 | LOSS: 0.1415 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0235/0468 | LOSS: 0.1419 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0236/0468 | LOSS: 0.1422 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0237/0468 | LOSS: 0.1423 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0238/0468 | LOSS: 0.1424 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0239/0468 | LOSS: 0.1421 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0240/0468 | LOSS: 0.1418 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0241/0468 | LOSS: 0.1416 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0242/0468 | LOSS: 0.1415 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0243/0468 | LOSS: 0.1417 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0244/0468 | LOSS: 0.1415 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0245/0468 | LOSS: 0.1416 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0246/0468 | LOSS: 0.1416 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0247/0468 | LOSS: 0.1418 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0248/0468 | LOSS: 0.1419 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0249/0468 | LOSS: 0.1417 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0250/0468 | LOSS: 0.1416 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0251/0468 | LOSS: 0.1415 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0252/0468 | LOSS: 0.1418 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0253/0468 | LOSS: 0.1418 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0254/0468 | LOSS: 0.1416 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0255/0468 | LOSS: 0.1414 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0256/0468 | LOSS: 0.1413 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0257/0468 | LOSS: 0.1413 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0258/0468 | LOSS: 0.1412 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0259/0468 | LOSS: 0.1413 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0260/0468 | LOSS: 0.1415 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0261/0468 | LOSS: 0.1417 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0262/0468 | LOSS: 0.1417 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0263/0468 | LOSS: 0.1417 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0264/0468 | LOSS: 0.1417 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0265/0468 | LOSS: 0.1421 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0266/0468 | LOSS: 0.1422 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0267/0468 | LOSS: 0.1427 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0268/0468 | LOSS: 0.1427 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0269/0468 | LOSS: 0.1428 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0270/0468 | LOSS: 0.1431 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0271/0468 | LOSS: 0.1427 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0272/0468 | LOSS: 0.1425 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0273/0468 | LOSS: 0.1424 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0274/0468 | LOSS: 0.1423 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0275/0468 | LOSS: 0.1425 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0276/0468 | LOSS: 0.1426 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0277/0468 | LOSS: 0.1427 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0278/0468 | LOSS: 0.1427 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0279/0468 | LOSS: 0.1430 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0280/0468 | LOSS: 0.1431 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0281/0468 | LOSS: 0.1431 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0282/0468 | LOSS: 0.1430 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0283/0468 | LOSS: 0.1430 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0284/0468 | LOSS: 0.1430 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0285/0468 | LOSS: 0.1430 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0286/0468 | LOSS: 0.1431 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0287/0468 | LOSS: 0.1429 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0288/0468 | LOSS: 0.1430 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0289/0468 | LOSS: 0.1429 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0290/0468 | LOSS: 0.1431 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0291/0468 | LOSS: 0.1430 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0292/0468 | LOSS: 0.1429 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0293/0468 | LOSS: 0.1426 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0294/0468 | LOSS: 0.1423 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0295/0468 | LOSS: 0.1422 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0296/0468 | LOSS: 0.1421 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0297/0468 | LOSS: 0.1422 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0298/0468 | LOSS: 0.1421 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0299/0468 | LOSS: 0.1422 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0300/0468 | LOSS: 0.1423 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0301/0468 | LOSS: 0.1425 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0302/0468 | LOSS: 0.1429 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0303/0468 | LOSS: 0.1430 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0304/0468 | LOSS: 0.1430 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0305/0468 | LOSS: 0.1429 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0306/0468 | LOSS: 0.1428 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0307/0468 | LOSS: 0.1429 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0308/0468 | LOSS: 0.1428 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0309/0468 | LOSS: 0.1427 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0310/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0311/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0312/0468 | LOSS: 0.1426 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0313/0468 | LOSS: 0.1427 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0314/0468 | LOSS: 0.1425 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0315/0468 | LOSS: 0.1425 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0316/0468 | LOSS: 0.1427 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0317/0468 | LOSS: 0.1426 | ACC 0.9578\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0318/0468 | LOSS: 0.1424 | ACC 0.9579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0005/0005 | BATCH 0319/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0320/0468 | LOSS: 0.1424 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0321/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0322/0468 | LOSS: 0.1423 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0323/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0324/0468 | LOSS: 0.1425 | ACC 0.9579\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0325/0468 | LOSS: 0.1422 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0326/0468 | LOSS: 0.1421 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0327/0468 | LOSS: 0.1421 | ACC 0.9580\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0328/0468 | LOSS: 0.1420 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0329/0468 | LOSS: 0.1419 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0330/0468 | LOSS: 0.1417 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0331/0468 | LOSS: 0.1419 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0332/0468 | LOSS: 0.1419 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0333/0468 | LOSS: 0.1420 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0334/0468 | LOSS: 0.1418 | ACC 0.9581\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0335/0468 | LOSS: 0.1418 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0336/0468 | LOSS: 0.1418 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0337/0468 | LOSS: 0.1416 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0338/0468 | LOSS: 0.1415 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0339/0468 | LOSS: 0.1414 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0340/0468 | LOSS: 0.1414 | ACC 0.9582\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0341/0468 | LOSS: 0.1412 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0342/0468 | LOSS: 0.1411 | ACC 0.9583\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0343/0468 | LOSS: 0.1409 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0344/0468 | LOSS: 0.1408 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0345/0468 | LOSS: 0.1407 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0346/0468 | LOSS: 0.1407 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0347/0468 | LOSS: 0.1405 | ACC 0.9584\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0348/0468 | LOSS: 0.1406 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0349/0468 | LOSS: 0.1404 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0350/0468 | LOSS: 0.1402 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0351/0468 | LOSS: 0.1402 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0352/0468 | LOSS: 0.1403 | ACC 0.9585\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0353/0468 | LOSS: 0.1402 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0354/0468 | LOSS: 0.1400 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0355/0468 | LOSS: 0.1400 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0356/0468 | LOSS: 0.1398 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0357/0468 | LOSS: 0.1398 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0358/0468 | LOSS: 0.1397 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0359/0468 | LOSS: 0.1397 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0360/0468 | LOSS: 0.1396 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0361/0468 | LOSS: 0.1396 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0362/0468 | LOSS: 0.1395 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0363/0468 | LOSS: 0.1397 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0364/0468 | LOSS: 0.1396 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0365/0468 | LOSS: 0.1394 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0366/0468 | LOSS: 0.1392 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0367/0468 | LOSS: 0.1391 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0368/0468 | LOSS: 0.1390 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0369/0468 | LOSS: 0.1390 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0370/0468 | LOSS: 0.1390 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0371/0468 | LOSS: 0.1388 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0372/0468 | LOSS: 0.1386 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0373/0468 | LOSS: 0.1385 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0374/0468 | LOSS: 0.1386 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0375/0468 | LOSS: 0.1385 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0376/0468 | LOSS: 0.1384 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0377/0468 | LOSS: 0.1386 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0378/0468 | LOSS: 0.1385 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0379/0468 | LOSS: 0.1385 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0380/0468 | LOSS: 0.1389 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0381/0468 | LOSS: 0.1391 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0382/0468 | LOSS: 0.1394 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0383/0468 | LOSS: 0.1393 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0384/0468 | LOSS: 0.1393 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0385/0468 | LOSS: 0.1393 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0386/0468 | LOSS: 0.1392 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0387/0468 | LOSS: 0.1393 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0388/0468 | LOSS: 0.1393 | ACC 0.9586\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0389/0468 | LOSS: 0.1393 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0390/0468 | LOSS: 0.1391 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0391/0468 | LOSS: 0.1391 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0392/0468 | LOSS: 0.1391 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0393/0468 | LOSS: 0.1390 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0394/0468 | LOSS: 0.1389 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0395/0468 | LOSS: 0.1388 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0396/0468 | LOSS: 0.1388 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0397/0468 | LOSS: 0.1388 | ACC 0.9587\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0398/0468 | LOSS: 0.1386 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0399/0468 | LOSS: 0.1385 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0400/0468 | LOSS: 0.1384 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0401/0468 | LOSS: 0.1383 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0402/0468 | LOSS: 0.1383 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0403/0468 | LOSS: 0.1382 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0404/0468 | LOSS: 0.1382 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0405/0468 | LOSS: 0.1382 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0406/0468 | LOSS: 0.1381 | ACC 0.9588\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0407/0468 | LOSS: 0.1380 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0408/0468 | LOSS: 0.1380 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0409/0468 | LOSS: 0.1379 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0410/0468 | LOSS: 0.1382 | ACC 0.9589\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0411/0468 | LOSS: 0.1380 | ACC 0.9590\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0412/0468 | LOSS: 0.1378 | ACC 0.9590\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0413/0468 | LOSS: 0.1377 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0414/0468 | LOSS: 0.1376 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0415/0468 | LOSS: 0.1375 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0416/0468 | LOSS: 0.1375 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0417/0468 | LOSS: 0.1374 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0418/0468 | LOSS: 0.1374 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0419/0468 | LOSS: 0.1373 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0420/0468 | LOSS: 0.1373 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0421/0468 | LOSS: 0.1374 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0422/0468 | LOSS: 0.1375 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0423/0468 | LOSS: 0.1377 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0424/0468 | LOSS: 0.1377 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0425/0468 | LOSS: 0.1376 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0426/0468 | LOSS: 0.1376 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0427/0468 | LOSS: 0.1379 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0428/0468 | LOSS: 0.1378 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0429/0468 | LOSS: 0.1376 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0430/0468 | LOSS: 0.1378 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0431/0468 | LOSS: 0.1378 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0432/0468 | LOSS: 0.1377 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0433/0468 | LOSS: 0.1377 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0434/0468 | LOSS: 0.1376 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0435/0468 | LOSS: 0.1375 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0436/0468 | LOSS: 0.1375 | ACC 0.9591\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0437/0468 | LOSS: 0.1374 | ACC 0.9591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0005/0005 | BATCH 0438/0468 | LOSS: 0.1374 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0439/0468 | LOSS: 0.1372 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0440/0468 | LOSS: 0.1372 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0441/0468 | LOSS: 0.1373 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0442/0468 | LOSS: 0.1374 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0443/0468 | LOSS: 0.1373 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0444/0468 | LOSS: 0.1374 | ACC 0.9592\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0445/0468 | LOSS: 0.1373 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0446/0468 | LOSS: 0.1374 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0447/0468 | LOSS: 0.1373 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0448/0468 | LOSS: 0.1373 | ACC 0.9593\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0449/0468 | LOSS: 0.1371 | ACC 0.9594\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0450/0468 | LOSS: 0.1371 | ACC 0.9594\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0451/0468 | LOSS: 0.1370 | ACC 0.9594\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0452/0468 | LOSS: 0.1371 | ACC 0.9594\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0453/0468 | LOSS: 0.1369 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0454/0468 | LOSS: 0.1369 | ACC 0.9594\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0455/0468 | LOSS: 0.1369 | ACC 0.9594\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0456/0468 | LOSS: 0.1370 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0457/0468 | LOSS: 0.1369 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0458/0468 | LOSS: 0.1368 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0459/0468 | LOSS: 0.1367 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0460/0468 | LOSS: 0.1366 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0461/0468 | LOSS: 0.1366 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0462/0468 | LOSS: 0.1365 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0463/0468 | LOSS: 0.1365 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0464/0468 | LOSS: 0.1366 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0465/0468 | LOSS: 0.1364 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0466/0468 | LOSS: 0.1364 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0467/0468 | LOSS: 0.1365 | ACC 0.9595\n",
      "TRAIN: EPOCH 0005/0005 | BATCH 0468/0468 | LOSS: 0.1365 | ACC 0.9595\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epoch + 1):\n",
    "    net.train() # net.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    acc_arr = []\n",
    "\n",
    "    for batch, (input, label) in enumerate(loader):\n",
    "        input = input.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = net(input)     # net.forward(input)\n",
    "        pred = fn_pred(output)  # softmax\n",
    "\n",
    "        optimizer.zero_grad()   # G = 0 \n",
    "\n",
    "        loss = fn_loss(output, label)   # \n",
    "        acc = fn_accuracy(pred, label)\n",
    "\n",
    "        loss.backward() # 그라디언트를 계산\n",
    "\n",
    "        optimizer.step()    # 그라디언트를 사용하여 파라미터를 업데이트\n",
    "\n",
    "        loss_arr += [loss.item()]\n",
    "        acc_arr += [acc.item()]\n",
    "\n",
    "        print('TRAIN: EPOCH %04d/%04d | BATCH %04d/%04d | LOSS: %.4f | ACC %.4f' %\n",
    "              (epoch, num_epoch, batch, num_batch, np.mean(loss_arr), np.mean(acc_arr)))\n",
    "\n",
    "    save(ckpt_dir = ckpt_dir, net=net, optim=optimizer, epoch=epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb0112c456f82d9d2ffc2ba40e932c79270a28c5f552c1ea081145b825aa262c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
