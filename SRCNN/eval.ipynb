{"cells":[{"cell_type":"markdown","metadata":{"id":"Fhq2uP8MODEZ"},"source":["# SRCNN\n","- 참고논문: [Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/pdf/1501.00092)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNT7DiFCOLEE","executionInfo":{"status":"ok","timestamp":1654673556604,"user_tz":-540,"elapsed":23138,"user":{"displayName":"김승욱","userId":"08177621807583907944"}},"outputId":"f15365f6-7904-4430-9ec8-a36616217b99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"xMBjKlXAODEc"},"source":["## 1. 필요 라이브러리 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gtb3Jq0IODEd"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","\n","import time\n","import PIL.Image as pil_image\n","import numpy as np\n","import os"]},{"cell_type":"markdown","metadata":{"id":"X1eyPQxnODEe"},"source":["## 3. 하이퍼 파라미터 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0HFeaU_ODEe"},"outputs":[],"source":["root = '/content/drive/MyDrive/ai_intro/SRCNN'  # PC에서는 root = './'로 변경\n","\n","ckpt_dir = os.path.join(root, 'checkpoint')\n","test_db_dir = os.path.join(root, 'DB/test')\n","img_dir = os.path.join(root, 'results')\n","\n","num_workers = 0\n","\n","device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"ZAPk0M3gODEf"},"source":["## 4. 파라미터 저장/불러오기 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--n07_hjODEf"},"outputs":[],"source":["def load(filename, net, optim=None):\n","    dict_model = torch.load(filename, map_location=device)\n","\n","    net.load_state_dict(dict_model['net'])\n","    if optim is not None:\n","        optim.load_state_dict(dict_model['optim'])\n","\n","    return net, optim"]},{"cell_type":"markdown","metadata":{"id":"dWCJurbcODEg"},"source":["## 5. 네트워크 및 손실함수 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlL5DTFnODEh"},"outputs":[],"source":["class SRCNN(nn.Module):\n","    def __init__(self, num_channels=3):\n","        super(SRCNN, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels=num_channels,\n","                               out_channels=64,\n","                               kernel_size=9,\n","                               padding=9 // 2)\n","        self.conv2 = nn.Conv2d(in_channels=64,\n","                               out_channels=32,\n","                               kernel_size=5,\n","                               padding=5 // 2)\n","        self.conv3 = nn.Conv2d(in_channels=32, \n","                               out_channels=num_channels, \n","                               kernel_size=5,\n","                               padding=5 // 2)\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self, x):\n","        x = self.relu(self.conv1(x))\n","        x = self.relu(self.conv2(x))\n","        x = self.conv3(x)\n","        \n","        return x\n","\n","model = SRCNN().to(device)"]},{"cell_type":"markdown","metadata":{"id":"gW9sIUzkODEh"},"source":["## 7. Dataset, data loader 선언"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfeCA6ApODEi"},"outputs":[],"source":["class SRDataset(Dataset):\n","    def __init__(self, \n","                 image_dir,\n","                 input_transform=None, \n","                 target_transform=None, \n","                 db_type='train'):\n","        super(SRDataset, self).__init__()\n","        \n","        self.db_type = db_type\n","        \n","        lr_dir = os.path.join(image_dir, \"lr/\")     # low-resolution image (input)\n","        self.lr_list = [os.path.join(lr_dir, f) for f in os.listdir(lr_dir)]\n","        self.lr_list.sort()\n","\n","        if self.db_type != 'test':\n","            hr_dir = os.path.join(image_dir, \"hr/\")     # high-resolution image (label)\n","            self.hr_list = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir)]\n","            self.hr_list.sort()\n","        \n","        total_len = len(self.lr_list)\n","        train_len = round(total_len * 0.9)\n","        \n","        self.input_transform = input_transform\n","        self.target_transform = target_transform\n","\n","        if self.db_type is 'train':\n","            self.lr_list = self.lr_list[:train_len]\n","            self.hr_list = self.hr_list[:train_len]\n","        elif self.db_type is 'val':\n","            self.lr_list = self.lr_list[train_len:]\n","            self.hr_list = self.hr_list[train_len:]\n","        elif self.db_type is 'test':\n","            self.lr_list = self.lr_list\n","\n","        self.crop_size = 33\n","\n","    def __getitem__(self, idx):\n","        lr_image = pil_image.open(self.lr_list[idx])\n","        if self.db_type != 'test':\n","            hr_image = pil_image.open(self.hr_list[idx])\n","        \n","        # Transform images\n","        if self.input_transform is not None:\n","            lr_image = self.input_transform(lr_image)\n","        \n","        if (self.target_transform is not None) and (self.db_type != 'test'):\n","            hr_image = self.target_transform(hr_image)\n","\n","        if self.db_type is 'train':\n","            # Random crop\n","            lr_image, hr_image = self.random_crop(lr_image, hr_image)\n","        \n","        if self.db_type is 'test':\n","            return lr_image\n","        else:\n","            return lr_image, hr_image\n","            \n","    def random_crop(self, input, target):\n","        h = input.size(-2)\n","        w = input.size(-1)\n","                \n","        rand_h = torch.randint(h - self.crop_size, [1, 1])\n","        rand_w = torch.randint(w - self.crop_size, [1, 1])\n","        \n","        input = input[:, rand_h:rand_h + self.crop_size, rand_w:rand_w + self.crop_size]\n","        target = target[:, rand_h:rand_h + self.crop_size, rand_w:rand_w + self.crop_size]\n","        \n","        return input, target\n","        \n","    def __len__(self):\n","        return len(self.lr_list)\n","\n","\n","input_transform = transforms.Compose(\n","                    [transforms.ToTensor()])\n","target_transform = transforms.Compose(\n","                    [transforms.ToTensor()])\n","\n","test_dataset = SRDataset(test_db_dir, \n","                         input_transform=input_transform,\n","                         target_transform=target_transform,\n","                         db_type='test')\n","test_dataloader = DataLoader(dataset=test_dataset,\n","                             batch_size=1,\n","                             shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"ulNse0M8ODEi"},"source":["## 8. Training & validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-vaR8VSODEj"},"outputs":[],"source":["model, _ = load(os.path.join(ckpt_dir, 'model_epoch_best.pth'), net=model)\n","\n","model.eval()\n","psnr_arr = []\n","\n","if not os.path.exists(img_dir):\n","    os.mkdir(img_dir)\n","\n","for n, input in enumerate(test_dataloader):\n","    input = input.to(device)\n","    \n","    with torch.no_grad():\n","        preds = model(input)\n","        preds = preds.clamp(0.0, 1.0)\n","        preds.squeeze()\n","        \n","    save_image(preds, os.path.join(img_dir, 'result%d.png' % (n)))   # 영상 저장\n","\n"]}],"metadata":{"interpreter":{"hash":"61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"},"kernelspec":{"display_name":"Python 3.7.13 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"name":"eval.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}